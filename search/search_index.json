{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#welcome-to-jarvais","title":"Welcome to jarvAIs","text":"<p>just a really versatile AI service</p> <p>jarvAIs is a Python package designed to automate and enhance machine learning workflows. The primary goal of this project is to reduce redundancy in repetitive tasks, improve consistency, and elevate the quality of standardized processes in oncology research.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install jarvais\n</code></pre>"},{"location":"#recommended-create-new-pixi-environment-for-a-project","title":"(recommended) Create new <code>pixi</code> environment for a project","text":"<pre><code>mkdir my_project\ncd my_project\npixi init\npixi add --pypi jarvais\n</code></pre>"},{"location":"#recommended-create-new-conda-virtual-environment","title":"(recommended) Create new conda virtual environment","text":"<pre><code>conda create -n jarvais python=3.11\nconda activate jarvais\npip install jarvais\n</code></pre>"},{"location":"#modules","title":"Modules","text":"<p>This package consists of 3 different modules:</p> <ul> <li>Analyzer: A module that analyzes and processes data, providing valuable insights for downstream tasks.</li> <li>Trainer: A module for training machine learning models, designed to be flexible and efficient.</li> <li>Explainer: A module that explains model predictions, offering interpretability and transparency in decision-making.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Please use the following angular commit message format:</p> <pre><code>&lt;type&gt;(optional scope): short summary in present tense\n\n(optional body: explains motivation for the change)\n\n(optional footer: note BREAKING CHANGES here, and issues to be closed)\n\n</code></pre> <p><code>&lt;type&gt;</code> refers to the kind of change made and is usually one of:</p> <ul> <li><code>feat</code>: A new feature.</li> <li><code>fix</code>: A bug fix.</li> <li><code>docs</code>: Documentation changes.</li> <li><code>style</code>: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc).</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature.</li> <li><code>perf</code>: A code change that improves performance.</li> <li><code>test</code>: Changes to the test framework.</li> <li><code>build</code>: Changes to the build process or tools.</li> </ul> <p><code>scope</code> is an optional keyword that provides context for where the change was made. It can be anything relevant to your package or development workflow (e.g., it could be the module or function - name affected by the change).</p> <p>Interested in contributing? Check out the contributing guidelines. Please note that this project is released with a Code of Conduct. By contributing to this project, you agree to abide by its terms.</p>"},{"location":"api/analyzer/","title":"Analyzer","text":""},{"location":"api/analyzer/#analyzer","title":"Analyzer","text":""},{"location":"api/analyzer/#analyzer-entry-point","title":"Analyzer Entry Point","text":"<p>The <code>Analyzer</code> class is part of the <code>jarvais.analyzer</code> module. It provides tools for exploring datasets and identifying issues.</p>"},{"location":"api/analyzer/#jarvais.analyzer.Analyzer","title":"<code>jarvais.analyzer.Analyzer</code>","text":"<p>Analyzer class for data visualization and exploration.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input data to be analyzed.</p> required <code>output_dir</code> <code>str | Path</code> <p>The output directory for saving the analysis report and visualizations.</p> required <code>categorical_columns</code> <code>list[str] | None</code> <p>List of categorical columns. If None, all remaining columns will be considered categorical.</p> <code>None</code> <code>continuous_columns</code> <code>list[str] | None</code> <p>List of continuous columns. If None, all remaining columns will be considered continuous.</p> <code>None</code> <code>date_columns</code> <code>list[str] | None</code> <p>List of date columns. If None, no date columns will be considered.</p> <code>None</code> <code>target_variable</code> <code>str | None</code> <p>The target variable for analysis. If None, analysis will be performed without a target variable.</p> <code>None</code> <code>task</code> <code>str | None</code> <p>The type of task for analysis, e.g. classification, regression, survival. If None, analysis will be performed without a task.</p> <code>None</code> <code>generate_report</code> <code>bool</code> <p>Whether to generate a PDF report of the analysis. Default is True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>The input data to be analyzed.</p> <code>missingness_module</code> <code>MissingnessModule</code> <p>Module for handling missing data.</p> <code>outlier_module</code> <code>OutlierModule</code> <p>Module for detecting outliers.</p> <code>encoding_module</code> <code>OneHotEncodingModule</code> <p>Module for encoding categorical variables.</p> <code>visualization_module</code> <code>VisualizationModule</code> <p>Module for generating visualizations.</p> <code>settings</code> <code>AnalyzerSettings</code> <p>Settings for the analyzer, including output directory and column specifications.</p> Source code in <code>src/jarvais/analyzer/analyzer.py</code> <pre><code>class Analyzer():\n    \"\"\"\n    Analyzer class for data visualization and exploration.\n\n    Parameters:\n        data (pd.DataFrame): The input data to be analyzed.\n        output_dir (str | Path): The output directory for saving the analysis report and visualizations.\n        categorical_columns (list[str] | None): List of categorical columns. If None, all remaining columns will be considered categorical.\n        continuous_columns (list[str] | None): List of continuous columns. If None, all remaining columns will be considered continuous.\n        date_columns (list[str] | None): List of date columns. If None, no date columns will be considered.\n        target_variable (str | None): The target variable for analysis. If None, analysis will be performed without a target variable.\n        task (str | None): The type of task for analysis, e.g. classification, regression, survival. If None, analysis will be performed without a task.\n        generate_report (bool): Whether to generate a PDF report of the analysis. Default is True.\n\n    Attributes:\n        data (pd.DataFrame): The input data to be analyzed.\n        missingness_module (MissingnessModule): Module for handling missing data.\n        outlier_module (OutlierModule): Module for detecting outliers.\n        encoding_module (OneHotEncodingModule): Module for encoding categorical variables.\n        visualization_module (VisualizationModule): Module for generating visualizations.\n        settings (AnalyzerSettings): Settings for the analyzer, including output directory and column specifications.\n    \"\"\"\n    def __init__(\n            self, \n            data: pd.DataFrame,\n            output_dir: str | Path,\n            categorical_columns: list[str] | None = None, \n            continuous_columns: list[str] | None = None,\n            date_columns: list[str] | None = None,\n            target_variable: str | None = None,\n            task: str | None = None,\n            generate_report: bool = True\n        ) -&gt; None:\n        self.data = data\n\n        # Infer all types if none provided\n        if not categorical_columns and not continuous_columns and not date_columns:\n            categorical_columns, continuous_columns, date_columns = infer_types(self.data)\n        else:\n            categorical_columns = categorical_columns or []\n            continuous_columns = continuous_columns or []\n            date_columns = date_columns or []\n\n            specified_cols = set(categorical_columns + continuous_columns + date_columns)\n            remaining_cols = set(self.data.columns) - specified_cols\n\n            if not categorical_columns:\n                logger.warning(\"Categorical columns not specified. Inferring from remaining columns.\")\n                categorical_columns = list(remaining_cols)\n\n            elif not continuous_columns:\n                logger.warning(\"Continuous columns not specified. Inferring from remaining columns.\")\n                continuous_columns = list(remaining_cols)\n\n            elif not date_columns:\n                logger.warning(\"Date columns not specified. Inferring from remaining columns.\")\n                date_columns = list(remaining_cols)        \n\n        self.missingness_module = MissingnessModule.build(\n            categorical_columns=categorical_columns, \n            continuous_columns=continuous_columns\n        )\n        self.outlier_module = OutlierModule.build(\n            categorical_columns=categorical_columns, \n            continuous_columns=continuous_columns\n        )\n        self.encoding_module = OneHotEncodingModule.build(\n            categorical_columns=categorical_columns, \n            target_variable=target_variable\n        )\n        self.visualization_module = VisualizationModule.build(\n            output_dir=Path(output_dir),\n            continuous_columns=continuous_columns,\n            categorical_columns=categorical_columns,\n            task=task,\n            target_variable=target_variable\n        )\n\n        self.settings = AnalyzerSettings(\n            output_dir=Path(output_dir),\n            categorical_columns=categorical_columns,\n            continuous_columns=continuous_columns,\n            date_columns=date_columns,\n            target_variable=target_variable,\n            task=task,\n            generate_report=generate_report,\n            missingness=self.missingness_module,\n            outlier=self.outlier_module,\n            visualization=self.visualization_module,\n            encoding=self.encoding_module\n        )\n\n    @classmethod\n    def from_settings(\n            cls, \n            data: pd.DataFrame, \n            settings_dict: dict\n        ) -&gt; \"Analyzer\":\n        \"\"\"\n        Initialize an Analyzer instance with a given settings dictionary. Settings are validated by pydantic.\n\n        Args:\n            data (pd.DataFrame): The input data for the analyzer.\n            settings_dict (dict): A dictionary containing the analyzer settings.\n\n        Returns:\n            Analyzer: An analyzer instance with the given settings.\n\n        Raises:\n            ValueError: If the settings dictionary is invalid.\n        \"\"\"\n        try:\n            settings = AnalyzerSettings.model_validate(settings_dict)\n        except Exception as e:\n            raise ValueError(\"Invalid analyzer settings\") from e\n\n        analyzer = cls(\n            data=data,\n            output_dir=settings.output_dir,\n        )\n\n        analyzer.missingness_module = settings.missingness\n        analyzer.outlier_module = settings.outlier\n        analyzer.visualization_module = settings.visualization\n\n        analyzer.settings = settings\n\n        return analyzer\n\n    def run(self) -&gt; None:\n        \"\"\"\n        Runs the analyzer pipeline.\n\n        This function runs the following steps:\n            1. Creates a TableOne summary of the input data.\n            2. Runs the data cleaning modules.\n            3. Runs the visualization module.\n            4. Runs the encoding module.\n            5. Saves the updated data.\n            6. Generates a PDF report of the analysis results.\n            7. Saves the settings to a JSON file.\n        \"\"\"\n\n        # Create Table One\n        self.mytable = TableOne(\n            self.data[self.settings.continuous_columns + self.settings.categorical_columns], \n            categorical=self.settings.categorical_columns, \n            continuous=self.settings.continuous_columns,\n            pval=False\n        )\n        print(self.mytable.tabulate(tablefmt = \"grid\"))\n        self.mytable.to_csv(self.settings.output_dir / 'tableone.csv')\n\n        # Run Data Cleaning\n        self.input_data = self.data.copy()\n        self.data = (\n            self.data\n            .pipe(self.missingness_module)\n            .pipe(self.outlier_module)\n        )\n\n        # Run Visualization\n        figures_dir = self.settings.output_dir / 'figures'\n        figures_dir.mkdir(exist_ok=True, parents=True)\n        self.visualization_module(self.data)\n\n        # Run Encoding\n        self.data = self.encoding_module(self.data)\n\n        # Save Data\n        self.data.to_csv(self.settings.output_dir / 'updated_data.csv', index=False)\n\n        # Generate Report\n        if self.settings.generate_report:\n            multiplots = (\n                [f for f in (figures_dir / 'multiplots').iterdir() if f.suffix == '.png']\n                if (figures_dir / 'multiplots').exists()\n                else []\n            )\n            generate_analysis_report_pdf(\n                outlier_analysis=self.outlier_module.report,\n                multiplots=multiplots,\n                categorical_columns=self.settings.categorical_columns,\n                continuous_columns=self.settings.continuous_columns,\n                output_dir=self.settings.output_dir\n            )\n        else:\n            logger.warning(\"Skipping report generation.\")\n\n        # Save Settings\n        self.settings.settings_schema_path = self.settings.output_dir / 'analyzer_settings.schema.json'\n        with self.settings.settings_schema_path.open(\"w\") as f:\n            json.dump(self.settings.model_json_schema(), f, indent=2)\n\n        self.settings.settings_path = self.settings.output_dir / 'analyzer_settings.json'\n        with self.settings.settings_path.open('w') as f:\n            json.dump({\n                \"$schema\": str(self.settings.settings_schema_path.relative_to(self.settings.output_dir)),\n                **self.settings.model_dump(mode=\"json\") \n            }, f, indent=2)\n\n    def __rich_repr__(self) -&gt; rich.repr.Result:\n        yield self.settings\n</code></pre>"},{"location":"api/analyzer/#jarvais.analyzer.Analyzer.from_settings","title":"<code>from_settings(data, settings_dict)</code>  <code>classmethod</code>","text":"<p>Initialize an Analyzer instance with a given settings dictionary. Settings are validated by pydantic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input data for the analyzer.</p> required <code>settings_dict</code> <code>dict</code> <p>A dictionary containing the analyzer settings.</p> required <p>Returns:</p> Name Type Description <code>Analyzer</code> <code>Analyzer</code> <p>An analyzer instance with the given settings.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the settings dictionary is invalid.</p> Source code in <code>src/jarvais/analyzer/analyzer.py</code> <pre><code>@classmethod\ndef from_settings(\n        cls, \n        data: pd.DataFrame, \n        settings_dict: dict\n    ) -&gt; \"Analyzer\":\n    \"\"\"\n    Initialize an Analyzer instance with a given settings dictionary. Settings are validated by pydantic.\n\n    Args:\n        data (pd.DataFrame): The input data for the analyzer.\n        settings_dict (dict): A dictionary containing the analyzer settings.\n\n    Returns:\n        Analyzer: An analyzer instance with the given settings.\n\n    Raises:\n        ValueError: If the settings dictionary is invalid.\n    \"\"\"\n    try:\n        settings = AnalyzerSettings.model_validate(settings_dict)\n    except Exception as e:\n        raise ValueError(\"Invalid analyzer settings\") from e\n\n    analyzer = cls(\n        data=data,\n        output_dir=settings.output_dir,\n    )\n\n    analyzer.missingness_module = settings.missingness\n    analyzer.outlier_module = settings.outlier\n    analyzer.visualization_module = settings.visualization\n\n    analyzer.settings = settings\n\n    return analyzer\n</code></pre>"},{"location":"api/analyzer/#jarvais.analyzer.Analyzer.run","title":"<code>run()</code>","text":"<p>Runs the analyzer pipeline.</p> This function runs the following steps <ol> <li>Creates a TableOne summary of the input data.</li> <li>Runs the data cleaning modules.</li> <li>Runs the visualization module.</li> <li>Runs the encoding module.</li> <li>Saves the updated data.</li> <li>Generates a PDF report of the analysis results.</li> <li>Saves the settings to a JSON file.</li> </ol> Source code in <code>src/jarvais/analyzer/analyzer.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"\n    Runs the analyzer pipeline.\n\n    This function runs the following steps:\n        1. Creates a TableOne summary of the input data.\n        2. Runs the data cleaning modules.\n        3. Runs the visualization module.\n        4. Runs the encoding module.\n        5. Saves the updated data.\n        6. Generates a PDF report of the analysis results.\n        7. Saves the settings to a JSON file.\n    \"\"\"\n\n    # Create Table One\n    self.mytable = TableOne(\n        self.data[self.settings.continuous_columns + self.settings.categorical_columns], \n        categorical=self.settings.categorical_columns, \n        continuous=self.settings.continuous_columns,\n        pval=False\n    )\n    print(self.mytable.tabulate(tablefmt = \"grid\"))\n    self.mytable.to_csv(self.settings.output_dir / 'tableone.csv')\n\n    # Run Data Cleaning\n    self.input_data = self.data.copy()\n    self.data = (\n        self.data\n        .pipe(self.missingness_module)\n        .pipe(self.outlier_module)\n    )\n\n    # Run Visualization\n    figures_dir = self.settings.output_dir / 'figures'\n    figures_dir.mkdir(exist_ok=True, parents=True)\n    self.visualization_module(self.data)\n\n    # Run Encoding\n    self.data = self.encoding_module(self.data)\n\n    # Save Data\n    self.data.to_csv(self.settings.output_dir / 'updated_data.csv', index=False)\n\n    # Generate Report\n    if self.settings.generate_report:\n        multiplots = (\n            [f for f in (figures_dir / 'multiplots').iterdir() if f.suffix == '.png']\n            if (figures_dir / 'multiplots').exists()\n            else []\n        )\n        generate_analysis_report_pdf(\n            outlier_analysis=self.outlier_module.report,\n            multiplots=multiplots,\n            categorical_columns=self.settings.categorical_columns,\n            continuous_columns=self.settings.continuous_columns,\n            output_dir=self.settings.output_dir\n        )\n    else:\n        logger.warning(\"Skipping report generation.\")\n\n    # Save Settings\n    self.settings.settings_schema_path = self.settings.output_dir / 'analyzer_settings.schema.json'\n    with self.settings.settings_schema_path.open(\"w\") as f:\n        json.dump(self.settings.model_json_schema(), f, indent=2)\n\n    self.settings.settings_path = self.settings.output_dir / 'analyzer_settings.json'\n    with self.settings.settings_path.open('w') as f:\n        json.dump({\n            \"$schema\": str(self.settings.settings_schema_path.relative_to(self.settings.output_dir)),\n            **self.settings.model_dump(mode=\"json\") \n        }, f, indent=2)\n</code></pre>"},{"location":"api/analyzer/#analyzer-modules","title":"Analyzer Modules","text":"<p>The <code>Analyzer</code> class contains the following modules:</p>"},{"location":"api/analyzer/#jarvais.analyzer.modules.MissingnessModule","title":"<code>jarvais.analyzer.modules.MissingnessModule</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/jarvais/analyzer/modules/missingness.py</code> <pre><code>class MissingnessModule(BaseModel):\n\n    categorical_strategy: Dict[str, Literal['unknown', 'knn', 'mode']] = Field(\n        description=\"Missingness strategy for categorical columns.\",\n        title=\"Categorical Strategy\",\n        examples=[{\"gender\": \"unknown\", \"treatment_type\": \"knn\", \"tumor_stage\": \"mode\"}]\n    )\n    continuous_strategy: Dict[str, Literal['mean', 'median', 'mode']] = Field(\n        description=\"Missingness strategy for continuous columns.\",\n        title=\"Continuous Strategy\",\n        examples=[{\"age\": \"median\", \"tumor_size\": \"mean\", \"survival_rate\": \"median\"}]\n    )\n    enabled: bool = Field(\n        default=True,\n        description=\"Whether to perform missingness analysis.\"\n    )\n\n    @classmethod\n    def build(\n            cls, \n            continuous_columns: list[str], \n            categorical_columns: list[str],\n        ) -&gt; \"MissingnessModule\":\n        return cls(\n            continuous_strategy={col: 'median' for col in continuous_columns},\n            categorical_strategy={col: 'unknown' for col in categorical_columns}\n        )\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame: # noqa: PLR0912\n        if not self.enabled:\n            logger.warning(\"Missingness analysis is disabled.\")\n            return df\n\n        logger.info(\"Performing missingness analysis...\")\n\n        df = df.copy()\n\n        # Handle continuous columns\n        for col, cont_strategy in self.continuous_strategy.items():\n            if col not in df.columns:\n                continue\n            if cont_strategy == \"mean\":\n                df[col] = df[col].fillna(df[col].mean())\n            elif cont_strategy == \"median\":\n                df[col] = df[col].fillna(df[col].median())\n            elif cont_strategy == \"mode\":\n                df[col] = df[col].fillna(df[col].mode().iloc[0])\n            else:\n                msg = f\"Unsupported strategy for continuous column: {cont_strategy}\"\n                raise ValueError(msg)\n\n        # Handle categorical columns\n        for col, cat_strategy in self.categorical_strategy.items():\n            if col not in df.columns:\n                continue\n            if cat_strategy == \"unknown\":\n                df[col] = df[col].astype(str).fillna(\"Unknown\").astype(\"category\")\n            elif cat_strategy == \"mode\":\n                df[col] = df[col].fillna(df[col].mode().iloc[0])\n            elif cat_strategy == \"knn\":\n                df = self._knn_impute(df, col)\n            else:\n                df[col] = df[col].fillna(cat_strategy)\n\n        return df\n\n    def _knn_impute(self, df: pd.DataFrame, target_col: str) -&gt; pd.DataFrame:\n        df = df.copy()\n        df_encoded = df.copy()\n\n        # Encode categorical columns for KNN\n        cat_cols = df_encoded.select_dtypes(include=\"category\").columns\n        encoders = {col: {k: v for v, k in enumerate(df_encoded[col].dropna().unique())} for col in cat_cols}\n        for col in cat_cols:\n            df_encoded[col] = df_encoded[col].map(encoders[col])\n\n        df_imputed = pd.DataFrame(\n            KNNImputer(n_neighbors=3).fit_transform(df_encoded),\n            columns=df.columns,\n            index=df.index\n        )\n\n        # Decode imputed categorical column\n        if target_col in encoders:\n            inverse = {v: k for k, v in encoders[target_col].items()}\n            df[target_col] = (\n                df_imputed[target_col]\n                .round()\n                .astype(int)\n                .map(inverse)\n                .astype(\"category\")\n            )\n        else:\n            df[target_col] = df_imputed[target_col]\n\n        return df\n</code></pre>"},{"location":"api/analyzer/#jarvais.analyzer.modules.OutlierModule","title":"<code>jarvais.analyzer.modules.OutlierModule</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/jarvais/analyzer/modules/outlier.py</code> <pre><code>class OutlierModule(BaseModel):\n\n    categorical_strategy: Dict[str, Literal['frequency']] = Field(\n        description=\"Outlier strategy for categorical columns.\",\n        title=\"Categorical Strategy\",\n        examples=[{\"treatment_type\": \"frequency\"}]\n    )\n    continuous_strategy: Dict[str, Literal['none']] = Field(\n        description=\"Outlier strategy for continuous columns (currently unsupported).\",\n        title=\"Continuous Strategy\",\n        examples=[{\"age\": \"none\"}]\n    )\n    threshold: float = Field(\n        default=0.01,\n        description=\"Frequency threshold below which a category is considered an outlier.\",\n        title=\"Threshold\",\n    )\n    enabled: bool = Field(\n        default=True,\n        description=\"Whether to perform outlier analysis.\"\n    )\n\n    _outlier_report: str = PrivateAttr(default=\"\")\n\n    @classmethod\n    def build(\n            cls, \n            categorical_columns: list[str],\n            continuous_columns: list[str] | None = None, \n        ) -&gt; \"OutlierModule\":\n        return cls(\n            categorical_strategy={col: \"frequency\" for col in categorical_columns},\n            continuous_strategy={col: \"none\" for col in continuous_columns} if continuous_columns is not None else {},\n        )\n\n    @property\n    def report(self) -&gt; str:\n        return self._outlier_report\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        if not self.enabled:\n            logger.warning(\"Outlier analysis is disabled.\")\n            return df\n\n        logger.info(\"Performing outlier analysis...\")\n\n        df = df.copy()\n\n        # Handle continuous outliers\n        # for col, strategy in self.continuous_strategy.items(): \n        #     continue\n\n        # Handle categorical outliers\n        for col, strategy in self.categorical_strategy.items():\n            if col not in df.columns or strategy != 'frequency':\n                continue\n\n            value_counts = df[col].value_counts()\n            threshold = int(len(df) * self.threshold)\n            outliers = value_counts[value_counts &lt; threshold].index\n\n            df[col] = df[col].apply(lambda x, outliers=outliers: \"Other\" if x in outliers else x).astype(\"category\")\n\n            if len(outliers) &gt; 0:\n                outliers_msg = [f'{o}: {value_counts[o]} out of {df[col].count()}' for o in outliers]\n                self._outlier_report += f'  - Outliers found in {col}: {outliers_msg}\\n'\n            else:\n                self._outlier_report += f'  - No Outliers found in {col}\\n'\n\n        if self._outlier_report:\n            print(f\"\\nOutlier Report:\\n{self._outlier_report}\")\n\n        return df\n</code></pre>"},{"location":"api/analyzer/#jarvais.analyzer.modules.VisualizationModule","title":"<code>jarvais.analyzer.modules.VisualizationModule</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/jarvais/analyzer/modules/visualization.py</code> <pre><code>class VisualizationModule(BaseModel):\n\n    plots: list[str] = Field(\n        description=\"List of plots to generate.\",\n        title=\"Plots\",\n        examples=[\"corr\", \"pairplot\", \"frequency_table\", \"multiplot\", \"umap\"]\n    )\n    output_dir: str | Path = Field(\n        description=\"Output directory.\",\n        title=\"Output Directory\",\n        examples=[\"output\"],\n        repr=False\n    )\n    continuous_columns: list[str] = Field(\n        description=\"List of continuous columns.\",\n        title=\"Continuous Columns\",\n        examples=[\"age\", \"tumor_size\", \"survival_rate\"],\n        repr=False\n    )\n    categorical_columns: list[str] = Field(\n        description=\"List of categorical columns.\",\n        title=\"Categorical Columns\",\n        examples=[\"gender\", \"treatment_type\", \"tumor_stage\"],\n        repr=False\n    )\n    task: str | None = Field(\n        description=\"Task to perform.\",\n        title=\"Task\",\n        examples=[\"classification\", \"regression\", \"survival\"],\n        repr=False\n    )\n    target_variable: str | None = Field(\n        description=\"Target variable.\",\n        title=\"Target Variable\",\n        examples=[\"death\"],\n        repr=False\n    )\n    enabled: bool = Field(\n        default=True,\n        description=\"Whether to perform visualization.\"\n    )\n\n    _figures_dir: Path = PrivateAttr(default=Path(\".\"))\n    _multiplots: list[str] = PrivateAttr(default_factory=list)\n    _umap_data: np.ndarray | None = PrivateAttr(default=None)\n\n    def model_post_init(self, context: Any) -&gt; None: \n\n        self._figures_dir = Path(self.output_dir) / \"figures\"\n        self._figures_dir.mkdir(exist_ok=True, parents=True)\n\n        plot_order = [\"corr\", \"pairplot\", \"umap\", \"frequency_table\", \"multiplot\", \"kaplan_meier\"]\n        self.plots = [p for p in plot_order if p in self.plots] # Need UMAP before frequency table\n\n    @classmethod\n    def validate_plots(cls, plots: list[str]) -&gt; list[str]:\n        plot_registry = [\"corr\", \"pairplot\", \"frequency_table\", \"multiplot\", \"umap\", \"kaplan_meier\"]\n        invalid = [p for p in plots if p not in plot_registry]\n        if invalid:\n            msg = f\"Invalid plots: {invalid}. Available: {plot_registry}\"\n            raise ValueError(msg)\n        return plots\n\n    @classmethod\n    def build(\n            cls,\n            output_dir: str | Path,\n            continuous_columns: list[str],\n            categorical_columns: list[str],\n            task: str | None,\n            target_variable: str | None\n        ) -&gt; \"VisualizationModule\":\n        plots = [\"corr\", \"pairplot\", \"frequency_table\", \"multiplot\", \"umap\"]\n\n        if task == \"survival\":\n            plots.append(\"kaplan_meier\")\n\n        return cls(plots=plots, \n                   output_dir=output_dir,\n                   continuous_columns=continuous_columns,\n                   categorical_columns=categorical_columns,\n                   task=task,\n                   target_variable=target_variable\n                )   \n\n    def __call__(self, data: pd.DataFrame) -&gt; None:\n        if not self.enabled:\n            logger.warning(\"Visualization is disabled.\")\n            return\n\n        for plot in self.plots:\n            try:\n                match plot:\n                    case \"corr\":\n                        logger.info(\"Plotting Correlation Matrix...\")\n                        self._plot_correlation(data)\n                    case \"pairplot\":\n                        logger.info(\"Plotting Pairplot...\")\n                        self._plot_pairplot(data)\n                    case \"frequency_table\":\n                        logger.info(\"Plotting Frequency Table...\")\n                        plot_frequency_table(data, self.categorical_columns, self._figures_dir)\n                    case \"umap\":\n                        logger.info(\"Plotting UMAP...\")\n                        self._umap_data = plot_umap(data, self.continuous_columns, self._figures_dir)\n                    case \"kaplan_meier\":\n                        logger.info(\"Plotting Kaplan Meier Curves...\")\n                        self._plot_kaplan_meier(data)\n            except Exception as e:\n                logger.info(f\"Skipping {plot} due to error: {e}\")\n\n        if 'multiplot' in self.plots:\n            if self._umap_data is None:\n                raise ValueError(\"Cannot plot multiplot without UMAP data.\")\n\n            logger.info(\"Plotting Multiplot...\")\n            self._plot_multiplot(data)\n\n    def _plot_correlation(self, data: pd.DataFrame) -&gt; None:\n        p_corr = data[self.continuous_columns].corr(method=\"pearson\")\n        s_corr = data[self.continuous_columns].corr(method=\"spearman\")\n        size = 1 + len(self.continuous_columns)*1.2\n        plot_corr(p_corr, size, file_name='pearson_correlation.png', output_dir=self._figures_dir, title=\"Pearson Correlation\")\n        plot_corr(s_corr, size, file_name='spearman_correlation.png', output_dir=self._figures_dir, title=\"Spearman Correlation\")\n\n    def _plot_pairplot(self, data: pd.DataFrame) -&gt; None:\n        if self.target_variable in self.categorical_columns:\n            plot_pairplot(data, self.continuous_columns, output_dir=self._figures_dir, target_variable=self.target_variable)\n        else:\n            plot_pairplot(data, self.continuous_columns, output_dir=self._figures_dir)\n\n    def _plot_multiplot(self, data: pd.DataFrame) -&gt; None:\n        (self._figures_dir / 'multiplots').mkdir(parents=True, exist_ok=True)\n        self._multiplots = Parallel(n_jobs=-1)(\n            delayed(plot_one_multiplot)(\n                data,\n                self._umap_data,\n                var,\n                self.continuous_columns,\n                self._figures_dir\n            ) for var in self.categorical_columns\n        )\n\n    def _plot_kaplan_meier(self, data: pd.DataFrame) -&gt; None:\n        data_x = data.drop(columns=['time', 'event'])\n        data_y = data[['time', 'event']]\n        categorical_columns = [cat for cat in self.categorical_columns if cat != 'event']\n        plot_kaplan_meier_by_category(data_x, data_y, categorical_columns, self._figures_dir / 'kaplan_meier')\n</code></pre>"},{"location":"api/analyzer/#jarvais.analyzer.modules.OneHotEncodingModule","title":"<code>jarvais.analyzer.modules.OneHotEncodingModule</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/jarvais/analyzer/modules/encoding.py</code> <pre><code>class OneHotEncodingModule(BaseModel):\n    columns: list[str] | None = Field(\n        default=None,\n        description=\"List of categorical columns to one-hot encode. If None, all columns are used.\"\n    )\n    target_variable: str | None = Field(\n        default=None,\n        description=\"Target variable to exclude from encoding.\"\n    )\n    prefix_sep: str = Field(\n        default=\"|\",\n        description=\"Prefix separator used in encoded feature names.\"\n    )\n    enabled: bool = Field(\n        default=True,\n        description=\"Whether to perform one-hot encoding.\"\n    )\n\n    @classmethod\n    def build(\n        cls,\n        categorical_columns: list[str],\n        target_variable: str | None = None,\n        prefix_sep: str = \"|\",\n    ) -&gt; \"OneHotEncodingModule\":\n        return cls(\n            columns=[col for col in categorical_columns if col != target_variable],\n            target_variable=target_variable,\n            prefix_sep=prefix_sep\n        )\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        if not self.enabled:\n            logger.warning(\"One-hot encoding is disabled.\")\n            return df\n\n        df = df.copy()\n        return pd.get_dummies(\n            df,\n            columns=self.columns,\n            dtype=float,\n            prefix_sep=self.prefix_sep\n        )\n</code></pre>"},{"location":"api/analyzer/#analyzer-settings","title":"Analyzer Settings","text":"<p>The <code>AnalyzerSettings</code> class is used to configure the <code>Analyzer</code> class.</p>"},{"location":"api/analyzer/#jarvais.analyzer.settings.AnalyzerSettings","title":"<code>jarvais.analyzer.settings.AnalyzerSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/jarvais/analyzer/settings.py</code> <pre><code>class AnalyzerSettings(BaseModel):\n    output_dir: Path = Field(\n        description=\"Output directory.\",\n        title=\"Output Directory\",\n        examples=[\"output\"],\n    )\n    categorical_columns: list[str] = Field(\n        description=\"List of categorical columns.\",\n        title=\"Categorical Columns\",\n        examples=[\"gender\", \"treatment_type\", \"tumor_stage\"],\n    )\n    continuous_columns: list[str] = Field(\n        description=\"List of continuous columns.\",\n        title=\"Continuous Columns\",\n        examples=[\"age\", \"tumor_size\", \"survival_rate\"],\n    )\n    date_columns: list[str] = Field(\n        description=\"List of date columns.\",\n        title=\"Date Columns\",\n        examples=[\"date_of_treatment\"],\n    )\n    task: str | None = Field(\n        description=\"Task to perform.\",\n        title=\"Task\",\n        examples=[\"classification\", \"regression\", \"survival\"],\n    )\n    target_variable: str | None = Field(\n        description=\"Target variable.\",\n        title=\"Target Variable\",\n        examples=[\"death\"],\n    )\n    generate_report: bool = Field(\n        default=True,\n        description=\"Whether to generate a pdf report.\"\n    )\n    settings_path: Path | None = Field(\n        default=None,\n        description=\"Path to settings file.\",\n    )\n    settings_schema_path: Path | None = Field(\n        default=None,\n        description=\"Path to settings schema file.\",\n    )\n\n    missingness: MissingnessModule\n    outlier: OutlierModule\n    encoding: OneHotEncodingModule\n    visualization: VisualizationModule\n\n    def model_post_init(self, context):\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    @classmethod\n    def validate_task(cls, task):\n        if task not in ['classification', 'regression', 'survival', None]:\n            raise ValueError(\"Invalid task parameter. Choose one of: 'classification', 'regression', 'survival'.\")\n        return task\n</code></pre>"},{"location":"api/explainer/","title":"Explainer","text":""},{"location":"api/explainer/#explainer","title":"Explainer","text":"<p>The <code>Explainer</code> class is part of the <code>jarvais.explainer</code> module. It generates explainability reports for trained models.</p> <p>The <code>BiasExplainer</code> class is used by the <code>Explainer</code> class to run a bias audit.</p>"},{"location":"api/explainer/#jarvais.explainer.Explainer","title":"<code>jarvais.explainer.Explainer</code>","text":"<p>A class to generate diagnostic plots and reports for models trained using TrainerSupervised.</p> <p>Attributes:</p> Name Type Description <code>trainer</code> <code>TrainerSupervised</code> <p>The TrainerSupervised object containing the trained model.</p> <code>predictor</code> <code>object</code> <p>The AutoGluon predictor object used for inference.</p> <code>X_train</code> <code>DataFrame</code> <p>The training dataset used to train the model.</p> <code>X_test</code> <code>DataFrame</code> <p>The test dataset for evaluating the model.</p> <code>y_test</code> <code>DataFrame</code> <p>The true target values for the test dataset.</p> <code>output_dir</code> <code>Path</code> <p>The directory where plots, reports, and outputs are saved.</p> <code>sensitive_features</code> <code>list</code> <p>List of features considered sensitive for bias auditing.</p> Source code in <code>src/jarvais/explainer/explainer.py</code> <pre><code>class Explainer():\n    \"\"\"\n    A class to generate diagnostic plots and reports for models trained using TrainerSupervised.\n\n    Attributes:\n        trainer (TrainerSupervised): The TrainerSupervised object containing the trained model.\n        predictor (object): The AutoGluon predictor object used for inference.\n        X_train (pd.DataFrame): The training dataset used to train the model.\n        X_test (pd.DataFrame): The test dataset for evaluating the model.\n        y_test (pd.DataFrame): The true target values for the test dataset.\n        output_dir (Path): The directory where plots, reports, and outputs are saved.\n        sensitive_features (list, optional): List of features considered sensitive for bias auditing.\n    \"\"\"\n    def __init__(\n            self,\n            trainer,\n            X_train: pd.DataFrame,\n            X_test: pd.DataFrame,\n            y_test: pd.DataFrame,\n            output_dir: str | Path | None = None,\n            sensitive_features: list | None = None,\n        ) -&gt; None:\n\n        self.trainer = trainer\n        self.predictor = trainer.predictor\n        self.X_train = X_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.sensitive_features = sensitive_features\n\n        self.output_dir = Path.cwd() if output_dir is None else Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        (self.output_dir / 'figures').mkdir(parents=True, exist_ok=True)\n\n    def run(self) -&gt; None:\n        \"\"\"Generate diagnostic plots and reports for the trained model.\"\"\"\n\n        self._run_bias_audit()\n\n        plot_violin_of_bootstrapped_metrics(\n            self.trainer,\n            self.X_test,\n            self.y_test,\n            self.trainer.X_val,\n            self.trainer.y_val,\n            self.X_train,\n            self.trainer.y_train,\n            output_dir=self.output_dir / 'figures'\n        )            \n\n        if self.trainer.task in ['binary', 'multiclass']:\n            plot_classification_diagnostics(\n                self.y_test,\n                self.predictor.predict_proba(self.X_test).iloc[:, 1],\n                self.trainer.y_val,\n                self.predictor.predict_proba(self.trainer.X_val).iloc[:, 1],\n                self.trainer.y_train,\n                self.predictor.predict_proba(self.X_train).iloc[:, 1],\n                output_dir=self.output_dir / 'figures'\n            )\n            plot_shap_values(\n                self.predictor,\n                self.X_train,\n                self.X_test,\n                output_dir=self.output_dir / 'figures'\n            )\n\n        elif self.trainer.task == 'regression':\n            plot_regression_diagnostics(\n                self.y_test,\n                self.predictor.predict(self.X_test, as_pandas=False),\n                output_dir=self.output_dir / 'figures'\n            )\n\n        # Plot feature importance\n        if self.trainer.task == 'survival': # NEEDS TO BE UPDATED\n            model = self.trainer.predictors['CoxPH']\n            result = permutation_importance(model, self.X_test,\n                                            Surv.from_dataframe('event', 'time', self.y_test),\n                                            n_repeats=15)\n\n            importance_df = pd.DataFrame(\n                {\n                    \"importance\": result[\"importances_mean\"],\n                    \"stddev\": result[\"importances_std\"],\n                },\n                index=self.X_test.columns,\n            ).sort_values(by=\"importance\", ascending=False)\n            model_name = 'CoxPH'\n        else:\n            importance_df = self.predictor.feature_importance(\n                pd.concat([self.X_test, self.y_test], axis=1))\n            model_name = self.predictor.model_best\n\n        plot_feature_importance(importance_df, self.output_dir / 'figures', model_name)\n        generate_explainer_report_pdf(self.trainer.task, self.output_dir)\n\n    def _run_bias_audit(self) -&gt; List[pd.DataFrame]:\n\n        bias_output_dir = self.output_dir / 'bias'\n        bias_output_dir.mkdir(parents=True, exist_ok=True)\n\n        if self.sensitive_features is None:\n            if self.trainer.task == 'survival': # Data needs to be not be one hot encoded\n                self.sensitive_features = infer_sensitive_features(undummify(self.X_test, prefix_sep='|'))\n            else:\n                self.sensitive_features = infer_sensitive_features(self.X_test)\n\n        y_pred = None if self.trainer.task == 'survival' else pd.Series(self.trainer.infer(self.X_test) )\n        metrics = ['mean_prediction'] if self.trainer.task == 'regression' else ['mean_prediction', 'false_positive_rate'] \n\n        bias = BiasExplainer(\n            self.y_test, \n            y_pred, \n            self.sensitive_features,\n            self.trainer.task, \n            bias_output_dir,\n            metrics\n        )\n        bias.run(relative=True)\n\n    @classmethod\n    def from_trainer(cls, trainer, **kwargs):\n        \"\"\"Create Explainer object from TrainerSupervised object.\"\"\"\n        return cls(trainer, trainer.X_train, trainer.X_test, trainer.y_test, trainer.output_dir, **kwargs)\n</code></pre>"},{"location":"api/explainer/#jarvais.explainer.Explainer.run","title":"<code>run()</code>","text":"<p>Generate diagnostic plots and reports for the trained model.</p> Source code in <code>src/jarvais/explainer/explainer.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Generate diagnostic plots and reports for the trained model.\"\"\"\n\n    self._run_bias_audit()\n\n    plot_violin_of_bootstrapped_metrics(\n        self.trainer,\n        self.X_test,\n        self.y_test,\n        self.trainer.X_val,\n        self.trainer.y_val,\n        self.X_train,\n        self.trainer.y_train,\n        output_dir=self.output_dir / 'figures'\n    )            \n\n    if self.trainer.task in ['binary', 'multiclass']:\n        plot_classification_diagnostics(\n            self.y_test,\n            self.predictor.predict_proba(self.X_test).iloc[:, 1],\n            self.trainer.y_val,\n            self.predictor.predict_proba(self.trainer.X_val).iloc[:, 1],\n            self.trainer.y_train,\n            self.predictor.predict_proba(self.X_train).iloc[:, 1],\n            output_dir=self.output_dir / 'figures'\n        )\n        plot_shap_values(\n            self.predictor,\n            self.X_train,\n            self.X_test,\n            output_dir=self.output_dir / 'figures'\n        )\n\n    elif self.trainer.task == 'regression':\n        plot_regression_diagnostics(\n            self.y_test,\n            self.predictor.predict(self.X_test, as_pandas=False),\n            output_dir=self.output_dir / 'figures'\n        )\n\n    # Plot feature importance\n    if self.trainer.task == 'survival': # NEEDS TO BE UPDATED\n        model = self.trainer.predictors['CoxPH']\n        result = permutation_importance(model, self.X_test,\n                                        Surv.from_dataframe('event', 'time', self.y_test),\n                                        n_repeats=15)\n\n        importance_df = pd.DataFrame(\n            {\n                \"importance\": result[\"importances_mean\"],\n                \"stddev\": result[\"importances_std\"],\n            },\n            index=self.X_test.columns,\n        ).sort_values(by=\"importance\", ascending=False)\n        model_name = 'CoxPH'\n    else:\n        importance_df = self.predictor.feature_importance(\n            pd.concat([self.X_test, self.y_test], axis=1))\n        model_name = self.predictor.model_best\n\n    plot_feature_importance(importance_df, self.output_dir / 'figures', model_name)\n    generate_explainer_report_pdf(self.trainer.task, self.output_dir)\n</code></pre>"},{"location":"api/explainer/#jarvais.explainer.Explainer.from_trainer","title":"<code>from_trainer(trainer, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create Explainer object from TrainerSupervised object.</p> Source code in <code>src/jarvais/explainer/explainer.py</code> <pre><code>@classmethod\ndef from_trainer(cls, trainer, **kwargs):\n    \"\"\"Create Explainer object from TrainerSupervised object.\"\"\"\n    return cls(trainer, trainer.X_train, trainer.X_test, trainer.y_test, trainer.output_dir, **kwargs)\n</code></pre>"},{"location":"api/explainer/#jarvais.explainer.BiasExplainer","title":"<code>jarvais.explainer.BiasExplainer</code>","text":"<p>A class for explaining and analyzing bias in a predictive model's outcomes based on sensitive features.</p> <p>This class performs various fairness audits by evaluating predictive outcomes with respect to sensitive features such as gender, age, race, and more. It first runs statistical analyses using the OLS regression F-statistic p-value to assess any possibility  of bias in the model's predictions based on sensitive features. If the p-value is less than 0.05, indicating potential bias,  the class generates visualizations (such as violin plots) and calculates fairness metrics (e.g., demographic parity, equalized odds).  The results are presented for each sensitive feature, with optional relative fairness comparisons.</p> <p>Attributes:</p> Name Type Description <code>y_true</code> <code>DataFrame</code> <p>The true target values for the model.</p> <code>y_pred</code> <code>DataFrame</code> <p>The predicted values from the model.</p> <code>sensitive_features</code> <code>dict or DataFrame</code> <p>A dictionary or DataFrame containing sensitive features used for fairness analysis.</p> <code>metrics</code> <code>list</code> <p>A list of metrics to calculate for fairness analysis. Defaults to ['mean_prediction', 'false_positive_rate', 'true_positive_rate'].</p> <code>mapper</code> <code>dict</code> <p>A dictionary mapping internal metric names to user-friendly descriptions.</p> <code>kwargs</code> <code>dict</code> <p>Additional parameters passed to various methods, such as metric calculation and plot generation.</p> Source code in <code>src/jarvais/explainer/bias.py</code> <pre><code>class BiasExplainer():\n    \"\"\"\n    A class for explaining and analyzing bias in a predictive model's outcomes based on sensitive features.\n\n    This class performs various fairness audits by evaluating predictive outcomes with respect to sensitive features such as\n    gender, age, race, and more. It first runs statistical analyses using the OLS regression F-statistic p-value to assess any possibility \n    of bias in the model's predictions based on sensitive features. If the p-value is less than 0.05, indicating potential bias, \n    the class generates visualizations (such as violin plots) and calculates fairness metrics (e.g., demographic parity, equalized odds). \n    The results are presented for each sensitive feature, with optional relative fairness comparisons.\n\n    Attributes:\n        y_true (pd.DataFrame):\n            The true target values for the model.\n        y_pred (pd.DataFrame):\n            The predicted values from the model.\n        sensitive_features (dict or pd.DataFrame):\n            A dictionary or DataFrame containing sensitive features used for fairness analysis.\n        metrics (list):\n            A list of metrics to calculate for fairness analysis. Defaults to ['mean_prediction', 'false_positive_rate', 'true_positive_rate'].\n        mapper (dict):\n            A dictionary mapping internal metric names to user-friendly descriptions.\n        kwargs (dict):\n            Additional parameters passed to various methods, such as metric calculation and plot generation.\n    \"\"\"\n    def __init__(\n            self, \n            y_true: pd.Series, \n            y_pred: np.ndarray, \n            sensitive_features: dict, \n            task: str,\n            output_dir: Path,\n            metrics: list = ['mean_prediction', 'false_positive_rate', 'true_positive_rate'], \n            **kwargs: dict\n        ) -&gt; None:\n        self.y_true = y_true\n        self.y_pred = y_pred\n        self.task = task\n        self.output_dir = output_dir\n        self.mapper = {\"mean_prediction\": \"Demographic Parity\",\n                       \"false_positive_rate\": \"(FPR) Equalized Odds\",\n                       \"true_positive_rate\": \"(TPR) Equalized Odds or Equal Opportunity\"}\n        self.metrics = metrics\n        self.kwargs = kwargs\n\n        # Convert sensitive_features to DataFrame or leave as Series\n        if isinstance(sensitive_features, pd.DataFrame) or isinstance(sensitive_features, pd.Series):\n            self.sensitive_features = sensitive_features\n        elif isinstance(sensitive_features, dict):\n            self.sensitive_features = pd.DataFrame.from_dict(sensitive_features)\n        elif isinstance(sensitive_features, list):\n            if any(isinstance(item, list) for item in sensitive_features):\n                self.sensitive_features = pd.DataFrame(sensitive_features, columns=[f'sensitive_feature_{i}' for i in range(len(sensitive_features))])\n            else:\n                self.sensitive_features = pd.DataFrame(sensitive_features, columns=['sensitive_feature'])\n        else:\n            raise ValueError(\"sensitive_features must be a pandas DataFrame, Series, dictionary or list\")\n\n    def _generate_violin(self, sensitive_feature: str, bias_metric:np.ndarray) -&gt; None:\n        \"\"\"Generate a violin plot for the bias metric.\"\"\"\n        plt.figure(figsize=(8, 6)) \n        sns.set_theme(style=\"whitegrid\")  \n\n        sns.violinplot(\n            x=self.sensitive_features[sensitive_feature], \n            y=bias_metric, \n            palette=\"muted\",  \n            inner=\"quart\", \n            linewidth=1.25 \n        )\n\n        bias_metric_name = 'log_loss' if self.task == 'binary' else 'root_mean_squared_error'\n\n        plt.title(f'{bias_metric_name.title()} Distribution by {sensitive_feature}', fontsize=16, weight='bold')  \n        plt.xlabel(f'{sensitive_feature}', fontsize=14)  \n        plt.ylabel(f'{bias_metric_name.title()} per Patient', fontsize=14) \n        plt.xticks(rotation=45, ha='right')\n\n        plt.tight_layout()  \n        plt.savefig(self.output_dir / f'{sensitive_feature}_{bias_metric_name}.png') \n        plt.show()\n\n    def _subgroup_analysis_OLS(self, sensitive_feature: str, bias_metric:np.ndarray) -&gt; float:\n        \"\"\"Fit a statsmodels OLS model to the bias metric data, using the sensitive feature and print summary based on p_val.\"\"\"\n        one_hot_encoded = pd.get_dummies(self.sensitive_features[sensitive_feature], prefix=sensitive_feature)\n        X_columns = one_hot_encoded.columns\n\n        X = one_hot_encoded.values  \n        y = bias_metric  \n\n        X = sm.add_constant(X.astype(float), has_constant='add')\n        model = sm.OLS(y, X).fit()\n\n        if model.f_pvalue &lt; 0.05:\n            output = []\n\n            print(f\"\u26a0\ufe0f  **Possible Bias Detected in {sensitive_feature.title()}** \u26a0\ufe0f\\n\")\n            output.append(f\"=== Subgroup Analysis for '{sensitive_feature.title()}' Using OLS Regression ===\\n\")\n\n            output.append(\"Model Statistics:\")\n            output.append(f\"    R-squared:                  {model.rsquared:.3f}\")\n            output.append(f\"    F-statistic:                {model.fvalue:.3f}\")\n            output.append(f\"    F-statistic p-value:        {model.f_pvalue:.4f}\")\n            output.append(f\"    AIC:                        {model.aic:.2f}\")\n            output.append(f\"    Log-Likelihood:             {model.llf:.2f}\")\n\n            summary_df = pd.DataFrame({\n                'Feature': ['const'] + X_columns.tolist(),     # Predictor names (includes 'const' if added)\n                'Coefficient': model.params,    # Coefficients\n                'Standard Error': model.bse     # Standard Errors\n            })\n            table_output = tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False, floatfmt=\".3f\")\n            output.append(\"Model Coefficients:\")\n            output.append('\\n'.join(['    ' + line for line in table_output.split('\\n')]))\n\n            output_text = '\\n'.join(output)\n            print(output_text)\n\n            with open(self.output_dir / f'{sensitive_feature}_Cox_model_summary.txt', 'w') as f:\n                f.write(output_text)\n\n        return model.f_pvalue\n\n    def _subgroup_analysis_CoxPH(self, sensitive_feature: str) -&gt; None:\n        \"\"\"Fit a CoxPH model using the sensitive feature and print summary based on p_val.\"\"\"\n        one_hot_encoded = pd.get_dummies(self.sensitive_features[sensitive_feature], prefix=sensitive_feature)\n        df_encoded = self.y_true.join(one_hot_encoded)\n\n        cph = CoxPHFitter(penalizer=0.0001)\n        cph.fit(df_encoded, duration_col='time', event_col='event')            \n\n        if cph.log_likelihood_ratio_test().p_value &lt; 0.05:\n            output = []\n\n            print(f\"\u26a0\ufe0f  **Possible Bias Detected in {sensitive_feature.title()}** \u26a0\ufe0f\")\n            output.append(f\"=== Subgroup Analysis for '{sensitive_feature.title()}' Using Cox Proportional Hazards Model ===\\n\")\n\n            output.append(\"Model Statistics:\")\n            output.append(f\"    AIC (Partial):               {cph.AIC_partial_:.2f}\")\n            output.append(f\"    Log-Likelihood:              {cph.log_likelihood_:.2f}\")\n            output.append(f\"    Log-Likelihood Ratio p-value: {cph.log_likelihood_ratio_test().p_value:.4f}\")\n            output.append(f\"    Concordance Index (C-index):   {cph.concordance_index_:.2f}\")\n\n            summary_df = pd.DataFrame({\n                'Feature': cph.summary.index.to_list(),\n                'Coefficient': cph.summary['coef'].to_list(),\n                'Standard Error': cph.summary['se(coef)'].to_list()\n            })\n            table_output = tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False, floatfmt=\".3f\")\n            output.append(\"Model Coefficients:\")\n            output.append('\\n'.join(['    ' + line for line in table_output.split('\\n')]))\n\n            output_text = '\\n'.join(output)\n            print(output_text)\n\n            with open(self.output_dir / f'{sensitive_feature}_OLS_model_summary.txt', 'w') as f:\n                f.write(output_text)\n\n    def _calculate_fair_metrics(\n            self, \n            sensitive_feature: str, \n            fairness_threshold: float, \n            relative: bool\n        ) -&gt; pd.DataFrame:\n        \"\"\"Calculate the Fairlearn metrics and return the results in a DataFrame.\"\"\"\n        _metrics = {metric: get_metric(metric, sensitive_features=self.sensitive_features[sensitive_feature]) for metric in self.metrics}\n        metric_frame = fm.MetricFrame(\n            metrics=_metrics, \n            y_true=self.y_true, \n            y_pred=self.y_pred, \n            sensitive_features=self.sensitive_features[sensitive_feature], \n            **self.kwargs\n        )\n        result = pd.DataFrame(metric_frame.by_group.T, index=_metrics.keys())\n        result = result.rename(columns=self.mapper)\n\n        if relative:\n            largest_feature = self.sensitive_features[sensitive_feature].mode().iloc[0]\n            results_relative = result.T / result[largest_feature]\n            results_relative = results_relative.applymap(\n                lambda x: f\"{x:.3f} \u2705\" if x &lt;= fairness_threshold or 1/x &lt;= fairness_threshold \n                else f\"{x:.3f} \u274c\")\n            result = pd.concat([result, results_relative.T.rename(index=lambda x: f\"Relative {x}\")])\n\n        return result\n\n    def run(\n            self, \n            relative: bool = False, \n            fairness_threshold: float = 1.2\n        ) -&gt; None:\n        \"\"\"\n        Runs the bias explainer analysis on the provided data. It first evaluates the potential bias in the model's predictions\n        using the OLS regression F-statistic p-value. If the p-value is below the threshold of 0.05, indicating \n        potential bias in the sensitive feature, the method proceeds to generate visualizations and calculate fairness metrics.\n\n        Args:\n            relative (bool): \n                If True, the metrics will be presented relative to the most frequent value of each sensitive feature.\n            fairness_threshold (float): \n                A threshold for determining fairness based on relative metrics. If the relative metric exceeds this threshold, \n                a warning flag will be applied.\n        \"\"\"\n        if self.task == 'binary':\n            y_true_array = self.y_true.to_numpy()\n            bias_metric = np.array([\n                log_loss([y_true_array[idx]], [self.y_pred[idx]], labels=np.unique(y_true_array))\n                for idx in range(len(y_true_array))\n            ])\n            self.y_pred = (self.y_pred &gt;= .5).astype(int)\n        elif self.task == 'regression':\n            bias_metric = np.sqrt((self.y_true.to_numpy() - self.y_pred) ** 2)\n\n        self.results = []\n        for sensitive_feature in self.sensitive_features.columns:\n            if self.task == 'survival':\n                self._subgroup_analysis_CoxPH(sensitive_feature)\n            else:\n                f_pvalue = self._subgroup_analysis_OLS(sensitive_feature, bias_metric)\n                if f_pvalue &lt; 0.05:\n                    self._generate_violin(sensitive_feature, bias_metric)\n                    result = self._calculate_fair_metrics(sensitive_feature, fairness_threshold, relative)\n\n                    print(f\"\\n=== Subgroup Analysis for '{sensitive_feature.title()}' using FairLearn ===\\n\")\n                    table_output = tabulate(result.iloc[:, :4], headers='keys', tablefmt='grid')\n                    print('\\n'.join(['    ' + line for line in table_output.split('\\n')]), '\\n')\n\n                    result.to_csv(self.output_dir / f'{sensitive_feature}_fm_metrics.csv')\n</code></pre>"},{"location":"api/explainer/#jarvais.explainer.BiasExplainer.run","title":"<code>run(relative=False, fairness_threshold=1.2)</code>","text":"<p>Runs the bias explainer analysis on the provided data. It first evaluates the potential bias in the model's predictions using the OLS regression F-statistic p-value. If the p-value is below the threshold of 0.05, indicating  potential bias in the sensitive feature, the method proceeds to generate visualizations and calculate fairness metrics.</p> <p>Parameters:</p> Name Type Description Default <code>relative</code> <code>bool</code> <p>If True, the metrics will be presented relative to the most frequent value of each sensitive feature.</p> <code>False</code> <code>fairness_threshold</code> <code>float</code> <p>A threshold for determining fairness based on relative metrics. If the relative metric exceeds this threshold,  a warning flag will be applied.</p> <code>1.2</code> Source code in <code>src/jarvais/explainer/bias.py</code> <pre><code>def run(\n        self, \n        relative: bool = False, \n        fairness_threshold: float = 1.2\n    ) -&gt; None:\n    \"\"\"\n    Runs the bias explainer analysis on the provided data. It first evaluates the potential bias in the model's predictions\n    using the OLS regression F-statistic p-value. If the p-value is below the threshold of 0.05, indicating \n    potential bias in the sensitive feature, the method proceeds to generate visualizations and calculate fairness metrics.\n\n    Args:\n        relative (bool): \n            If True, the metrics will be presented relative to the most frequent value of each sensitive feature.\n        fairness_threshold (float): \n            A threshold for determining fairness based on relative metrics. If the relative metric exceeds this threshold, \n            a warning flag will be applied.\n    \"\"\"\n    if self.task == 'binary':\n        y_true_array = self.y_true.to_numpy()\n        bias_metric = np.array([\n            log_loss([y_true_array[idx]], [self.y_pred[idx]], labels=np.unique(y_true_array))\n            for idx in range(len(y_true_array))\n        ])\n        self.y_pred = (self.y_pred &gt;= .5).astype(int)\n    elif self.task == 'regression':\n        bias_metric = np.sqrt((self.y_true.to_numpy() - self.y_pred) ** 2)\n\n    self.results = []\n    for sensitive_feature in self.sensitive_features.columns:\n        if self.task == 'survival':\n            self._subgroup_analysis_CoxPH(sensitive_feature)\n        else:\n            f_pvalue = self._subgroup_analysis_OLS(sensitive_feature, bias_metric)\n            if f_pvalue &lt; 0.05:\n                self._generate_violin(sensitive_feature, bias_metric)\n                result = self._calculate_fair_metrics(sensitive_feature, fairness_threshold, relative)\n\n                print(f\"\\n=== Subgroup Analysis for '{sensitive_feature.title()}' using FairLearn ===\\n\")\n                table_output = tabulate(result.iloc[:, :4], headers='keys', tablefmt='grid')\n                print('\\n'.join(['    ' + line for line in table_output.split('\\n')]), '\\n')\n\n                result.to_csv(self.output_dir / f'{sensitive_feature}_fm_metrics.csv')\n</code></pre>"},{"location":"api/functional/","title":"Functional","text":""},{"location":"api/functional/#functional","title":"Functional","text":""},{"location":"api/functional/#jarvais.utils.functional","title":"<code>jarvais.utils.functional</code>","text":""},{"location":"api/functional/#jarvais.utils.functional.auprc","title":"<code>auprc(y_true, y_scores)</code>","text":"<p>Calculate the Area Under the Precision-Recall Curve (AUPRC).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>True binary labels. Shape (n_samples,).</p> required <code>y_scores</code> <code>ndarray</code> <p>Predicted scores or probabilities. Shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>auprc_score</code> <code>float</code> <p>The AUPRC value.</p> Source code in <code>src/jarvais/utils/functional.py</code> <pre><code>def auprc(y_true: np.ndarray, y_scores: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the Area Under the Precision-Recall Curve (AUPRC).\n\n    Args:\n        y_true (np.ndarray): True binary labels. Shape (n_samples,).\n        y_scores (np.ndarray): Predicted scores or probabilities. Shape (n_samples,).\n\n    Returns:\n        auprc_score (float): The AUPRC value.\n    \"\"\"\n    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n    return auc(recall, precision)\n</code></pre>"},{"location":"api/functional/#jarvais.utils.functional.ci_wrapper","title":"<code>ci_wrapper(y_true, y_pred)</code>","text":"<p>Wrapper for <code>sksurv.metrics.concordance_index_censored</code> to ensure compatibility  with <code>bootstrap_metric</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>A 2D NumPy array of shape (n_samples, 2), where: - <code>y_true[:, 0]</code> represents the observed survival times. - <code>y_true[:, 1]</code> represents the event indicator    (1 if the event occurred, 0 if censored).</p> required <code>y_pred</code> <code>ndarray</code> <p>A 1D NumPy array of predicted risk scores or  survival times. Higher scores typically indicate higher risk.</p> required <p>Returns:</p> Name Type Description <code>concordance_index</code> <code>float</code> <p>The concordance index.</p> Source code in <code>src/jarvais/utils/functional.py</code> <pre><code>def ci_wrapper(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n    \"\"\"\n    Wrapper for `sksurv.metrics.concordance_index_censored` to ensure compatibility \n    with `bootstrap_metric`.\n\n    Args:\n        y_true (np.ndarray): A 2D NumPy array of shape (n_samples, 2), where:\n            - `y_true[:, 0]` represents the observed survival times.\n            - `y_true[:, 1]` represents the event indicator \n              (1 if the event occurred, 0 if censored).\n        y_pred (np.ndarray): A 1D NumPy array of predicted risk scores or \n            survival times. Higher scores typically indicate higher risk.\n\n    Returns:\n        concordance_index (float): The concordance index.\n    \"\"\"\n    time = y_true[:, 0]\n    event = y_true[:, 1]\n\n    return concordance_index_censored(event.astype(bool), time, y_pred)[0]\n</code></pre>"},{"location":"api/functional/#jarvais.utils.functional.bootstrap_metric","title":"<code>bootstrap_metric(y_true, y_pred, metric_func, nsamples=100)</code>","text":"<p>Compute a metric using bootstrapping to estimate its variability.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>True labels. Shape (n_samples,).</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted values. Shape (n_samples,).</p> required <code>metric_func</code> <code>Callable[[ndarray, ndarray], float]</code> <p>A function that calculates the metric.</p> required <code>nsamples</code> <code>int</code> <p>The number of bootstrap samples. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>bootstrapped_values</code> <code>List[float]</code> <p>A list of metric values computed on each bootstrap sample.</p> Source code in <code>src/jarvais/utils/functional.py</code> <pre><code>def bootstrap_metric(\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        metric_func: Callable[[np.ndarray, np.ndarray], float],\n        nsamples: int = 100\n    ) -&gt; List[float]:\n    \"\"\"\n    Compute a metric using bootstrapping to estimate its variability.\n\n    Args:\n        y_true (np.ndarray): True labels. Shape (n_samples,).\n        y_pred (np.ndarray): Predicted values. Shape (n_samples,).\n        metric_func (Callable[[np.ndarray, np.ndarray], float]): A function that calculates the metric.\n        nsamples (int, optional): The number of bootstrap samples. Defaults to 100.\n\n    Returns:\n        bootstrapped_values (List[float]): A list of metric values computed on each bootstrap sample.\n    \"\"\"\n    np.random.seed(0)\n    values = []\n\n    for _ in range(nsamples):\n        idx = np.random.randint(len(y_true), size=len(y_true))\n        pred_sample = y_pred[idx]\n        y_true_sample = y_true[idx]\n        val = metric_func(y_true_sample, pred_sample)\n        values.append(val)\n\n    return values\n</code></pre>"},{"location":"api/functional/#jarvais.utils.functional.undummify","title":"<code>undummify(df, prefix_sep='_')</code>","text":"<p>Undummifies a DataFrame by collapsing dummy/one-hot encoded columns back into their original categorical column.</p> <p>Found here: https://stackoverflow.com/a/62085741</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing dummy/one-hot encoded columns.</p> required <code>prefix_sep</code> <code>str</code> <p>The separator used to distinguish between the prefix (category) and the column name in the dummy columns.  Defaults to \"_\".</p> <code>'_'</code> <p>Returns:</p> Name Type Description <code>undummified_df</code> <code>DataFrame</code> <p>A new DataFrame with the undummified (reconstructed) categorical columns.</p> Source code in <code>src/jarvais/utils/functional.py</code> <pre><code>def undummify(df, prefix_sep=\"_\"):\n    \"\"\"\n    Undummifies a DataFrame by collapsing dummy/one-hot encoded columns back into their original categorical column.\n\n    Found here: https://stackoverflow.com/a/62085741\n\n    Args:\n        df (pandas.DataFrame): The input DataFrame containing dummy/one-hot encoded columns.\n        prefix_sep (str, optional): The separator used to distinguish between the prefix (category) and the column name in the dummy columns. \n            Defaults to \"_\".\n\n    Returns:\n        undummified_df (pandas.DataFrame): A new DataFrame with the undummified (reconstructed) categorical columns.\n    \"\"\"\n    dummy_cols = {\n        item.split(prefix_sep)[0]: (prefix_sep in item) for item in df.columns\n    }\n    series_list = []\n    for col, needs_to_collapse in dummy_cols.items():\n        if needs_to_collapse:\n            undummified = (\n                df.filter(like=col)\n                .idxmax(axis=1)\n                .apply(lambda x: x.split(prefix_sep, maxsplit=1)[1])\n                .rename(col)\n            )\n            series_list.append(undummified)\n        else:\n            series_list.append(df[col])\n    undummified_df = pd.concat(series_list, axis=1)\n    return undummified_df\n</code></pre>"},{"location":"api/functional/#jarvais.utils.functional.process_RADCURE_clinical","title":"<code>process_RADCURE_clinical(df)</code>","text":"<p>Processes RADCURE clinical data.</p> <p>Raw data found here: https://www.cancerimagingarchive.net/collection/radcure/</p> Source code in <code>src/jarvais/utils/functional.py</code> <pre><code>def process_RADCURE_clinical(df):\n    \"\"\"\n    Processes RADCURE clinical data.\n\n    Raw data found here: https://www.cancerimagingarchive.net/collection/radcure/\n    \"\"\"\n    df_converted = pd.DataFrame({\n        'Study ID': df['patient_id'],\n        'survival_time': df['Length FU'],\n        'death': df['Status'].apply(lambda x: 1 if x == 'Dead' else 0),\n        'age at dx': df['Age'],\n        'Sex': df['Sex'],\n        'T Stage': df['T'],\n        'N Stage': df['N'],\n        'Stage': df['Stage'],\n        'Dose': df['Dose'],\n        'Chemotherapy': df['Chemo'].apply(lambda x: 1 if x != 'none' else 0),\n        'HPV Combined': df['HPV'].apply(lambda x: 1 if isinstance(x, str) and 'positive' in x.lower() else None),\n        'Smoking Status': df['Smoking Status'],\n        'Disease Site': df['Ds Site'].str.lower()\n    })\n\n    return df_converted\n</code></pre>"},{"location":"api/pdf/","title":"PDF","text":""},{"location":"api/pdf/#pdf","title":"PDF","text":""},{"location":"api/pdf/#jarvais.utils.pdf","title":"<code>jarvais.utils.pdf</code>","text":""},{"location":"api/pdf/#jarvais.utils.pdf.generate_analysis_report_pdf","title":"<code>generate_analysis_report_pdf(outlier_analysis, multiplots, categorical_columns, continuous_columns, output_dir)</code>","text":"<p>Generate a PDF report for the analysis, including plots, tables, and outlier analysis.</p> <p>Parameters:</p> Name Type Description Default <code>outlier_analysis</code> <code>str</code> <p>Text summary of outlier analysis to include in the report.</p> required <code>multiplots</code> <code>list</code> <p>A list of paths to plots to include in the multiplots section.</p> required <code>categorical_columns</code> <code>list</code> <p>A list of categorical columns to use for multiplots.</p> required <code>continuous_columns</code> <code>list</code> <p>A list of continuous columns to use for multiplots.</p> required <code>output_dir</code> <code>str | Path</code> <p>The directory where the generated PDF report will be saved.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function saves the generated PDF to the specified output directory.</p> Source code in <code>src/jarvais/utils/pdf.py</code> <pre><code>def generate_analysis_report_pdf(\n        outlier_analysis: str,\n        multiplots: list,\n        categorical_columns: list,\n        continuous_columns: list,\n        output_dir: str | Path\n    ) -&gt; None:\n    \"\"\"\n    Generate a PDF report for the analysis, including plots, tables, and outlier analysis.\n\n    Args:\n        outlier_analysis (str): Text summary of outlier analysis to include in the report.\n        multiplots (list): A list of paths to plots to include in the multiplots section.\n        categorical_columns (list): A list of categorical columns to use for multiplots.\n        continuous_columns (list): A list of continuous columns to use for multiplots.\n        output_dir (str | Path): The directory where the generated PDF report will be saved.\n\n    Returns:\n        None: The function saves the generated PDF to the specified output directory.\n    \"\"\"\n    output_dir = Path(output_dir)\n    figures_dir = output_dir / 'figures'\n\n    # Instantiate PDF\n    pdf = FPDF()\n    pdf.add_page()\n    script_dir = Path(__file__).resolve().parent\n\n    # Adding unicode fonts\n    font_path = (script_dir / 'fonts/Inter_28pt-Regular.ttf')\n    pdf.add_font(\"inter\", style=\"\", fname=font_path)\n    font_path = (script_dir / 'fonts/Inter_28pt-Bold.ttf')\n    pdf.add_font(\"inter\", style=\"b\", fname=font_path)\n\n    # Title\n    pdf.set_font('inter', 'B', 36)\n    pdf.cell(text=\"jarvAIs Analyzer Report\\n\\n\", new_y=YPos.NEXT) \n\n    # Add outlier analysis\n    if outlier_analysis != '':\n        pdf.set_y(pdf.get_y() + 10)\n        pdf.set_font('inter', 'B', 24)\n        pdf.cell(text=\"Outlier Analysis\", new_y=YPos.NEXT)\n        pdf = _add_outlier_analysis(pdf, outlier_analysis)\n\n    # Add page-wide pairplots\n    if (figures_dir / 'pairplot.png').exists():\n        pdf.add_page()\n        pdf.set_font('inter', 'B', 24)\n        pdf.cell(h=pdf.t_margin, text='Pair Plot of Continuous Variables', new_y=YPos.NEXT)\n        img = pdf.image((figures_dir / 'pairplot.png'), Align.C, h=pdf.eph*.5)\n        pdf.set_y(pdf.t_margin + img.rendered_height + 25)\n\n    # Add correlation plots\n    if (figures_dir / 'pearson_correlation.png').exists() and (figures_dir / 'spearman_correlation.png').exists():\n        pdf.set_font('inter', 'B', 24)\n        pdf.cell(h=pdf.t_margin, text='Pearson and Spearman Correlation Plots', new_y=YPos.NEXT)\n\n        corr_y = pdf.get_y() + 5\n\n        pdf.image((figures_dir / 'pearson_correlation.png'), Align.L, corr_y, w=pdf.epw*.475)\n        pdf.image((figures_dir / 'spearman_correlation.png'), Align.R, corr_y, w=pdf.epw*.475)\n\n    # Add multiplots\n    if multiplots and categorical_columns:\n        pdf = _add_multiplots(pdf, multiplots, categorical_columns, continuous_columns)\n\n    # Add demographic breakdown \"table one\"\n    path_tableone = output_dir / 'tableone.csv'\n    if path_tableone.exists():\n        try:\n            csv_df = pd.read_csv(path_tableone, na_filter=False).astype(str)\n            pdf = _add_tableone(pdf, csv_df)\n        except Exception as e:\n            logger.warning(f\"Unable to add table one to analysis report: {e}\")\n\n    # Save PDF\n    pdf.output(output_dir / 'analysis_report.pdf')\n</code></pre>"},{"location":"api/pdf/#jarvais.utils.pdf.generate_explainer_report_pdf","title":"<code>generate_explainer_report_pdf(problem_type, output_dir)</code>","text":"<p>Generate a PDF report for the explainer with visualizations and metrics.</p> <p>This function creates a PDF report that includes plots and metrics  relevant to the specified problem type. The report is saved in the  specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>problem_type</code> <code>str</code> <p>The type of machine learning problem.  Supported values are 'binary', 'multiclass', 'regression',  and 'survival'.</p> required <code>output_dir</code> <code>str | Path</code> <p>The directory where the generated PDF  report will be saved.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function saves the generated PDF to the specified output directory.</p> Source code in <code>src/jarvais/utils/pdf.py</code> <pre><code>def generate_explainer_report_pdf(\n        problem_type: str,\n        output_dir: str | Path\n    ) -&gt; None:\n    \"\"\"\n    Generate a PDF report for the explainer with visualizations and metrics.\n\n    This function creates a PDF report that includes plots and metrics \n    relevant to the specified problem type. The report is saved in the \n    specified output directory.\n\n    Args:\n        problem_type (str): The type of machine learning problem. \n            Supported values are 'binary', 'multiclass', 'regression', \n            and 'survival'.\n        output_dir (str | Path): The directory where the generated PDF \n            report will be saved.\n\n    Returns:\n        None: The function saves the generated PDF to the specified output directory.\n    \"\"\"\n    output_dir = Path(output_dir)\n    figures_dir = output_dir / 'figures'\n\n    # Instantiate PDF\n    pdf = FPDF()\n    pdf.add_page()\n    script_dir = Path(__file__).resolve().parent\n\n    # Adding unicode fonts\n    font_path = (script_dir / 'fonts/Inter_28pt-Regular.ttf')\n    pdf.add_font(\"inter\", style=\"\", fname=font_path)\n    font_path = (script_dir / 'fonts/Inter_28pt-Bold.ttf')\n    pdf.add_font(\"inter\", style=\"b\", fname=font_path)\n    pdf.set_font('inter', '', 24)\n\n    # Title\n    pdf.write(5, \"Explainer Report\\n\\n\")\n\n    pdf.image((figures_dir / 'test_metrics_bootstrap.png'), Align.C, h=pdf.eph//3.5, w=pdf.epw-20)\n    pdf.image((figures_dir / 'validation_metrics_bootstrap.png'), Align.C, h=pdf.eph//3.5, w=pdf.epw-20)\n    pdf.image((figures_dir /  'train_metrics_bootstrap.png'), Align.C, h=pdf.eph//3.5, w=pdf.epw-20)\n    pdf.add_page()\n\n    pdf.image((figures_dir / 'feature_importance.png'), Align.C, w=pdf.epw-20)\n    pdf.add_page()\n\n    if problem_type in ['binary', 'multiclass']:\n        pdf.image((figures_dir / 'model_evaluation.png'), Align.C, w=pdf.epw-20)\n        pdf.image((figures_dir / 'confusion_matrix.png'), Align.C, h=pdf.eph/2, w=pdf.epw-20)\n        pdf.add_page()\n\n        pdf.image((figures_dir / 'shap_barplot.png'), Align.C, h=pdf.eph/2, w=pdf.epw-20)\n        pdf.image((output_dir /  'figures' / 'shap_heatmap.png'), Align.C, h=pdf.eph/2, w=pdf.epw-20)\n    elif problem_type == 'regression':\n        pdf.image((figures_dir / 'residual_plot.png'), Align.C, h=pdf.eph/2, w=pdf.epw-20)\n        pdf.image((output_dir /  'figures' / 'true_vs_predicted.png'), Align.C, h=pdf.eph/2, w=pdf.epw-20)\n\n    # Save PDF\n    pdf.output((output_dir / 'explainer_report.pdf'))\n</code></pre>"},{"location":"api/plot/","title":"Plotting","text":""},{"location":"api/plot/#plot","title":"Plot","text":""},{"location":"api/plot/#jarvais.utils.plot","title":"<code>jarvais.utils.plot</code>","text":""},{"location":"api/plot/#jarvais.utils.plot.plot_corr","title":"<code>plot_corr(corr, size, output_dir, file_name='correlation_matrix.png', title='Correlation Matrix')</code>","text":"<p>Plots a lower-triangle heatmap of the correlation matrix and saves it as an image file.</p> <p>Parameters:</p> Name Type Description Default <code>corr</code> <code>DataFrame</code> <p>Correlation matrix to visualize.</p> required <code>size</code> <code>float</code> <p>Size of the heatmap figure.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the output image.</p> required <code>file_name</code> <code>str</code> <p>Name of the saved image file. Defaults to 'correlation_matrix.png'.</p> <code>'correlation_matrix.png'</code> Example <pre><code>import pandas as pd\nfrom pathlib import Path\n\n# Sample data\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [5, 4, 3, 2, 1],\n    'C': [2, 3, 4, 5, 6]\n})\n\n# Compute Spearman correlation\ncorr_matrix = df.corr(method='spearman')\n\n# Plot and save the heatmap\nplot_corr(corr=corr_matrix, size=6, output_dir=Path('./output'))\n</code></pre> Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_corr(\n        corr: pd.DataFrame,\n        size: float,\n        output_dir: Path,\n        file_name: str = 'correlation_matrix.png',\n        title: str = \"Correlation Matrix\"\n    ) -&gt; None:\n    \"\"\"\n    Plots a lower-triangle heatmap of the correlation matrix and saves it as an image file.\n\n    Args:\n        corr (pd.DataFrame): Correlation matrix to visualize.\n        size (float): Size of the heatmap figure.\n        output_dir (Path): Directory to save the output image.\n        file_name (str): Name of the saved image file. Defaults to 'correlation_matrix.png'.\n\n    Example:\n        ```python\n        import pandas as pd\n        from pathlib import Path\n\n        # Sample data\n        df = pd.DataFrame({\n            'A': [1, 2, 3, 4, 5],\n            'B': [5, 4, 3, 2, 1],\n            'C': [2, 3, 4, 5, 6]\n        })\n\n        # Compute Spearman correlation\n        corr_matrix = df.corr(method='spearman')\n\n        # Plot and save the heatmap\n        plot_corr(corr=corr_matrix, size=6, output_dir=Path('./output'))\n        ```\n    \"\"\"\n    fig, ax = plt.subplots(1, 1, figsize=(size*1.2, size))\n    mask = np.triu(np.ones_like(corr, dtype=bool)) # Keep only lower triangle\n    np.fill_diagonal(mask, False)\n    sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidth=.5, fmt=\"1.2f\", ax=ax)\n    plt.title(title)\n    plt.tight_layout()\n\n    figure_path = output_dir / file_name\n    fig.savefig(figure_path)\n    plt.close()\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_frequency_table","title":"<code>plot_frequency_table(data, columns, output_dir)</code>","text":"<p>Generates and saves heatmap visualizations for frequency tables of all column pair combinations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input dataset containing the columns to analyze.</p> required <code>columns</code> <code>list</code> <p>List of column names to create frequency tables for.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the generated heatmaps.</p> required Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot(plot_type='ft')\ndef plot_frequency_table(\n        data: pd.DataFrame,\n        columns: list,\n        output_dir: Path\n    ) -&gt; None:\n    \"\"\"\n    Generates and saves heatmap visualizations for frequency tables of all column pair combinations.\n\n    Args:\n        data (pd.DataFrame): Input dataset containing the columns to analyze.\n        columns (list): List of column names to create frequency tables for.\n        output_dir (Path): Directory to save the generated heatmaps.\n    \"\"\"\n    frequency_dir = Path(output_dir) / 'frequency_tables'\n    frequency_dir.mkdir(parents=True, exist_ok=True)\n\n    for column_1, column_2 in combinations(columns, 2):\n        heatmap_data = pd.crosstab(data[column_1], data[column_2])\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt='d', linewidth=.5)\n        plt.title(f'Frequency Table for {column_1} and {column_2}')\n        plt.xlabel(column_2)\n        plt.ylabel(column_1)\n        plt.savefig(frequency_dir / f'{column_1}_vs_{column_2}.png')\n        plt.close()\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_pairplot","title":"<code>plot_pairplot(data, continuous_columns, output_dir, target_variable=None, n_keep=10)</code>","text":"<p>Generates a pair plot of the specified continuous columns in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be visualized.</p> required <code>continuous_columns</code> <code>list</code> <p>A list of column names corresponding to continuous variables.</p> required <code>output_dir</code> <code>Path</code> <p>Directory where the resulting plot will be saved.</p> required <code>target_variable</code> <code>str</code> <p>The target variable to use as a hue for coloring the pair plot. Defaults to None.</p> <code>None</code> <code>n_keep</code> <code>int</code> <p>The maximum number of continuous columns to include in the plot.  If exceeded, the most correlated columns are selected. Defaults to 10.</p> <code>10</code> Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_pairplot(\n        data: pd.DataFrame,\n        continuous_columns: list,\n        output_dir: Path,\n        target_variable: str = None,\n        n_keep: int = 10\n    ) -&gt; None:\n    \"\"\"\n    Generates a pair plot of the specified continuous columns in the dataset.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the data to be visualized.\n        continuous_columns (list): A list of column names corresponding to continuous variables.\n        output_dir (Path): Directory where the resulting plot will be saved.\n        target_variable (str, optional): The target variable to use as a hue for coloring the pair plot. Defaults to None.\n        n_keep (int, optional): The maximum number of continuous columns to include in the plot. \n            If exceeded, the most correlated columns are selected. Defaults to 10.\n    \"\"\"\n    if len(continuous_columns) &gt; n_keep:\n        spearman_corr = data[continuous_columns].corr(method=\"spearman\") \n        corr_pairs = spearman_corr.abs().unstack().sort_values(\n            kind=\"quicksort\",\n            ascending=False\n        ).drop_duplicates()\n        top_10_pairs = corr_pairs[corr_pairs &lt; 1].nlargest(5)\n        columns_to_plot = list({index for pair in top_10_pairs.index for index in pair})\n    else:\n        columns_to_plot = continuous_columns.copy()\n\n    hue = target_variable\n    if target_variable is not None:\n        columns_to_plot += [target_variable]\n\n    sns.set_theme(style=\"darkgrid\")\n    g = sns.pairplot(data[columns_to_plot], hue=hue)\n    g.figure.suptitle(\"Pair Plot\", y=1.08)\n\n    figure_path = output_dir / 'pairplot.png'\n    plt.savefig(figure_path)\n    plt.close()\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_umap","title":"<code>plot_umap(data, continuous_columns, output_dir)</code>","text":"<p>Generates a 2D UMAP projection of the specified continuous columns and saves the resulting scatter plot.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame containing the data to be visualized.</p> required <code>continuous_columns</code> <code>list</code> <p>A list of column names corresponding to continuous variables  to be included in the UMAP projection.</p> required <code>output_dir</code> <code>Path</code> <p>Directory where the resulting plot will be saved.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D NumPy array of the UMAP-transformed data.</p> Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_umap(\n        data: pd.DataFrame,\n        continuous_columns: list,\n        output_dir: Path\n    ) -&gt; np.ndarray:\n    \"\"\"\n    Generates a 2D UMAP projection of the specified continuous columns and saves the resulting scatter plot.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the data to be visualized.\n        continuous_columns (list): A list of column names corresponding to continuous variables \n            to be included in the UMAP projection.\n        output_dir (Path): Directory where the resulting plot will be saved.\n\n    Returns:\n        np.ndarray: A 2D NumPy array of the UMAP-transformed data.\n    \"\"\"\n    umap_data = UMAP(n_components=2).fit_transform(data[continuous_columns])\n\n    plt.figure(figsize=(8, 8))\n    sns.scatterplot(x=umap_data[:,0], y=umap_data[:,1], alpha=.7)\n    plt.title('UMAP of Continuous Variables')\n    plt.savefig(output_dir / 'umap_continuous_data.png')\n    plt.close()\n\n    return umap_data\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_kaplan_meier_by_category","title":"<code>plot_kaplan_meier_by_category(data_x, data_y, categorical_columns, output_dir)</code>","text":"<p>Plots Kaplan-Meier survival curves for each category in the specified categorical columns.</p> <p>Parameters:</p> Name Type Description Default <code>data_x</code> <code>DataFrame</code> <p>Dataset containing the categorical columns to group by.</p> required <code>data_y</code> <code>DataFrame</code> <p>Dataset containing 'time' and 'event' columns for survival analysis.</p> required <code>categorical_columns</code> <code>list</code> <p>List of categorical column names to generate survival curves for.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the Kaplan-Meier survival curve plots.</p> required Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_kaplan_meier_by_category(\n        data_x: pd.DataFrame,\n        data_y: pd.DataFrame,\n        categorical_columns: list,\n        output_dir: Path\n    ) -&gt; None:\n    \"\"\"\n    Plots Kaplan-Meier survival curves for each category in the specified categorical columns.\n\n    Args:\n        data_x (pd.DataFrame): Dataset containing the categorical columns to group by.\n        data_y (pd.DataFrame): Dataset containing 'time' and 'event' columns for survival analysis.\n        categorical_columns (list): List of categorical column names to generate survival curves for.\n        output_dir (Path): Directory to save the Kaplan-Meier survival curve plots.\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    for cat_col in categorical_columns:\n        plt.figure(figsize=(10, 6))\n        plt.title(f\"Kaplan-Meier Survival Curve by {cat_col}\")\n\n        unique_categories = data_x[cat_col].unique()\n\n        # Plot survival curves for each category\n        for category in unique_categories:\n            mask_category = data_x[cat_col] == category\n            try: # To catch when there are not enough samples for category\n                time_category, survival_prob_category, conf_int = kaplan_meier_estimator(\n                    data_y[\"event\"][mask_category].astype(bool),\n                    data_y[\"time\"][mask_category],\n                    conf_type=\"log-log\",\n                )\n\n                plt.step(\n                    time_category,\n                    survival_prob_category,\n                    where=\"post\",\n                    label=f\"{cat_col} = {category}\"\n                )\n                plt.fill_between(\n                    time_category,\n                    conf_int[0],\n                    conf_int[1],\n                    alpha=0.25,\n                    step=\"post\"\n                )\n            except Exception as _:\n                pass\n\n        results_multivariate = multivariate_logrank_test(\n            data_y['time'], \n            data_x[cat_col], \n            data_y['event']\n        )\n        multivariate_p_value = results_multivariate.p_value\n\n        plt.text(0.6, 0.1, f\"Multivariate log-rank p-value: {multivariate_p_value:.4e}\",\n                 fontsize=10, transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8))\n        plt.ylim(0, 1)\n        plt.ylabel(r\"Estimated Probability of Survival $\\hat{S}(t)$\")\n        plt.xlabel(\"Time $t$\")\n        plt.legend(loc=\"best\")\n        plt.grid(alpha=0.3)\n        plt.savefig(output_dir / f'kaplan_meier_{cat_col}.png')\n        plt.close()\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_feature_importance","title":"<code>plot_feature_importance(df, output_dir, model_name='')</code>","text":"<p>Plots feature importance with standard deviation and p-value significance.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the feature importance data.  Look at example for required format.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the feature importance plot.</p> required <code>model_name</code> <code>str</code> <p>Optional name of the model, included in the plot title.</p> <code>''</code> Example <pre><code>import pandas as pd\nfrom pathlib import Path\n\ndf = pd.DataFrame({\n    'importance': [0.25, 0.18, 0.12, 0.10],\n    'stddev': [0.03, 0.02, 0.01, 0.015],\n    'p_value': [0.03, 0.07, 0.01, 0.2]\n}, index=['Feature A', 'Feature B', 'Feature C', 'Feature D'])\n\nplot_feature_importance(df, Path('./output'))\n</code></pre> Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_feature_importance(df: pd.DataFrame, output_dir: Path, model_name: str=''):\n    \"\"\"\n    Plots feature importance with standard deviation and p-value significance.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the feature importance data. \n            Look at example for required format.\n        output_dir (Path): Directory to save the feature importance plot.\n        model_name (str): Optional name of the model, included in the plot title.\n\n    Example:\n        ```python\n        import pandas as pd\n        from pathlib import Path\n\n        df = pd.DataFrame({\n            'importance': [0.25, 0.18, 0.12, 0.10],\n            'stddev': [0.03, 0.02, 0.01, 0.015],\n            'p_value': [0.03, 0.07, 0.01, 0.2]\n        }, index=['Feature A', 'Feature B', 'Feature C', 'Feature D'])\n\n        plot_feature_importance(df, Path('./output'))\n        ```\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(20, 12), dpi=72)\n\n    bars = ax.bar(df.index, df['importance'], yerr=df['stddev'], capsize=5, color='skyblue', edgecolor='black')\n\n    if 'p_value' in df.columns:\n        for bar, p_value in zip(bars, df['p_value']):\n            height = bar.get_height()\n            significance = '*' if p_value &lt; 0.05 else ''\n            ax.text(bar.get_x() + bar.get_width() / 2.0, height + 0.002, significance,\n                    ha='center', va='bottom', fontsize=10, color='red')\n\n    ax.set_xlabel('Feature', fontsize=14)\n    ax.set_ylabel('Importance', fontsize=14)\n    ax.set_title(f'Feature Importance with Standard Deviation and p-value Significance ({model_name})', fontsize=16)\n    ax.axhline(0, color='grey', linewidth=0.8)\n\n    ax.set_xticks(np.arange(len(df.index.values)))\n    ax.set_xticklabels(df.index.values, rotation=60, ha='right', fontsize=10)\n\n    ax.grid(axis='y', linestyle='--', alpha=0.7)\n\n    significance_patch = plt.Line2D([0], [0], color='red', marker='*', linestyle='None', markersize=10, label='p &lt; 0.05')\n    ax.legend(handles=[significance_patch], loc='upper right', fontsize=12)\n\n    plt.tight_layout()\n    fig.savefig(output_dir / 'feature_importance.png')\n    plt.close()\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_shap_values","title":"<code>plot_shap_values(predictor, X_train, X_test, output_dir, max_display=10)</code>","text":"<p>Generates and saves SHAP value visualizations, including a heatmap and a bar plot, for a autogluon tabular model.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>TabularPredictor</code> <p>The trained tabular predictor model for which SHAP values are calculated.</p> required <code>X_train</code> <code>DataFrame</code> <p>Training dataset used to create the SHAP background data.</p> required <code>X_test</code> <code>DataFrame</code> <p>Test dataset used to evaluate and compute SHAP values.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the SHAP value visualizations.</p> required <code>max_display</code> <code>int</code> <p>Maximum number of features to display in the visualizations. Defaults to 10.</p> <code>10</code> Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_shap_values(\n        predictor: TabularPredictor,\n        X_train: pd.DataFrame,\n        X_test: pd.DataFrame,\n        output_dir: Path,\n        max_display: int = 10,\n    ) -&gt; None:\n    \"\"\"\n    Generates and saves SHAP value visualizations, including a heatmap and a bar plot, for a autogluon tabular model.\n\n    Args:\n        predictor (TabularPredictor): The trained tabular predictor model for which SHAP values are calculated.\n        X_train (pd.DataFrame): Training dataset used to create the SHAP background data.\n        X_test (pd.DataFrame): Test dataset used to evaluate and compute SHAP values.\n        output_dir (Path): Directory to save the SHAP value visualizations.\n        max_display (int): Maximum number of features to display in the visualizations. Defaults to 10.\n    \"\"\"\n    predictor = ModelWrapper(predictor, X_train.columns)\n    background_data = shap.sample(X_train, 100)\n    shap_exp = shap.KernelExplainer(predictor.predict_proba, background_data)\n\n    # sample 100 samples from test set to evaluate with shap values\n    test_data = shap.sample(X_test, 100)\n\n    # Compute SHAP values for the test set\n    shap_values = shap_exp(test_data)\n\n    sns.set_theme(style=\"darkgrid\")\n    fig, ax = plt.subplots(figsize=(20, 12), dpi=72)\n    shap.plots.heatmap(shap_values[...,1], max_display=max_display, show=False, ax=ax)\n    fig.savefig(output_dir / 'shap_heatmap.png')\n    plt.close()\n\n    fig, ax = plt.subplots(figsize=(20, 12), dpi=72)\n    shap.plots.bar(shap_values[...,1], max_display=max_display, show=False, ax=ax)\n    fig.savefig(output_dir / 'shap_barplot.png')\n    plt.close()\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_violin_of_bootstrapped_metrics","title":"<code>plot_violin_of_bootstrapped_metrics(trainer, X_test, y_test, X_val, y_val, X_train, y_train, output_dir)</code>","text":"<p>Generates violin plots for bootstrapped model performance metrics across train, validation, and test datasets.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>TrainerSupervised</code> <p>The trained model predictor for evaluating performance metrics.</p> required <code>X_test</code> <code>Series</code> <p>Test features dataset.</p> required <code>y_test</code> <code>Series</code> <p>Test target values.</p> required <code>X_val</code> <code>Series</code> <p>Validation features dataset.</p> required <code>y_val</code> <code>Series</code> <p>Validation target values.</p> required <code>X_train</code> <code>Series</code> <p>Training features dataset.</p> required <code>y_train</code> <code>Series</code> <p>Training target values.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the generated violin plots.</p> required Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>def plot_violin_of_bootstrapped_metrics(\n        trainer,\n        X_test: pd.Series,\n        y_test: pd.Series,\n        X_val: pd.Series,\n        y_val: pd.Series,\n        X_train: pd.Series,\n        y_train: pd.Series,\n        output_dir: Path\n    ) -&gt; None:\n    \"\"\"\n    Generates violin plots for bootstrapped model performance metrics across train, validation, and test datasets.\n\n    Args:\n        trainer (TrainerSupervised): The trained model predictor for evaluating performance metrics.\n        X_test (pd.Series): Test features dataset.\n        y_test (pd.Series): Test target values.\n        X_val (pd.Series): Validation features dataset.\n        y_val (pd.Series): Validation target values.\n        X_train (pd.Series): Training features dataset.\n        y_train (pd.Series): Training target values.\n        output_dir (Path): Directory to save the generated violin plots.\n    \"\"\"\n    # Define metrics based on the problem type\n    if trainer.task == 'regression':\n        metrics = [('R Squared', r2_score), ('Root Mean Squared Error', root_mean_squared_error)]\n    elif trainer.task == 'binary':\n        metrics = [('AUROC', roc_auc_score), ('AUPRC', auprc)]\n    elif trainer.task == 'survival':\n        metrics = [('Concordance Index', ci_wrapper)]\n\n    # Prepare lists for DataFrame\n    results = []\n\n    # Loop through models and metrics to compute bootstrapped values\n    for model_name in trainer.model_names():\n        y_pred_test = trainer.infer(X_test, model=model_name)\n        y_pred_val = trainer.infer(X_val, model=model_name)\n        y_pred_train = trainer.infer(X_train, model=model_name)\n\n        for metric_name, metric_func in metrics:\n            test_values = bootstrap_metric(y_test.to_numpy(), y_pred_test, metric_func)\n            results.extend([(model_name, metric_name, 'Test', value) for value in test_values])\n\n            val_values = bootstrap_metric(y_val.to_numpy(), y_pred_val, metric_func)\n            results.extend([(model_name, metric_name, 'Validation', value) for value in val_values])\n\n            train_values = bootstrap_metric(y_train.to_numpy(), y_pred_train, metric_func)\n            results.extend([(model_name, metric_name, 'Train', value) for value in train_values])\n\n    # Create a results DataFrame\n    result_df = pd.DataFrame(results, columns=['model', 'metric', 'data_split', 'value'])\n\n     # Sort models by median metric value within each combination of metric and data_split\n    model_order_per_split = {}\n    for split in ['Test', 'Validation', 'Train']:\n        split_order = (\n            result_df[result_df['data_split'] == split]\n            .groupby(['metric', 'model'])['value']\n            .median()\n            .reset_index()\n            .sort_values(by=['metric', 'value'], ascending=[True, False])\n            .groupby('metric')['model']\n            .apply(list)\n            .to_dict()\n        )\n        model_order_per_split[split] = split_order\n\n    # Function to create violin plots for a specific data split\n    def create_violin_plot(data_split, save_path):\n        sns.set_theme(style=\"darkgrid\")\n        subset = result_df[result_df['data_split'] == data_split]\n        g = sns.FacetGrid(\n            subset,\n            col=\"metric\",\n            margin_titles=True,\n            height=4,\n            aspect=1.5,\n            sharex=False,\n        )\n\n        # Create violin plots with sorted models\n        def violin_plot(data, **kwargs):\n            metric = data.iloc[0]['metric']\n            order = model_order_per_split[data_split].get(metric, None)\n            sns.violinplot(data=data, x=\"value\", y=\"model\", linewidth=1, order=order, **kwargs)\n\n        g.map_dataframe(violin_plot)\n\n        # Adjust the titles and axis labels\n        g.set_titles(col_template=\"{col_name}\")\n        g.set_axis_labels(\"\", \"Model\")\n\n        # Add overall title and adjust layout\n        g.figure.suptitle(f\"Model Performance of {data_split} Data (Bootstrapped)\", fontsize=16)\n        g.tight_layout(w_pad=0.5, h_pad=1)\n\n        # Save the plot\n        g.savefig(save_path, dpi=500)\n        plt.close()\n\n    # Generate and save plots for each data split\n    create_violin_plot('Test', output_dir / 'test_metrics_bootstrap.png')\n    create_violin_plot('Validation', output_dir / 'validation_metrics_bootstrap.png')\n    create_violin_plot('Train', output_dir / 'train_metrics_bootstrap.png')\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_regression_diagnostics","title":"<code>plot_regression_diagnostics(y_true, y_pred, output_dir)</code>","text":"<p>Generates diagnostic plots for evaluating a regression model.</p> Plots <ul> <li>True vs. Predicted values plot.</li> <li>Residuals plot.</li> <li>Histogram of residuals.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>Array of true target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>Array of predicted values from the regression model.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the diagnostic plots.</p> required Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_regression_diagnostics(\n        y_true: np.ndarray, \n        y_pred: np.ndarray, \n        output_dir: Path\n    ) -&gt; None:\n    \"\"\"\n    Generates diagnostic plots for evaluating a regression model.\n\n    Plots:\n        - True vs. Predicted values plot.\n        - Residuals plot.\n        - Histogram of residuals.\n\n    Args:\n        y_true (np.ndarray): Array of true target values.\n        y_pred (np.ndarray): Array of predicted values from the regression model.\n        output_dir (Path): Directory to save the diagnostic plots.\n    \"\"\"\n    residuals = y_true - y_pred\n\n    # Regression Line\n    plt.figure(figsize=(10, 6))\n    sns.set_theme(style=\"darkgrid\")\n    sns.scatterplot(x=y_true, y=y_pred, alpha=0.5)\n    sns.lineplot(x=y_true, y=y_true, color='red')  # Perfect prediction line\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title('True vs Predicted Values')\n    plt.savefig(output_dir / 'true_vs_predicted.png')\n    plt.close()\n\n    # Residuals\n    plt.figure(figsize=(10, 6))\n    sns.set_theme(style=\"darkgrid\")\n    sns.scatterplot(x=y_pred, y=residuals, alpha=0.5)\n    plt.axhline(0, color='red', linestyle='--')\n    plt.xlabel('Fitted Values')\n    plt.ylabel('Residuals')\n    plt.title('Residual Plot')\n    plt.savefig(output_dir / 'residual_plot.png')\n    plt.close()\n\n    # Residual Histogram\n    plt.figure(figsize=(10, 6))\n    sns.set_theme(style=\"darkgrid\")\n    sns.histplot(residuals, kde=True, bins=30)\n    plt.xlabel('Residuals')\n    plt.title('Histogram of Residuals')\n    plt.savefig(output_dir / 'residual_hist.png')\n    plt.close()\n</code></pre>"},{"location":"api/plot/#jarvais.utils.plot.plot_classification_diagnostics","title":"<code>plot_classification_diagnostics(y_test, y_test_pred, y_val, y_val_pred, y_train, y_train_pred, output_dir)</code>","text":"<p>Generates diagnostic plots for evaluating a classification model.</p> Plots <ul> <li>Epic model evaluation plots (ROC Curve, Precision-Recall Curve, Calibration Curve, Sensitivity/Flag Curve).</li> <li>Confusion Matrix.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>DataFrame</code> <p>True labels for the test dataset.</p> required <code>y_test_pred</code> <code>DataFrame</code> <p>Predicted probabilities for the test dataset.</p> required <code>y_val</code> <code>DataFrame</code> <p>True labels for the validation dataset.</p> required <code>y_val_pred</code> <code>DataFrame</code> <p>Predicted probabilities for the validation dataset.</p> required <code>y_train</code> <code>DataFrame</code> <p>True labels for the training dataset.</p> required <code>y_train_pred</code> <code>DataFrame</code> <p>Predicted probabilities for the training dataset.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save the diagnostic plots.</p> required Source code in <code>src/jarvais/utils/plot.py</code> <pre><code>@config_plot()\ndef plot_classification_diagnostics(\n        y_test: pd.DataFrame,\n        y_test_pred: pd.DataFrame,\n        y_val: pd.DataFrame,\n        y_val_pred: pd.DataFrame,\n        y_train: pd.DataFrame,\n        y_train_pred: pd.DataFrame,\n        output_dir: Path\n    ) -&gt; None:\n    \"\"\"\n    Generates diagnostic plots for evaluating a classification model.\n\n    Plots:\n        - Epic model evaluation plots (ROC Curve, Precision-Recall Curve, Calibration Curve, Sensitivity/Flag Curve).\n        - Confusion Matrix.\n\n    Args:\n        y_test (pd.DataFrame): True labels for the test dataset.\n        y_test_pred (pd.DataFrame): Predicted probabilities for the test dataset.\n        y_val (pd.DataFrame): True labels for the validation dataset.\n        y_val_pred (pd.DataFrame): Predicted probabilities for the validation dataset.\n        y_train (pd.DataFrame): True labels for the training dataset.\n        y_train_pred (pd.DataFrame): Predicted probabilities for the training dataset.\n        output_dir (Path): Directory to save the diagnostic plots.\n    \"\"\"\n    plot_epic_copy(\n        y_test.to_numpy(),\n        y_test_pred.to_numpy(),\n        y_val.to_numpy(),\n        y_val_pred.to_numpy(),\n        y_train.to_numpy(),\n        y_train_pred.to_numpy() ,\n        output_dir\n    )\n\n    conf_matrix = confusion_matrix(y_test, y_test_pred.apply(lambda x: 1 if x &gt;= 0.5 else 0))\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.savefig(output_dir / 'confusion_matrix.png')\n    plt.close()\n</code></pre>"},{"location":"api/trainer/","title":"Trainer","text":""},{"location":"api/trainer/#trainersupervised","title":"TrainerSupervised","text":"<p>The <code>TrainerSupervised</code> class is part of the <code>jarvais.trainer</code> module.</p>"},{"location":"api/trainer/#jarvais.trainer.TrainerSupervised","title":"<code>jarvais.trainer.TrainerSupervised</code>","text":"<p>TrainerSupervised class for supervised jarvAIs workflows.</p> <p>This class provides functionality for feature reduction, training models (e.g., AutoGluon, survival models),  and performing inference. It supports various tasks such as binary/multiclass classification, regression,  and survival analysis.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>Type of task. Must be one of {'binary', 'multiclass', 'regression', 'survival'}. </p> <code>reduction_method</code> <code>str | None</code> <p>Feature reduction method. Supported methods include  {'mrmr', 'variance_threshold', 'corr', 'chi2'}.</p> <code>keep_k</code> <code>int</code> <p>Number of features to retain during reduction.</p> <code>output_dir</code> <code>str | Path</code> <p>Directory for saving outputs. Defaults to the current working directory.</p> Example <pre><code>from jarvais.trainer import TrainerSupervised\n\ntrainer = TrainerSupervised(\n    task=\"binary\",\n    reduction_method=\"mrmr\",\n    keep_k=10,\n    output_dir=\"./results\"\n)\ntrainer.run(data=my_data, target_variable=\"target\")\n\npredictions = trainer.infer(new_data)\n</code></pre> Source code in <code>src/jarvais/trainer/trainer.py</code> <pre><code>class TrainerSupervised:\n    \"\"\"\n    TrainerSupervised class for supervised jarvAIs workflows.\n\n    This class provides functionality for feature reduction, training models (e.g., AutoGluon, survival models), \n    and performing inference. It supports various tasks such as binary/multiclass classification, regression, \n    and survival analysis.\n\n    Attributes:\n        task (str, optional): Type of task. Must be one of {'binary', 'multiclass', 'regression', 'survival'}. \n        reduction_method (str | None, optional): Feature reduction method. Supported methods include \n            {'mrmr', 'variance_threshold', 'corr', 'chi2'}.\n        keep_k (int, optional): Number of features to retain during reduction.\n        output_dir (str | Path, optional): Directory for saving outputs. Defaults to the current working directory.\n\n    Example:\n        ```python\n        from jarvais.trainer import TrainerSupervised\n\n        trainer = TrainerSupervised(\n            task=\"binary\",\n            reduction_method=\"mrmr\",\n            keep_k=10,\n            output_dir=\"./results\"\n        )\n        trainer.run(data=my_data, target_variable=\"target\")\n\n        predictions = trainer.infer(new_data)\n        ```\n    \"\"\"\n    def __init__(\n            self,\n            task: str=None,\n            reduction_method: str | None = None,\n            keep_k: int = 2,\n            output_dir: str | Path = None\n        ) -&gt; None:\n\n        self.task = task\n        self.reduction_method = reduction_method\n        self.keep_k = keep_k\n\n        if task not in ['binary', 'multiclass', 'regression', 'survival', None]:\n            raise ValueError(\"Invalid task parameter. Choose one of: 'binary', 'multiclass', 'regression', 'survival'. Providing None defaults to Autogluon infering.\")\n\n        self.output_dir = Path.cwd() if output_dir is None else Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True, parents=True)\n\n    def _feature_reduction(self, X: pd.DataFrame, y: pd.DataFrame | pd.Series) -&gt; pd.DataFrame:\n        \"\"\"\n        Reduce features based on the specified reduction method. \n\n        One-hot encoding applied before reduction and reverted afterward.\n        \"\"\"\n        # Step 1: Identify categorical columns\n        categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n\n        mappin = {}\n\n        def find_category_mappings(df, variable):\n            return {k: i for i, k in enumerate(df[variable].dropna().unique(), 0)}\n\n        def integer_encode(df, variable, ordinal_mapping):\n            df[variable] = df[variable].map(ordinal_mapping)\n\n        for variable in categorical_columns:\n            mappings = find_category_mappings(X, variable)\n            mappin[variable] = mappings\n\n        for variable in categorical_columns:\n            integer_encode(X, variable, mappin[variable])\n\n        # Step 3: Perform feature reduction\n        if self.reduction_method == 'mrmr':\n            X_reduced = mrmr_reduction(self.task, X, y, self.keep_k)\n        elif self.reduction_method == 'variance_threshold':\n            X_reduced = var_reduction(X, y)\n        elif self.reduction_method == 'corr':\n            X_reduced = kbest_reduction(self.task, X, y, self.keep_k)\n        elif self.reduction_method == 'chi2':\n            if self.task not in ['binary', 'multiclass']:\n                raise ValueError('chi-squared reduction can only be done with classification tasks')\n            X_reduced = chi2_reduction(X, y, self.keep_k)\n        else:\n            raise ValueError('Unsupported reduction method: {}'.format(self.reduction_method))\n\n        for col in categorical_columns:\n            if col in X_reduced.columns:\n                inv_map = {v: k for k, v in mappin[col].items()}\n                X_reduced[col] = X_reduced[col].map(inv_map)\n\n        return X_reduced\n\n    def _train_autogluon_with_cv(self) -&gt; None:\n        self.predictors, leaderboard, self.best_fold, self.X_val, self.y_val = train_autogluon_with_cv(\n            pd.concat([self.X_train, self.y_train], axis=1),\n            pd.concat([self.X_test, self.y_test], axis=1),\n            target_variable=self.target_variable,\n            task=self.task,\n            extra_metrics=self.extra_metrics,\n            eval_metric=self.eval_metric,\n            num_folds=self.k_folds,\n            output_dir=(self.output_dir / 'autogluon_models'),\n            **self.kwargs\n        )\n\n        self.predictor = self.predictors[self.best_fold]\n        self.trainer_config['best_fold'] = self.best_fold\n\n        # Update train data to remove validation\n        self.X_train = self.X_train[~self.X_train.index.isin(self.X_val.index)]\n        self.y_train = self.y_train[~self.y_train.index.isin(self.y_val.index)]\n\n        print('\\nModel Leaderboard (Displays values in \"mean [min, max]\" format across training folds)\\n------------------------------------------------------------------------------------')\n        print(tabulate(\n            leaderboard.sort_values(by='score_test', ascending=False)[self.show_leaderboard],\n            tablefmt = \"grid\",\n            headers=\"keys\",\n            showindex=False\n        ))\n\n    def _train_autogluon(self) -&gt; None:\n        self.predictor = TabularPredictor(\n            label=self.target_variable, \n            problem_type=self.task, \n            eval_metric=self.eval_metric,\n            path=(self.output_dir / 'autogluon_models' / 'autogluon_models_best_fold'),\n            log_to_file=False,\n        ).fit(\n            pd.concat([self.X_train, self.y_train], axis=1),\n            **self.kwargs\n        )\n\n        self.X_val, self.y_val = self.predictor.load_data_internal(data='val', return_y=True)\n        # Update train data to remove validation\n        self.X_train = self.X_train[~self.X_train.index.isin(self.X_val.index)]\n        self.y_train = self.y_train[~self.y_train.index.isin(self.y_val.index)]\n\n        train_leaderboard = self.predictor.leaderboard(\n            pd.concat([self.X_train, self.y_train], axis=1),\n            extra_metrics=self.extra_metrics).round(2)\n        val_leaderboard = self.predictor.leaderboard(\n            pd.concat([self.X_val, self.y_val], axis=1),\n            extra_metrics=self.extra_metrics).round(2)\n        test_leaderboard = self.predictor.leaderboard(\n            pd.concat([self.X_test, self.y_test], axis=1),\n            extra_metrics=self.extra_metrics).round(2)\n\n        leaderboard = pd.merge(\n            pd.merge(\n                format_leaderboard(train_leaderboard, self.extra_metrics, 'score_train'),\n                format_leaderboard(val_leaderboard, self.extra_metrics, 'score_val'),\n                on='model'\n            ),\n            format_leaderboard(test_leaderboard, self.extra_metrics, 'score_test'),\n            on='model'\n        )\n\n        print('\\nModel Leaderboard\\n----------------')\n        print(tabulate(\n            leaderboard.sort_values(by='score_test', ascending=False)[self.show_leaderboard],\n            tablefmt = \"grid\",\n            headers=\"keys\",\n            showindex=False))\n\n    def run(\n            self,\n            data: pd.DataFrame,\n            target_variable: str,\n            test_size: float = 0.2,\n            exclude: List[str] | None = None,\n            stratify_on: str | None = None,\n            explain: bool = False,\n            k_folds: int = 5,\n            **kwargs:dict\n        ) -&gt; None:\n        \"\"\"\n        Execute the jarvAIs Trainer pipeline on the given dataset.\n\n        Args:\n            data (pd.DataFrame): The input dataset containing features and target.\n            target_variable (str): The name of the target variable in the dataset.\n            test_size (float, optional): Proportion of the dataset to include in the test split. \n                Must be between 0 and 1. Default is 0.2.\n            exclude (list of str, optional): List of columns to exclude from the feature set. \n                Default is an empty list.\n            stratify_on (str, optional): Column to use for stratification, if any. \n                Must be compatible with `target_variable`.\n            explain (bool, optional): Whether to generate explainability reports for the model. \n                Default is False.\n            k_folds (int, optional): Number of folds for cross-validation. If 1, uses AutoGluon-specific validation. \n                Default is 5.\n            kwargs (dict, optional): Additional arguments passed to the AutoGluon predictor's `fit` method.\n        \"\"\"\n        self.trainer_config = dict()\n        self.trainer_config['task'] = self.task\n        self.trainer_config['output_dir'] = self.output_dir.as_posix()\n\n        self.target_variable = target_variable\n        self.trainer_config['target_variable'] = target_variable\n        self.k_folds = k_folds\n        self.trainer_config['k_folds'] = k_folds\n        self.kwargs = kwargs\n\n        self.trainer_config['test_size'] = test_size\n        self.trainer_config['stratify_on'] = stratify_on\n\n        # Initialize mutable defaults\n        if exclude is None:\n            exclude = []\n\n        if isinstance(target_variable, list): # Happens for survival data\n            exclude += target_variable\n        else:\n            exclude.append(target_variable)\n\n        try:\n            X = data.drop(columns=exclude)\n            y = data[target_variable]\n        except KeyError as e:\n            raise ValueError(f\"Invalid column specified: {e}\")\n\n        # Optional feature reduction\n        if getattr(self, \"reduction_method\", None):\n            print(f\"Applying {self.reduction_method} for feature reduction\")\n            X = self._feature_reduction(X, y)\n            print(f\"Features retained: {list(X.columns)}\")\n\n            self.feature_names = list(X.columns)\n            self.trainer_config['reduction_method'] = self.reduction_method\n            self.trainer_config['reduced_feature_set'] = self.feature_names\n\n        if self.task in {'binary', 'multiclass'}:\n            stratify_col = (\n                y.astype(str) + '_' + data[stratify_on].astype(str)\n                if stratify_on is not None\n                else y\n            )\n        else:\n            stratify_col = None\n\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            X, y, test_size=test_size, stratify=stratify_col, random_state=42)\n\n        if self.task == 'survival':\n            self.predictors, scores, data_train, data_val = train_survival_models(\n                self.X_train, \n                self.y_train, \n                self.X_test, \n                self.y_test, \n                self.output_dir\n            )\n            self.predictor = self.predictors[max(scores, key=scores.get)]\n            self.trainer_config['survival_models_info'] = scores\n\n            self.X_train, self.y_train = data_train.drop(columns=['time', 'event']), data_train[['time', 'event']] \n            self.X_val, self.y_val = data_val.drop(columns=['time', 'event']), data_val[['time', 'event']] \n        else:\n            (self.output_dir / 'autogluon_models').mkdir(exist_ok=True, parents=True)\n\n            if self.task in ['binary', 'multiclass']:\n                self.eval_metric = 'roc_auc'\n            elif self.task == 'regression':\n                self.eval_metric = 'r2'\n\n            ag_auprc_scorer = make_scorer(\n                name='auprc', # Move this to a seperate file?\n                score_func=auprc,\n                optimum=1,\n                greater_is_better=True,\n                needs_class=True)\n\n            # When changing extra_metrics consider where it's used and make updates accordingly\n            self.extra_metrics = ['f1', ag_auprc_scorer] if self.task in ['binary', 'multiclass'] else ['root_mean_squared_error']\n            self.show_leaderboard = ['model', 'score_test', 'score_val', 'score_train']\n\n            custom_hyperparameters = get_hyperparameter_config('default')\n            custom_hyperparameters[SimpleRegressionModel] = {}\n            kwargs['hyperparameters'] = custom_hyperparameters\n\n            if k_folds &gt; 1:\n                self._train_autogluon_with_cv()\n            else:\n                self._train_autogluon()\n\n        self.trained = True\n\n        self.data_dir = self.output_dir / 'data'\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        self.X_train.to_csv((self.data_dir / 'X_train.csv'), index=False)\n        self.X_test.to_csv((self.data_dir / 'X_test.csv'), index=False)\n        self.X_val.to_csv((self.data_dir / 'X_val.csv'), index=False)\n        self.y_train.to_csv((self.data_dir / 'y_train.csv'), index=False)\n        self.y_test.to_csv((self.data_dir / 'y_test.csv'), index=False)\n        self.y_val.to_csv((self.data_dir / 'y_val.csv'), index=False)\n\n        with (self.output_dir / 'trainer_config.yaml').open('w') as f:\n            yaml.dump(self.trainer_config, f)\n\n        if explain:\n            explainer = Explainer.from_trainer(self)\n            explainer.run()\n\n    def model_names(self) -&gt; List[str]:\n        \"\"\"\n        Returns all trainer model names.\n\n        This method retrieves the names of all models associated with the \n        current predictor. It requires that the predictor has been trained.\n\n        Returns:\n            list: A list of model names available in the predictor.\n\n        Raises:\n            ValueError: If the model has not been trained (`self.trained` is False).\n        \"\"\"\n        if not self.trained:\n            raise ValueError(\"The model must be trained before accessing model names.\")\n\n        if self.task == 'survival':\n            return list(self.predictors.keys())\n        else:        \n            return self.predictor.model_names()\n\n    def infer(self, data: pd.DataFrame, model: str = None) -&gt; np.ndarray:\n        \"\"\"\n        Perform inference using the trained predictor on the provided data.\n\n        This method generates predictions based on the input data using the \n        specified model. If no model is provided, the default model is used. \n        The predictor must be trained before inference can be performed.\n\n        Args:\n            data (pd.DataFrame): The input data for which inference is to be performed.\n            model (str, optional): The name of the model to use for inference. \n                If None, the default model is used.\n\n        Returns:\n            np.ndarray: The prediction results from the model.\n\n        Raises:\n            ValueError: If the model has not been trained (`self.trained` is False).\n            ValueError: If the specified model name is not found in the predictor.\n        \"\"\"\n        if not self.trained:\n            raise ValueError(\"The model must be trained before performing inference.\")\n        if not model is None and not model in self.model_names():\n            raise ValueError(f\"Model '{model}' not in trainer. Use model_names() to list valid available models.\")\n\n        if self.task == 'survival':\n            if model is None:\n                inference = self.predictor.predict(data)\n            else:\n                inference = self.predictors[model].predict(data)\n        else:\n            if self.predictor.can_predict_proba:\n                inference = self.predictor.predict_proba(data, model, as_pandas=False)[:, 1]\n            else:\n                inference = self.predictor.predict(data, model, as_pandas=False)\n\n        return inference\n\n    @classmethod\n    def load_trainer(cls, project_dir: str | Path):\n        \"\"\"\n        Load a trained TrainerSupervised from the specified directory.\n\n        Args:\n            project_dir (str or Path, optional): The directory where the trainer was run.\n\n        Returns:\n            trainer (TrainerSupervised): The loaded Trainer.\n        \"\"\"\n        project_dir = Path(project_dir)\n        with (project_dir / 'trainer_config.yaml').open('r') as f:\n            trainer_config = yaml.safe_load(f)\n\n        trainer = cls()\n        trainer.task = trainer_config['task']\n        trainer.output_dir = project_dir\n\n        if trainer.task == 'survival':\n            model_dir = (project_dir / 'survival_models')\n\n            trainer.predictors = {}\n            model_info = trainer_config['survival_models_info']\n            for model_name, _ in model_info.items():\n                if model_name == 'MTLR':\n                    trainer.predictors[model_name] = LitMTLR.load_from_checkpoint(model_dir / \"MTLR.ckpt\")\n                elif model_name == 'DeepSurv':\n                    trainer.predictors[model_name] = LitDeepSurv.load_from_checkpoint(model_dir / \"DeepSurv.ckpt\")\n                else:\n                    with (model_dir / f'{model_name}.pkl').open(\"rb\") as f:\n                        trainer.predictors[model_name] = pickle.load(f)\n\n            trainer.predictor = trainer.predictors[max(model_info, key=model_info.get)]\n        else:\n            model_dir = (project_dir / 'autogluon_models' / 'autogluon_models_best_fold')\n            trainer.predictor = TabularPredictor.load(model_dir, verbosity=1)\n\n        trainer.trained = True\n\n        trainer.X_test = pd.read_csv(project_dir / 'data' / 'X_test.csv')\n        trainer.X_val = pd.read_csv(project_dir / 'data' / 'X_val.csv')\n        trainer.X_train = pd.read_csv(project_dir / 'data' / 'X_train.csv')\n        trainer.y_test = pd.read_csv(project_dir / 'data' / 'y_test.csv').squeeze()\n        trainer.y_val = pd.read_csv(project_dir / 'data' / 'y_val.csv').squeeze()\n        trainer.y_train = pd.read_csv(project_dir / 'data' / 'y_train.csv').squeeze()\n\n        return trainer\n</code></pre>"},{"location":"api/trainer/#jarvais.trainer.TrainerSupervised.run","title":"<code>run(data, target_variable, test_size=0.2, exclude=None, stratify_on=None, explain=False, k_folds=5, **kwargs)</code>","text":"<p>Execute the jarvAIs Trainer pipeline on the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input dataset containing features and target.</p> required <code>target_variable</code> <code>str</code> <p>The name of the target variable in the dataset.</p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the test split.  Must be between 0 and 1. Default is 0.2.</p> <code>0.2</code> <code>exclude</code> <code>list of str</code> <p>List of columns to exclude from the feature set.  Default is an empty list.</p> <code>None</code> <code>stratify_on</code> <code>str</code> <p>Column to use for stratification, if any.  Must be compatible with <code>target_variable</code>.</p> <code>None</code> <code>explain</code> <code>bool</code> <p>Whether to generate explainability reports for the model.  Default is False.</p> <code>False</code> <code>k_folds</code> <code>int</code> <p>Number of folds for cross-validation. If 1, uses AutoGluon-specific validation.  Default is 5.</p> <code>5</code> <code>kwargs</code> <code>dict</code> <p>Additional arguments passed to the AutoGluon predictor's <code>fit</code> method.</p> <code>{}</code> Source code in <code>src/jarvais/trainer/trainer.py</code> <pre><code>def run(\n        self,\n        data: pd.DataFrame,\n        target_variable: str,\n        test_size: float = 0.2,\n        exclude: List[str] | None = None,\n        stratify_on: str | None = None,\n        explain: bool = False,\n        k_folds: int = 5,\n        **kwargs:dict\n    ) -&gt; None:\n    \"\"\"\n    Execute the jarvAIs Trainer pipeline on the given dataset.\n\n    Args:\n        data (pd.DataFrame): The input dataset containing features and target.\n        target_variable (str): The name of the target variable in the dataset.\n        test_size (float, optional): Proportion of the dataset to include in the test split. \n            Must be between 0 and 1. Default is 0.2.\n        exclude (list of str, optional): List of columns to exclude from the feature set. \n            Default is an empty list.\n        stratify_on (str, optional): Column to use for stratification, if any. \n            Must be compatible with `target_variable`.\n        explain (bool, optional): Whether to generate explainability reports for the model. \n            Default is False.\n        k_folds (int, optional): Number of folds for cross-validation. If 1, uses AutoGluon-specific validation. \n            Default is 5.\n        kwargs (dict, optional): Additional arguments passed to the AutoGluon predictor's `fit` method.\n    \"\"\"\n    self.trainer_config = dict()\n    self.trainer_config['task'] = self.task\n    self.trainer_config['output_dir'] = self.output_dir.as_posix()\n\n    self.target_variable = target_variable\n    self.trainer_config['target_variable'] = target_variable\n    self.k_folds = k_folds\n    self.trainer_config['k_folds'] = k_folds\n    self.kwargs = kwargs\n\n    self.trainer_config['test_size'] = test_size\n    self.trainer_config['stratify_on'] = stratify_on\n\n    # Initialize mutable defaults\n    if exclude is None:\n        exclude = []\n\n    if isinstance(target_variable, list): # Happens for survival data\n        exclude += target_variable\n    else:\n        exclude.append(target_variable)\n\n    try:\n        X = data.drop(columns=exclude)\n        y = data[target_variable]\n    except KeyError as e:\n        raise ValueError(f\"Invalid column specified: {e}\")\n\n    # Optional feature reduction\n    if getattr(self, \"reduction_method\", None):\n        print(f\"Applying {self.reduction_method} for feature reduction\")\n        X = self._feature_reduction(X, y)\n        print(f\"Features retained: {list(X.columns)}\")\n\n        self.feature_names = list(X.columns)\n        self.trainer_config['reduction_method'] = self.reduction_method\n        self.trainer_config['reduced_feature_set'] = self.feature_names\n\n    if self.task in {'binary', 'multiclass'}:\n        stratify_col = (\n            y.astype(str) + '_' + data[stratify_on].astype(str)\n            if stratify_on is not None\n            else y\n        )\n    else:\n        stratify_col = None\n\n    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n        X, y, test_size=test_size, stratify=stratify_col, random_state=42)\n\n    if self.task == 'survival':\n        self.predictors, scores, data_train, data_val = train_survival_models(\n            self.X_train, \n            self.y_train, \n            self.X_test, \n            self.y_test, \n            self.output_dir\n        )\n        self.predictor = self.predictors[max(scores, key=scores.get)]\n        self.trainer_config['survival_models_info'] = scores\n\n        self.X_train, self.y_train = data_train.drop(columns=['time', 'event']), data_train[['time', 'event']] \n        self.X_val, self.y_val = data_val.drop(columns=['time', 'event']), data_val[['time', 'event']] \n    else:\n        (self.output_dir / 'autogluon_models').mkdir(exist_ok=True, parents=True)\n\n        if self.task in ['binary', 'multiclass']:\n            self.eval_metric = 'roc_auc'\n        elif self.task == 'regression':\n            self.eval_metric = 'r2'\n\n        ag_auprc_scorer = make_scorer(\n            name='auprc', # Move this to a seperate file?\n            score_func=auprc,\n            optimum=1,\n            greater_is_better=True,\n            needs_class=True)\n\n        # When changing extra_metrics consider where it's used and make updates accordingly\n        self.extra_metrics = ['f1', ag_auprc_scorer] if self.task in ['binary', 'multiclass'] else ['root_mean_squared_error']\n        self.show_leaderboard = ['model', 'score_test', 'score_val', 'score_train']\n\n        custom_hyperparameters = get_hyperparameter_config('default')\n        custom_hyperparameters[SimpleRegressionModel] = {}\n        kwargs['hyperparameters'] = custom_hyperparameters\n\n        if k_folds &gt; 1:\n            self._train_autogluon_with_cv()\n        else:\n            self._train_autogluon()\n\n    self.trained = True\n\n    self.data_dir = self.output_dir / 'data'\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n    self.X_train.to_csv((self.data_dir / 'X_train.csv'), index=False)\n    self.X_test.to_csv((self.data_dir / 'X_test.csv'), index=False)\n    self.X_val.to_csv((self.data_dir / 'X_val.csv'), index=False)\n    self.y_train.to_csv((self.data_dir / 'y_train.csv'), index=False)\n    self.y_test.to_csv((self.data_dir / 'y_test.csv'), index=False)\n    self.y_val.to_csv((self.data_dir / 'y_val.csv'), index=False)\n\n    with (self.output_dir / 'trainer_config.yaml').open('w') as f:\n        yaml.dump(self.trainer_config, f)\n\n    if explain:\n        explainer = Explainer.from_trainer(self)\n        explainer.run()\n</code></pre>"},{"location":"api/trainer/#jarvais.trainer.TrainerSupervised.model_names","title":"<code>model_names()</code>","text":"<p>Returns all trainer model names.</p> <p>This method retrieves the names of all models associated with the  current predictor. It requires that the predictor has been trained.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>A list of model names available in the predictor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been trained (<code>self.trained</code> is False).</p> Source code in <code>src/jarvais/trainer/trainer.py</code> <pre><code>def model_names(self) -&gt; List[str]:\n    \"\"\"\n    Returns all trainer model names.\n\n    This method retrieves the names of all models associated with the \n    current predictor. It requires that the predictor has been trained.\n\n    Returns:\n        list: A list of model names available in the predictor.\n\n    Raises:\n        ValueError: If the model has not been trained (`self.trained` is False).\n    \"\"\"\n    if not self.trained:\n        raise ValueError(\"The model must be trained before accessing model names.\")\n\n    if self.task == 'survival':\n        return list(self.predictors.keys())\n    else:        \n        return self.predictor.model_names()\n</code></pre>"},{"location":"api/trainer/#jarvais.trainer.TrainerSupervised.infer","title":"<code>infer(data, model=None)</code>","text":"<p>Perform inference using the trained predictor on the provided data.</p> <p>This method generates predictions based on the input data using the  specified model. If no model is provided, the default model is used.  The predictor must be trained before inference can be performed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input data for which inference is to be performed.</p> required <code>model</code> <code>str</code> <p>The name of the model to use for inference.  If None, the default model is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The prediction results from the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been trained (<code>self.trained</code> is False).</p> <code>ValueError</code> <p>If the specified model name is not found in the predictor.</p> Source code in <code>src/jarvais/trainer/trainer.py</code> <pre><code>def infer(self, data: pd.DataFrame, model: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Perform inference using the trained predictor on the provided data.\n\n    This method generates predictions based on the input data using the \n    specified model. If no model is provided, the default model is used. \n    The predictor must be trained before inference can be performed.\n\n    Args:\n        data (pd.DataFrame): The input data for which inference is to be performed.\n        model (str, optional): The name of the model to use for inference. \n            If None, the default model is used.\n\n    Returns:\n        np.ndarray: The prediction results from the model.\n\n    Raises:\n        ValueError: If the model has not been trained (`self.trained` is False).\n        ValueError: If the specified model name is not found in the predictor.\n    \"\"\"\n    if not self.trained:\n        raise ValueError(\"The model must be trained before performing inference.\")\n    if not model is None and not model in self.model_names():\n        raise ValueError(f\"Model '{model}' not in trainer. Use model_names() to list valid available models.\")\n\n    if self.task == 'survival':\n        if model is None:\n            inference = self.predictor.predict(data)\n        else:\n            inference = self.predictors[model].predict(data)\n    else:\n        if self.predictor.can_predict_proba:\n            inference = self.predictor.predict_proba(data, model, as_pandas=False)[:, 1]\n        else:\n            inference = self.predictor.predict(data, model, as_pandas=False)\n\n    return inference\n</code></pre>"},{"location":"api/trainer/#jarvais.trainer.TrainerSupervised.load_trainer","title":"<code>load_trainer(project_dir)</code>  <code>classmethod</code>","text":"<p>Load a trained TrainerSupervised from the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>project_dir</code> <code>str or Path</code> <p>The directory where the trainer was run.</p> required <p>Returns:</p> Name Type Description <code>trainer</code> <code>TrainerSupervised</code> <p>The loaded Trainer.</p> Source code in <code>src/jarvais/trainer/trainer.py</code> <pre><code>@classmethod\ndef load_trainer(cls, project_dir: str | Path):\n    \"\"\"\n    Load a trained TrainerSupervised from the specified directory.\n\n    Args:\n        project_dir (str or Path, optional): The directory where the trainer was run.\n\n    Returns:\n        trainer (TrainerSupervised): The loaded Trainer.\n    \"\"\"\n    project_dir = Path(project_dir)\n    with (project_dir / 'trainer_config.yaml').open('r') as f:\n        trainer_config = yaml.safe_load(f)\n\n    trainer = cls()\n    trainer.task = trainer_config['task']\n    trainer.output_dir = project_dir\n\n    if trainer.task == 'survival':\n        model_dir = (project_dir / 'survival_models')\n\n        trainer.predictors = {}\n        model_info = trainer_config['survival_models_info']\n        for model_name, _ in model_info.items():\n            if model_name == 'MTLR':\n                trainer.predictors[model_name] = LitMTLR.load_from_checkpoint(model_dir / \"MTLR.ckpt\")\n            elif model_name == 'DeepSurv':\n                trainer.predictors[model_name] = LitDeepSurv.load_from_checkpoint(model_dir / \"DeepSurv.ckpt\")\n            else:\n                with (model_dir / f'{model_name}.pkl').open(\"rb\") as f:\n                    trainer.predictors[model_name] = pickle.load(f)\n\n        trainer.predictor = trainer.predictors[max(model_info, key=model_info.get)]\n    else:\n        model_dir = (project_dir / 'autogluon_models' / 'autogluon_models_best_fold')\n        trainer.predictor = TabularPredictor.load(model_dir, verbosity=1)\n\n    trainer.trained = True\n\n    trainer.X_test = pd.read_csv(project_dir / 'data' / 'X_test.csv')\n    trainer.X_val = pd.read_csv(project_dir / 'data' / 'X_val.csv')\n    trainer.X_train = pd.read_csv(project_dir / 'data' / 'X_train.csv')\n    trainer.y_test = pd.read_csv(project_dir / 'data' / 'y_test.csv').squeeze()\n    trainer.y_val = pd.read_csv(project_dir / 'data' / 'y_val.csv').squeeze()\n    trainer.y_train = pd.read_csv(project_dir / 'data' / 'y_train.csv').squeeze()\n\n    return trainer\n</code></pre>"},{"location":"get_started/analyzer/","title":"Analyzer Quick Start","text":""},{"location":"get_started/analyzer/#analyzer","title":"Analyzer","text":"<p>The <code>Analyzer</code> module is designed for data visualization and exploration. It helps to gain insights into the data, identify patterns, and assess relationships between different features, which is essential for building effective models.</p>"},{"location":"get_started/analyzer/#example-usage","title":"Example Usage","text":"<pre><code>from jarvais.analyzer import Analyzer\n\ndata = pd.DataFrame({\n        \"stage\": [\"I\", \"I\", \"II\", \"III\", \"IV\", \"IV\", \"IV\", \"IV\", \"IV\", \"IV\"],\n        \"treatment\": [\"surgery\", \"surgery\", \"chemo\", \"chemo\", \"chemo\", \"chemo\", \"hormone\", \"hormone\", \"hormone\", \"hormone\"],\n        \"age\": [45, 45, 60, 70, 80, 80, 80, 80, 80, 80],\n        \"tumor_size\": [2.1, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5],  \n        \"death\": [True, False, True, False, True, False, True, False, True, False],\n    })\n\nanalyzer = Analyzer(\n    data, \n    output_dir=\"./temp_output/test\",\n    categorical_columns=[\"stage\", \"treatment\", \"death\"], \n    target_variable=\"death\", \n    task=\"classification\"\n)\n\nanalyzer.run()\n</code></pre>"},{"location":"get_started/analyzer/#settings","title":"Settings","text":"<p>The <code>Analyzer</code> module can be configured using settings. The settings are stored in a JSON file and can be loaded using the <code>Analyzer.from_settings</code> function. Example settings:</p> <pre><code>Analyzer(\n    AnalyzerSettings(\n        output_dir=PosixPath('temp_output/test'),\n        categorical_columns=['stage', 'treatment', 'death'],\n        continuous_columns=['tumor_size', 'age'],\n        date_columns=[],\n        task='classification',\n        target_variable='death',\n        generate_report=True,\n        settings_path=PosixPath('temp_output/test/analyzer_settings.json'),\n        settings_schema_path=PosixPath('temp_output/test/analyzer_settings.schema.json'),\n        missingness=MissingnessModule(\n            categorical_strategy={'stage': 'unknown', 'treatment': 'unknown', 'death': 'unknown'},\n            continuous_strategy={'tumor_size': 'median', 'age': 'median'},\n            enabled=True\n        ),\n        outlier=OutlierModule(\n            categorical_strategy={'stage': 'frequency', 'treatment': 'frequency', 'death': 'frequency'},\n            continuous_strategy={'tumor_size': 'none', 'age': 'none'},\n            threshold=0.01,\n            enabled=True\n        ),\n        encoding=OneHotEncodingModule(columns=['stage', 'treatment'], target_variable='death', prefix_sep='|', enabled=True),\n        visualization=VisualizationModule(plots=['corr', 'pairplot', 'umap', 'frequency_table', 'multiplot'], enabled=True)\n    )\n)\n</code></pre>"},{"location":"get_started/analyzer/#example-output","title":"Example Output","text":"<pre><code>Outlier Detection:\n  - Outliers found in Gender: ['Male: 5 out of 1000']\n  - Outliers found in Disease Type: ['Lung Cancer: 10 out of 1000']\n  - No Outliers found in Treatment\n  - Outliers found in Tumor Size: ['12.5: 2 out of 1000']\n</code></pre>"},{"location":"get_started/analyzer/#tableonedata-summary","title":"TableOne(Data Summary)","text":"Category Missing Overall n 1000 Age, mean (SD) 0 58.2 (12.3) Tumor Size, mean (SD) 0 4.5 (1.2) Gender, n (%) Female 520 (52%) Male 480 (48%) Disease Type, n (%) Breast Cancer 300 (30%) Lung Cancer 150 (15%) Prostate Cancer 100 (10%)"},{"location":"get_started/analyzer/#output-files","title":"Output Files","text":"<p>The Analyzer module generates the following files and directories:</p> <ul> <li>analysis_report.pdf: A PDF report summarizing the analysis results.</li> <li>tableone.csv: CSV file containing summary statistics for the dataset.</li> <li>updated_data.csv: CSV file with the cleaned and processed data.</li> <li>analyzer_settings.json: Configuration file for the analysis setup.</li> <li>analyzer_settings.schema.json: Schema file for the analyzer settings.</li> </ul>"},{"location":"get_started/analyzer/#figures","title":"Figures","text":""},{"location":"get_started/analyzer/#1-frequency-tables","title":"1. Frequency Tables","text":"<p>Visualizations comparing different categorical features.</p> <p></p>"},{"location":"get_started/analyzer/#2-multi-plots","title":"2. Multi-plots","text":"<p>Visualizations showing combinations of features for deeper analysis.</p> <p></p>"},{"location":"get_started/analyzer/#additional-figures","title":"Additional Figures","text":""},{"location":"get_started/analyzer/#1-pairplot","title":"1. Pairplot","text":"<p>Pairwise relationships between continuous variables.</p> <p></p>"},{"location":"get_started/analyzer/#2-pearson-correlation-matrix","title":"2. Pearson Correlation Matrix","text":"<p>A heatmap visualizing Pearson correlations between variables.</p> <p></p>"},{"location":"get_started/analyzer/#3-spearman-correlation-matrix","title":"3. Spearman Correlation Matrix","text":"<p>A heatmap visualizing Spearman correlations between variables.</p> <p></p>"},{"location":"get_started/analyzer/#4-umap-of-continuous-data","title":"4. UMAP of Continuous Data","text":"<p>UMAP visualization of continuous data.</p> <p></p>"},{"location":"get_started/analyzer/#5-kaplan-meier-curve-survival","title":"5. Kaplan Meier Curve (Survival)","text":"<p>Kaplan Meier Curves for all categorical variables. An option exclusive to survival data.</p> <p></p>"},{"location":"get_started/analyzer/#analysis-report","title":"Analysis Report","text":""},{"location":"get_started/explainer/","title":"Explainer Quick Start","text":""},{"location":"get_started/explainer/#explainer-module","title":"Explainer Module","text":"<p>The <code>Explainer</code> module is designed to evaluate trained models by generating diagnostic plots, auditing bias, and producing comprehensive reports. It supports various supervised learning tasks, including classification, regression, and survival models. </p> <p>The module provides an easy-to-use interface for model diagnostics, bias analysis, and feature importance visualization, facilitating deeper insights into the model's performance and fairness.</p>"},{"location":"get_started/explainer/#features","title":"Features","text":"<ul> <li>Diagnostic Plots: Generates performance diagnostics, including classification metrics, regression plots, and SHAP value visualizations.</li> <li>Bias Audit: Identifies potential biases in model predictions with respect to sensitive features.</li> <li>Feature Importance: Calculates and visualizes feature importance using permutation importance or model-specific methods.</li> <li>Comprehensive Reports: Creates a detailed PDF report summarizing all diagnostic results.</li> </ul>"},{"location":"get_started/explainer/#example-usage","title":"Example Usage","text":"<pre><code>from jarvais.explainer import Explainer\n\n# Prefered method is to initialize from trainer\nexp = Explainer.from_trainer(trainer)\nexp.run()\n</code></pre>"},{"location":"get_started/explainer/#output-files","title":"Output Files","text":"<p>The Explainer module generates the following files and directories:</p> <ul> <li>explainer_report.pdf: A PDF report summarizing the model diagnostics, bias audit results, and feature importance.</li> <li>bias/: Contains CSV files with bias metrics for different sensitive features.</li> </ul>"},{"location":"get_started/explainer/#common-figures","title":"Common Figures","text":""},{"location":"get_started/explainer/#feature-importance","title":"Feature Importance","text":""},{"location":"get_started/explainer/#bootsrapped-metrics","title":"Bootsrapped Metrics","text":""},{"location":"get_started/explainer/#classification-figures","title":"Classification Figures","text":""},{"location":"get_started/explainer/#confusion-matrix","title":"Confusion Matrix","text":""},{"location":"get_started/explainer/#model-evaluation","title":"Model Evaluation","text":""},{"location":"get_started/explainer/#shap-plots","title":"Shap Plots","text":""},{"location":"get_started/explainer/#regression-figures","title":"Regression Figures","text":""},{"location":"get_started/explainer/#residual-plots","title":"Residual Plots","text":""},{"location":"get_started/explainer/#true-vs-predicted","title":"True vs Predicted","text":""},{"location":"get_started/explainer/#explainer-report","title":"Explainer Report","text":""},{"location":"get_started/trainer/","title":"Trainer Quick Start","text":""},{"location":"get_started/trainer/#trainer-module","title":"Trainer Module","text":"<p>The <code>Trainer</code> module simplifies and automates the process of feature reduction, model training, and evaluation for various machine learning tasks, ensuring flexibility and efficiency.</p>"},{"location":"get_started/trainer/#key-features","title":"Key Features","text":"<ol> <li> <p>Feature Reduction:</p> <ul> <li>Supports methods such as <code>mrmr</code>, <code>variance_threshold</code>, <code>corr</code>, and <code>chi2</code> to identify and retain relevant features.</li> </ul> </li> <li> <p>Automated Model Training:</p> <ul> <li>Integrates with AutoGluon for model training, selection, and optimization.</li> <li>Handles tasks such as binary classification, multiclass classification, regression, and survival.</li> </ul> </li> </ol>"},{"location":"get_started/trainer/#example-usage","title":"Example Usage","text":"<pre><code>from jarvais.trainer import TrainerSupervised\n\ntrainer = TrainerSupervised(task='binary', output_dir='./trainer_outputs')\ntrainer.run(data=data, target_variable='target', save_data=True)\n</code></pre>"},{"location":"get_started/trainer/#example-output","title":"Example Output","text":"<pre><code>Training fold 1/5...  \nFold 1 score: `0.8467207586933614`\n\nTraining fold 2/5...  \nFold 2 score: `0.8487846136306914`\n...\n</code></pre>"},{"location":"get_started/trainer/#model-leaderboard","title":"Model Leaderboard","text":"<p>Displays values in <code>mean [min, max]</code> format across training folds.</p> Model Score Test Score Val Score Train WeightedEnsemble_L2 AUROC: 0.82 [0.82, 0.83] AUROC: 0.85 [0.85, 0.85] AUROC: 1.0 [1.0, 1.0] F1: 0.13 [0.11, 0.14] F1: 0.09 [0.07, 0.12] F1: 0.95 [0.9, 1.0] AUPRC: 0.48 [0.45, 0.52] AUPRC: 0.47 [0.44, 0.49] AUPRC: 0.96 [0.91, 1.0] ExtraTreesGini AUROC: 0.82 [0.82, 0.82] AUROC: 0.84 [0.84, 0.84] AUROC: 1.0 [1.0, 1.0] F1: 0.21 [0.19, 0.22] F1: 0.16 [0.14, 0.18] F1: 1.0 [1.0, 1.0] AUPRC: 0.45 [0.45, 0.45] AUPRC: 0.43 [0.41, 0.45] AUPRC: 1.0 [1.0, 1.0] ..."},{"location":"get_started/trainer/#output-files","title":"Output Files","text":"<p>Binary/Regression/Multiclass:</p> <pre><code>\u251c\u2500\u2500 autogluon_models\n\u2502   \u251c\u2500\u2500 autogluon_models_best_fold\n\u2502   \u2502   \u251c\u2500\u2500 learner.pkl\n\u2502   \u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 autogluon_models_fold_1\n\u2502   \u2502   \u251c\u2500\u2500 learner.pkl\n\u2502   \u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 autogluon_models_fold_2\n\u2502   \u2502   \u251c\u2500\u2500 learner.pkl\n\u2502   \u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 autogluon_models_fold_3\n\u2502   \u2502   \u251c\u2500\u2500 learner.pkl\n\u2502   \u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 autogluon_models_fold_4\n\u2502   \u2502   \u251c\u2500\u2500 learner.pkl\n\u2502   \u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 autogluon_models_fold_5\n\u2502   \u2502   \u251c\u2500\u2500 learner.pkl\n\u2502   \u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 ...\n</code></pre> <p>Survival:</p> <pre><code>\u2514\u2500\u2500 survival_models\n    \u251c\u2500\u2500 CoxPH.pkl\n    \u251c\u2500\u2500 GradientBoosting.pkl\n    \u251c\u2500\u2500 RandomForest.pkl\n    \u2514\u2500\u2500 SVM.pkl\n    \u251c\u2500\u2500 lightning_logs\n    \u2502   \u251c\u2500\u2500 version_0\n    \u2502   \u251c\u2500\u2500 ...\n    \u251c\u2500\u2500 DeepSurv.ckpt\n    \u251c\u2500\u2500 MTLR.ckpt\n</code></pre>"},{"location":"tutorials/","title":"Navigating Our Tutorial Notebooks","text":""},{"location":"tutorials/#navigating-our-tutorial-notebooks","title":"Navigating Our Tutorial Notebooks","text":"<p>The following notebooks showcase examples of how you can use <code>jarvAIs</code> for classification, regression, and survival tasks!  </p> <p>All of the notebooks below use clinical data from RADCURE.  </p> <pre><code>The RADCURE dataset was collected clinically for radiation therapy treatment \nplanning and retrospectively reconstructed for quantitative imaging research.\n</code></pre>"},{"location":"tutorials/#accessing-the-data","title":"Accessing the Data","text":"<p>You can find the raw and processed clinical data here. </p>"},{"location":"tutorials/#data-processing","title":"Data Processing","text":"<p>The following function was used to process the raw data:</p> Source code in <code>src/jarvais/utils/functional.py</code> <pre><code>def process_RADCURE_clinical(df):\n    \"\"\"\n    Processes RADCURE clinical data.\n\n    Raw data found here: https://www.cancerimagingarchive.net/collection/radcure/\n    \"\"\"\n    df_converted = pd.DataFrame({\n        'Study ID': df['patient_id'],\n        'survival_time': df['Length FU'],\n        'death': df['Status'].apply(lambda x: 1 if x == 'Dead' else 0),\n        'age at dx': df['Age'],\n        'Sex': df['Sex'],\n        'T Stage': df['T'],\n        'N Stage': df['N'],\n        'Stage': df['Stage'],\n        'Dose': df['Dose'],\n        'Chemotherapy': df['Chemo'].apply(lambda x: 1 if x != 'none' else 0),\n        'HPV Combined': df['HPV'].apply(lambda x: 1 if isinstance(x, str) and 'positive' in x.lower() else None),\n        'Smoking Status': df['Smoking Status'],\n        'Disease Site': df['Ds Site'].str.lower()\n    })\n\n    return df_converted\n</code></pre>"},{"location":"tutorials/classification/","title":"Classification","text":"In\u00a0[4]: Copied! <pre>import sys\nimport pandas as pd\nimport os\n\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n\nsys.path.append(project_root)\ndata_dir = os.path.join(project_root, 'data')\n\ndata_file_path = os.path.join(data_dir, 'RADCURE_processed_clinical.csv')\ndf = pd.read_csv(data_file_path, index_col=0)\n\ndf.drop(columns=[\"Study ID\", \"survival_time\"], inplace=True)\n</pre> import sys import pandas as pd import os  project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))  sys.path.append(project_root) data_dir = os.path.join(project_root, 'data')  data_file_path = os.path.join(data_dir, 'RADCURE_processed_clinical.csv') df = pd.read_csv(data_file_path, index_col=0)  df.drop(columns=[\"Study ID\", \"survival_time\"], inplace=True) In\u00a0[\u00a0]: Copied! <pre>from jarvais.analyzer import Analyzer\nfrom rich import print\n\nanalyzer = Analyzer(\n    data=df, \n    output_dir='./outputs/analyzer',\n    categorical_columns= [\n      \"Sex\",\n      \"T Stage\",\n      \"N Stage\",\n      \"Stage\",\n      \"Smoking Status\",\n      \"Disease Site\",\n      \"death\",\n      \"HPV Combined\",\n      \"Chemotherapy\"\n    ],\n    continuous_columns = [\n      \"age at dx\",\n      \"Dose\"\n    ],\n    target_variable='death', \n    task='classification'\n)\nanalyzer.encoding_module.enabled = False # AutoGluon will handle encoding\n\nprint(analyzer)\n\nanalyzer.run()\n</pre> from jarvais.analyzer import Analyzer from rich import print  analyzer = Analyzer(     data=df,      output_dir='./outputs/analyzer',     categorical_columns= [       \"Sex\",       \"T Stage\",       \"N Stage\",       \"Stage\",       \"Smoking Status\",       \"Disease Site\",       \"death\",       \"HPV Combined\",       \"Chemotherapy\"     ],     continuous_columns = [       \"age at dx\",       \"Dose\"     ],     target_variable='death',      task='classification' ) analyzer.encoding_module.enabled = False # AutoGluon will handle encoding  print(analyzer)  analyzer.run() <pre>14:45:47 [warning  ] Date columns not specified. Inferring from remaining columns. [jarvais] call=analyzer.__init__:76\n</pre> <pre>Analyzer(\n    AnalyzerSettings(\n        output_dir=PosixPath('outputs/analyzer'),\n        categorical_columns=[\n            'Sex',\n            'T Stage',\n            'N Stage',\n            'Stage',\n            'Smoking Status',\n            'Disease Site',\n            'death',\n            'HPV Combined',\n            'Chemotherapy'\n        ],\n        continuous_columns=['age at dx', 'Dose'],\n        date_columns=[],\n        task='classification',\n        target_variable='death',\n        generate_report=True,\n        settings_path=None,\n        settings_schema_path=None,\n        missingness=MissingnessModule(\n            categorical_strategy={\n                'Sex': 'unknown',\n                'T Stage': 'unknown',\n                'N Stage': 'unknown',\n                'Stage': 'unknown',\n                'Smoking Status': 'unknown',\n                'Disease Site': 'unknown',\n                'death': 'unknown',\n                'HPV Combined': 'unknown',\n                'Chemotherapy': 'unknown'\n            },\n            continuous_strategy={'age at dx': 'median', 'Dose': 'median'},\n            enabled=True\n        ),\n        outlier=OutlierModule(\n            categorical_strategy={\n                'Sex': 'frequency',\n                'T Stage': 'frequency',\n                'N Stage': 'frequency',\n                'Stage': 'frequency',\n                'Smoking Status': 'frequency',\n                'Disease Site': 'frequency',\n                'death': 'frequency',\n                'HPV Combined': 'frequency',\n                'Chemotherapy': 'frequency'\n            },\n            continuous_strategy={'age at dx': 'none', 'Dose': 'none'},\n            threshold=0.01,\n            enabled=True\n        ),\n        encoding=OneHotEncodingModule(\n            columns=[\n                'Sex',\n                'T Stage',\n                'N Stage',\n                'Stage',\n                'Smoking Status',\n                'Disease Site',\n                'HPV Combined',\n                'Chemotherapy'\n            ],\n            target_variable='death',\n            prefix_sep='|',\n            enabled=False\n        ),\n        visualization=VisualizationModule(\n            plots=['corr', 'pairplot', 'umap', 'frequency_table', 'multiplot'],\n            enabled=True\n        )\n    )\n)\n</pre> <pre>         [info     ] Performing missingness analysis... [jarvais] call=missingness.__call__:43\n         [info     ] Performing outlier analysis... [jarvais] call=outlier.__call__:53\n         [info     ] Plotting Correlation Matrix... [jarvais] call=visualization.__call__:115\n</pre> <pre>+-----------------------+-------------------+-----------+-------------+\n|                       |                   | Missing   | Overall     |\n+=======================+===================+===========+=============+\n| n                     |                   |           | 3346        |\n+-----------------------+-------------------+-----------+-------------+\n| age at dx, mean (SD)  |                   | 0         | 62.3 (11.6) |\n+-----------------------+-------------------+-----------+-------------+\n| Dose, mean (SD)       |                   | 0         | 66.7 (5.8)  |\n+-----------------------+-------------------+-----------+-------------+\n| Sex, n (%)            | Female            |           | 686 (20.5)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Male              |           | 2660 (79.5) |\n+-----------------------+-------------------+-----------+-------------+\n| T Stage, n (%)        | None              |           | 12 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T0                |           | 167 (5.0)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1                |           | 454 (13.6)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1 (2)            |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1a               |           | 179 (5.3)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1b               |           | 88 (2.6)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2                |           | 927 (27.7)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2 (2)            |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2a               |           | 4 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2b               |           | 5 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T3                |           | 861 (25.7)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T3 (2)            |           | 3 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T4                |           | 116 (3.5)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T4a               |           | 358 (10.7)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T4b               |           | 121 (3.6)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | TX                |           | 4 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Tis               |           | 44 (1.3)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | rT0               |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n| N Stage, n (%)        | N0                |           | 1147 (34.3) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N1                |           | 344 (10.3)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2                |           | 182 (5.4)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2a               |           | 125 (3.7)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2b               |           | 791 (23.6)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2c               |           | 532 (15.9)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N3                |           | 170 (5.1)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N3a               |           | 13 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N3b               |           | 28 (0.8)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | NX                |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | None              |           | 13 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n| Stage, n (%)          | 0                 |           | 44 (1.3)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | I                 |           | 352 (10.5)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IB                |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | II                |           | 400 (12.0)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIA               |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIB               |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | III               |           | 605 (18.1)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIIA              |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIIC              |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IV                |           | 12 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IVA               |           | 1581 (47.3) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IVB               |           | 309 (9.2)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IVC               |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | None              |           | 27 (0.8)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | X                 |           | 6 (0.2)     |\n+-----------------------+-------------------+-----------+-------------+\n| Smoking Status, n (%) | Current           |           | 1139 (34.0) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Ex-smoker         |           | 1290 (38.6) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Non-smoker        |           | 872 (26.1)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | unknown           |           | 45 (1.3)    |\n+-----------------------+-------------------+-----------+-------------+\n| Disease Site, n (%)   | benign tumor      |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | esophagus         |           | 33 (1.0)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | hypopharynx       |           | 162 (4.8)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | lacrimal gland    |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | larynx            |           | 877 (26.2)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | lip &amp; oral cavity |           | 100 (3.0)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | nasal cavity      |           | 62 (1.9)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | nasopharynx       |           | 355 (10.6)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | orbit             |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | oropharynx        |           | 1501 (44.9) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | other             |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | paraganglioma     |           | 7 (0.2)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | paranasal sinus   |           | 28 (0.8)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | salivary glands   |           | 4 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | sarcoma           |           | 20 (0.6)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | skin              |           | 24 (0.7)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | unknown           |           | 168 (5.0)   |\n+-----------------------+-------------------+-----------+-------------+\n| death, n (%)          | 0                 |           | 2288 (68.4) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | 1                 |           | 1058 (31.6) |\n+-----------------------+-------------------+-----------+-------------+\n| HPV Combined, n (%)   | 1.0               |           | 1139 (34.0) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | None              |           | 2207 (66.0) |\n+-----------------------+-------------------+-----------+-------------+\n| Chemotherapy, n (%)   | 0                 |           | 1923 (57.5) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | 1                 |           | 1423 (42.5) |\n+-----------------------+-------------------+-----------+-------------+\n\nOutlier Report:\n  - No Outliers found in Sex\n  - Outliers found in T Stage: ['nan: 12 out of 3346', 'T2b: 5 out of 3346', 'T2a: 4 out of 3346', 'TX: 4 out of 3346', 'T3 (2): 3 out of 3346', 'T2 (2): 1 out of 3346', 'T1 (2): 1 out of 3346', 'rT0: 1 out of 3346']\n  - Outliers found in N Stage: ['N3b: 28 out of 3346', 'N3a: 13 out of 3346', 'nan: 13 out of 3346', 'NX: 1 out of 3346']\n  - Outliers found in Stage: ['nan: 27 out of 3346', 'IV: 12 out of 3346', 'X: 6 out of 3346', 'IIA: 2 out of 3346', 'IIIA: 2 out of 3346', 'IIIC: 2 out of 3346', 'IVC: 2 out of 3346', 'IB: 1 out of 3346', 'IIB: 1 out of 3346']\n  - No Outliers found in Smoking Status\n  - Outliers found in Disease Site: ['paranasal sinus: 28 out of 3346', 'skin: 24 out of 3346', 'sarcoma: 20 out of 3346', 'paraganglioma: 7 out of 3346', 'salivary glands: 4 out of 3346', 'other: 2 out of 3346', 'benign tumor: 1 out of 3346', 'lacrimal gland: 1 out of 3346', 'orbit: 1 out of 3346']\n  - No Outliers found in death\n  - No Outliers found in HPV Combined\n  - No Outliers found in Chemotherapy\n\n</pre> <pre>         [info     ] Plotting Pairplot...           [jarvais] call=visualization.__call__:118\n14:45:48 [info     ] Plotting UMAP...               [jarvais] call=visualization.__call__:124\n14:45:54 [info     ] Plotting Frequency Table...    [jarvais] call=visualization.__call__:121\n14:46:03 [info     ] Plotting Multiplot...          [jarvais] call=visualization.__call__:136\n14:46:06 [warning  ] One-hot encoding is disabled.  [jarvais] call=encoding.__call__:40\nFont MPDFAA+Inter28ptBold is missing the following glyphs: '\n' (\\n)\n</pre> In\u00a0[6]: Copied! <pre>from jarvais.trainer import TrainerSupervised\n\ndf = pd.read_csv('./outputs/analyzer/updated_data.csv')\n\ntrainer = TrainerSupervised(task='binary', output_dir='./outputs/trainer')\ntrainer.run(df, 'death')\n</pre> from jarvais.trainer import TrainerSupervised  df = pd.read_csv('./outputs/analyzer/updated_data.csv')  trainer = TrainerSupervised(task='binary', output_dir='./outputs/trainer') trainer.run(df, 'death') <pre>Training fold 1/5...\nFold 1 score: 0.8136486155354079\nTraining fold 2/5...\nFold 2 score: 0.7975441509916009\nTraining fold 3/5...\nFold 3 score: 0.7589550761923609\nTraining fold 4/5...\nFold 4 score: 0.7952386488192115\nTraining fold 5/5...\nFold 5 score: 0.7860460795243404\n\nModel Leaderboard (Displays values in \"mean [min, max]\" format across training folds)\n------------------------------------------------------------------------------------\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| model                 | score_test               | score_val                | score_train              |\n+=======================+==========================+==========================+==========================+\n| WeightedEnsemble_L2   | AUROC 0.76 [0.75, 0.76]  | AUROC 0.79 [0.76, 0.81]  | AUROC 0.85 [0.81, 0.91]  |\n|                       | F1: 0.49 [0.46, 0.51]    | F1: 0.52 [0.47, 0.6]     | F1: 0.6 [0.52, 0.7]      |\n|                       | AUPRC: 0.6 [0.59, 0.62]  | AUPRC: 0.63 [0.57, 0.69] | AUPRC: 0.71 [0.65, 0.78] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| CatBoost              | AUROC 0.75 [0.75, 0.76]  | AUROC 0.78 [0.74, 0.81]  | AUROC 0.84 [0.83, 0.85]  |\n|                       | F1: 0.46 [0.44, 0.5]     | F1: 0.51 [0.47, 0.56]    | F1: 0.59 [0.54, 0.63]    |\n|                       | AUPRC: 0.58 [0.58, 0.6]  | AUPRC: 0.64 [0.59, 0.68] | AUPRC: 0.7 [0.69, 0.71]  |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| NeuralNetTorch        | AUROC 0.75 [0.75, 0.76]  | AUROC 0.78 [0.75, 0.8]   | AUROC 0.82 [0.78, 0.85]  |\n|                       | F1: 0.45 [0.35, 0.52]    | F1: 0.49 [0.41, 0.55]    | F1: 0.56 [0.39, 0.66]    |\n|                       | AUPRC: 0.59 [0.57, 0.61] | AUPRC: 0.62 [0.56, 0.67] | AUPRC: 0.68 [0.57, 0.73] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| XGBoost               | AUROC 0.75 [0.74, 0.76]  | AUROC 0.76 [0.72, 0.78]  | AUROC 0.89 [0.87, 0.89]  |\n|                       | F1: 0.49 [0.43, 0.54]    | F1: 0.5 [0.46, 0.55]     | F1: 0.65 [0.58, 0.69]    |\n|                       | AUPRC: 0.61 [0.58, 0.64] | AUPRC: 0.61 [0.57, 0.64] | AUPRC: 0.75 [0.71, 0.77] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| LightGBM              | AUROC 0.75 [0.74, 0.76]  | AUROC 0.76 [0.71, 0.79]  | AUROC 0.88 [0.87, 0.9]   |\n|                       | F1: 0.48 [0.44, 0.52]    | F1: 0.5 [0.4, 0.57]      | F1: 0.65 [0.62, 0.71]    |\n|                       | AUPRC: 0.59 [0.56, 0.62] | AUPRC: 0.62 [0.53, 0.66] | AUPRC: 0.74 [0.73, 0.78] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| LightGBMXT            | AUROC 0.75 [0.74, 0.76]  | AUROC 0.77 [0.74, 0.79]  | AUROC 0.84 [0.83, 0.85]  |\n|                       | F1: 0.47 [0.46, 0.49]    | F1: 0.52 [0.47, 0.59]    | F1: 0.59 [0.56, 0.61]    |\n|                       | AUPRC: 0.58 [0.58, 0.59] | AUPRC: 0.63 [0.58, 0.69] | AUPRC: 0.69 [0.68, 0.71] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| NeuralNetFastAI       | AUROC 0.74 [0.71, 0.76]  | AUROC 0.77 [0.72, 0.8]   | AUROC 0.8 [0.79, 0.81]   |\n|                       | F1: 0.51 [0.45, 0.54]    | F1: 0.56 [0.49, 0.61]    | F1: 0.59 [0.55, 0.6]     |\n|                       | AUPRC: 0.59 [0.55, 0.61] | AUPRC: 0.64 [0.58, 0.68] | AUPRC: 0.66 [0.63, 0.68] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| SimpleRegressionModel | AUROC 0.73 [0.73, 0.73]  | AUROC 0.77 [0.75, 0.78]  | AUROC 0.78 [0.78, 0.78]  |\n|                       | F1: 0.47 [0.46, 0.49]    | F1: 0.52 [0.48, 0.55]    | F1: 0.53 [0.52, 0.54]    |\n|                       | AUPRC: 0.59 [0.58, 0.6]  | AUPRC: 0.64 [0.59, 0.66] | AUPRC: 0.64 [0.63, 0.65] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| LightGBMLarge         | AUROC 0.72 [0.71, 0.74]  | AUROC 0.74 [0.67, 0.77]  | AUROC 0.96 [0.94, 0.97]  |\n|                       | F1: 0.42 [0.33, 0.49]    | F1: 0.45 [0.34, 0.51]    | F1: 0.76 [0.7, 0.81]     |\n|                       | AUPRC: 0.55 [0.49, 0.6]  | AUPRC: 0.58 [0.5, 0.62]  | AUPRC: 0.84 [0.82, 0.87] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| RandomForestGini      | AUROC 0.71 [0.7, 0.72]   | AUROC 0.74 [0.71, 0.77]  | AUROC 1.0 [1.0, 1.0]     |\n|                       | F1: 0.49 [0.48, 0.5]     | F1: 0.5 [0.46, 0.55]     | F1: 1.0 [0.99, 1.0]      |\n|                       | AUPRC: 0.58 [0.57, 0.59] | AUPRC: 0.6 [0.54, 0.63]  | AUPRC: 1.0 [1.0, 1.0]    |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| RandomForestEntr      | AUROC 0.71 [0.7, 0.72]   | AUROC 0.74 [0.71, 0.77]  | AUROC 1.0 [1.0, 1.0]     |\n|                       | F1: 0.49 [0.47, 0.5]     | F1: 0.51 [0.47, 0.55]    | F1: 1.0 [0.99, 1.0]      |\n|                       | AUPRC: 0.57 [0.56, 0.58] | AUPRC: 0.6 [0.56, 0.63]  | AUPRC: 1.0 [1.0, 1.0]    |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| ExtraTreesGini        | AUROC 0.7 [0.69, 0.7]    | AUROC 0.73 [0.7, 0.77]   | AUROC 1.0 [1.0, 1.0]     |\n|                       | F1: 0.49 [0.46, 0.5]     | F1: 0.5 [0.47, 0.53]     | F1: 1.0 [0.99, 1.0]      |\n|                       | AUPRC: 0.57 [0.55, 0.58] | AUPRC: 0.59 [0.55, 0.62] | AUPRC: 1.0 [1.0, 1.0]    |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| ExtraTreesEntr        | AUROC 0.7 [0.69, 0.71]   | AUROC 0.73 [0.71, 0.76]  | AUROC 1.0 [1.0, 1.0]     |\n|                       | F1: 0.48 [0.46, 0.5]     | F1: 0.5 [0.46, 0.54]     | F1: 1.0 [0.99, 1.0]      |\n|                       | AUPRC: 0.56 [0.55, 0.58] | AUPRC: 0.59 [0.55, 0.63] | AUPRC: 1.0 [1.0, 1.0]    |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| KNeighborsUnif        | AUROC 0.6 [0.58, 0.61]   | AUROC 0.59 [0.56, 0.63]  | AUROC 0.78 [0.77, 0.78]  |\n|                       | F1: 0.37 [0.35, 0.38]    | F1: 0.34 [0.29, 0.41]    | F1: 0.51 [0.51, 0.52]    |\n|                       | AUPRC: 0.49 [0.47, 0.5]  | AUPRC: 0.47 [0.42, 0.52] | AUPRC: 0.63 [0.62, 0.63] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n| KNeighborsDist        | AUROC 0.57 [0.55, 0.58]  | AUROC 0.58 [0.55, 0.61]  | AUROC 0.9 [0.9, 0.91]    |\n|                       | F1: 0.33 [0.32, 0.35]    | F1: 0.32 [0.27, 0.38]    | F1: 0.67 [0.65, 0.68]    |\n|                       | AUPRC: 0.46 [0.45, 0.47] | AUPRC: 0.45 [0.41, 0.49] | AUPRC: 0.78 [0.77, 0.79] |\n+-----------------------+--------------------------+--------------------------+--------------------------+\n</pre> In\u00a0[7]: Copied! <pre>from jarvais.explainer import Explainer\n\nsensitive_features = {k: trainer.X_test[k] for k in ['N Stage', 'Disease Site', 'Sex']}\n\nexp = Explainer.from_trainer(trainer, sensitive_features=sensitive_features)\nexp.run()\n</pre> from jarvais.explainer import Explainer  sensitive_features = {k: trainer.X_test[k] for k in ['N Stage', 'Disease Site', 'Sex']}  exp = Explainer.from_trainer(trainer, sensitive_features=sensitive_features) exp.run() <pre>\u26a0\ufe0f  **Possible Bias Detected in N Stage** \u26a0\ufe0f\n\n=== Subgroup Analysis for 'N Stage' Using OLS Regression ===\n\nModel Statistics:\n    R-squared:                  0.025\n    F-statistic:                2.461\n    F-statistic p-value:        0.0169\n    AIC:                        1126.48\n    Log-Likelihood:             -555.24\nModel Coefficients:\n    +---------------+---------------+------------------+\n    | Feature       |   Coefficient |   Standard Error |\n    +===============+===============+==================+\n    | const         |         0.438 |            0.029 |\n    +---------------+---------------+------------------+\n    | N Stage_N0    |         0.127 |            0.043 |\n    +---------------+---------------+------------------+\n    | N Stage_N1    |         0.131 |            0.067 |\n    +---------------+---------------+------------------+\n    | N Stage_N2    |        -0.187 |            0.086 |\n    +---------------+---------------+------------------+\n    | N Stage_N2a   |        -0.068 |            0.097 |\n    +---------------+---------------+------------------+\n    | N Stage_N2b   |         0.088 |            0.051 |\n    +---------------+---------------+------------------+\n    | N Stage_N2c   |         0.207 |            0.056 |\n    +---------------+---------------+------------------+\n    | N Stage_N3    |         0.069 |            0.099 |\n    +---------------+---------------+------------------+\n    | N Stage_Other |         0.071 |            0.151 |\n    +---------------+---------------+------------------+\n</pre> <pre>\n=== Subgroup Analysis for 'N Stage' using FairLearn ===\n\n    +------------------------------+---------------------+---------------------+----------------------+----------------------+\n    |                              | N0                  | N1                  | N2                   | N2a                  |\n    +==============================+=====================+=====================+======================+======================+\n    | mean_prediction              | 0.18326693227091634 | 0.23880597014925373 | 0.08108108108108109  | 0.07142857142857142  |\n    +------------------------------+---------------------+---------------------+----------------------+----------------------+\n    | false_positive_rate          | 0.13142857142857142 | 0.11363636363636363 | 0.030303030303030304 | 0.045454545454545456 |\n    +------------------------------+---------------------+---------------------+----------------------+----------------------+\n    | Relative mean_prediction     | 1.000 \u2705            | 1.303 \u2705            | 0.442 \u2705             | 0.390 \u2705             |\n    +------------------------------+---------------------+---------------------+----------------------+----------------------+\n    | Relative false_positive_rate | 1.000 \u2705            | 0.865 \u2705            | 0.231 \u2705             | 0.346 \u2705             |\n    +------------------------------+---------------------+---------------------+----------------------+----------------------+ \n\nSince target_class not specified, SHAP will explain predictions for each class\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [04:05&lt;00:00,  2.46s/it]\n</pre>"},{"location":"tutorials/regression/","title":"Regression","text":"In\u00a0[1]: Copied! <pre>import sys\nimport pandas as pd\nimport os\n\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n\nsys.path.append(project_root)\ndata_dir = os.path.join(project_root, 'data')\n\ndata_file_path = os.path.join(data_dir, 'RADCURE_processed_clinical.csv')\ndf = pd.read_csv(data_file_path, index_col=0)\n\ndf.drop(columns=[\"Study ID\", \"survival_time\", \"death\"], inplace=True)\n</pre> import sys import pandas as pd import os  project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))  sys.path.append(project_root) data_dir = os.path.join(project_root, 'data')  data_file_path = os.path.join(data_dir, 'RADCURE_processed_clinical.csv') df = pd.read_csv(data_file_path, index_col=0)  df.drop(columns=[\"Study ID\", \"survival_time\", \"death\"], inplace=True) In\u00a0[2]: Copied! <pre>from jarvais.analyzer import Analyzer\nfrom rich import print\n\nanalyzer = Analyzer(\n    data=df, \n    output_dir='./outputs/analyzer',\n    categorical_columns= [\n      \"Sex\",\n      \"T Stage\",\n      \"N Stage\",\n      \"Stage\",\n      \"Smoking Status\",\n      \"Disease Site\",\n      \"HPV Combined\",\n      \"Chemotherapy\"\n    ],\n    continuous_columns = [\n      \"age at dx\",\n      \"Dose\"\n    ],\n    target_variable='Dose', \n    task='classification'\n)\nanalyzer.encoding_module.enabled = False # AutoGluon will handle encoding\n\nprint(analyzer)\n\nanalyzer.run()\n</pre> from jarvais.analyzer import Analyzer from rich import print  analyzer = Analyzer(     data=df,      output_dir='./outputs/analyzer',     categorical_columns= [       \"Sex\",       \"T Stage\",       \"N Stage\",       \"Stage\",       \"Smoking Status\",       \"Disease Site\",       \"HPV Combined\",       \"Chemotherapy\"     ],     continuous_columns = [       \"age at dx\",       \"Dose\"     ],     target_variable='Dose',      task='classification' ) analyzer.encoding_module.enabled = False # AutoGluon will handle encoding  print(analyzer)  analyzer.run() <pre>/home/joshua-siraj/Documents/CDI/jarvais/.pixi/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n15:02:49 [warning  ] Date columns not specified. Inferring from remaining columns. [jarvais] call=analyzer.__init__:76\n</pre> <pre>Analyzer(\n    AnalyzerSettings(\n        output_dir=PosixPath('outputs/analyzer'),\n        categorical_columns=[\n            'Sex',\n            'T Stage',\n            'N Stage',\n            'Stage',\n            'Smoking Status',\n            'Disease Site',\n            'HPV Combined',\n            'Chemotherapy'\n        ],\n        continuous_columns=['age at dx', 'Dose'],\n        date_columns=[],\n        task='classification',\n        target_variable='Dose',\n        generate_report=True,\n        settings_path=None,\n        settings_schema_path=None,\n        missingness=MissingnessModule(\n            categorical_strategy={\n                'Sex': 'unknown',\n                'T Stage': 'unknown',\n                'N Stage': 'unknown',\n                'Stage': 'unknown',\n                'Smoking Status': 'unknown',\n                'Disease Site': 'unknown',\n                'HPV Combined': 'unknown',\n                'Chemotherapy': 'unknown'\n            },\n            continuous_strategy={'age at dx': 'median', 'Dose': 'median'},\n            enabled=True\n        ),\n        outlier=OutlierModule(\n            categorical_strategy={\n                'Sex': 'frequency',\n                'T Stage': 'frequency',\n                'N Stage': 'frequency',\n                'Stage': 'frequency',\n                'Smoking Status': 'frequency',\n                'Disease Site': 'frequency',\n                'HPV Combined': 'frequency',\n                'Chemotherapy': 'frequency'\n            },\n            continuous_strategy={'age at dx': 'none', 'Dose': 'none'},\n            threshold=0.01,\n            enabled=True\n        ),\n        encoding=OneHotEncodingModule(\n            columns=[\n                'Sex',\n                'T Stage',\n                'N Stage',\n                'Stage',\n                'Smoking Status',\n                'Disease Site',\n                'HPV Combined',\n                'Chemotherapy'\n            ],\n            target_variable='Dose',\n            prefix_sep='|',\n            enabled=False\n        ),\n        visualization=VisualizationModule(\n            plots=['corr', 'pairplot', 'umap', 'frequency_table', 'multiplot'],\n            enabled=True\n        )\n    )\n)\n</pre> <pre>         [info     ] Performing missingness analysis... [jarvais] call=missingness.__call__:43\n         [info     ] Performing outlier analysis... [jarvais] call=outlier.__call__:53\n         [info     ] Plotting Correlation Matrix... [jarvais] call=visualization.__call__:115\n</pre> <pre>+-----------------------+-------------------+-----------+-------------+\n|                       |                   | Missing   | Overall     |\n+=======================+===================+===========+=============+\n| n                     |                   |           | 3346        |\n+-----------------------+-------------------+-----------+-------------+\n| age at dx, mean (SD)  |                   | 0         | 62.3 (11.6) |\n+-----------------------+-------------------+-----------+-------------+\n| Dose, mean (SD)       |                   | 0         | 66.7 (5.8)  |\n+-----------------------+-------------------+-----------+-------------+\n| Sex, n (%)            | Female            |           | 686 (20.5)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Male              |           | 2660 (79.5) |\n+-----------------------+-------------------+-----------+-------------+\n| T Stage, n (%)        | None              |           | 12 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T0                |           | 167 (5.0)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1                |           | 454 (13.6)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1 (2)            |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1a               |           | 179 (5.3)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T1b               |           | 88 (2.6)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2                |           | 927 (27.7)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2 (2)            |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2a               |           | 4 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T2b               |           | 5 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T3                |           | 861 (25.7)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T3 (2)            |           | 3 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T4                |           | 116 (3.5)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T4a               |           | 358 (10.7)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | T4b               |           | 121 (3.6)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | TX                |           | 4 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Tis               |           | 44 (1.3)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | rT0               |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n| N Stage, n (%)        | N0                |           | 1147 (34.3) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N1                |           | 344 (10.3)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2                |           | 182 (5.4)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2a               |           | 125 (3.7)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2b               |           | 791 (23.6)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N2c               |           | 532 (15.9)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N3                |           | 170 (5.1)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N3a               |           | 13 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | N3b               |           | 28 (0.8)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | NX                |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | None              |           | 13 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n| Stage, n (%)          | 0                 |           | 44 (1.3)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | I                 |           | 352 (10.5)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IB                |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | II                |           | 400 (12.0)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIA               |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIB               |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | III               |           | 605 (18.1)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIIA              |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IIIC              |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IV                |           | 12 (0.4)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IVA               |           | 1581 (47.3) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IVB               |           | 309 (9.2)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | IVC               |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | None              |           | 27 (0.8)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | X                 |           | 6 (0.2)     |\n+-----------------------+-------------------+-----------+-------------+\n| Smoking Status, n (%) | Current           |           | 1139 (34.0) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Ex-smoker         |           | 1290 (38.6) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | Non-smoker        |           | 872 (26.1)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | unknown           |           | 45 (1.3)    |\n+-----------------------+-------------------+-----------+-------------+\n| Disease Site, n (%)   | benign tumor      |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | esophagus         |           | 33 (1.0)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | hypopharynx       |           | 162 (4.8)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | lacrimal gland    |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | larynx            |           | 877 (26.2)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | lip &amp; oral cavity |           | 100 (3.0)   |\n+-----------------------+-------------------+-----------+-------------+\n|                       | nasal cavity      |           | 62 (1.9)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | nasopharynx       |           | 355 (10.6)  |\n+-----------------------+-------------------+-----------+-------------+\n|                       | orbit             |           | 1 (0.0)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | oropharynx        |           | 1501 (44.9) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | other             |           | 2 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | paraganglioma     |           | 7 (0.2)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | paranasal sinus   |           | 28 (0.8)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | salivary glands   |           | 4 (0.1)     |\n+-----------------------+-------------------+-----------+-------------+\n|                       | sarcoma           |           | 20 (0.6)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | skin              |           | 24 (0.7)    |\n+-----------------------+-------------------+-----------+-------------+\n|                       | unknown           |           | 168 (5.0)   |\n+-----------------------+-------------------+-----------+-------------+\n| HPV Combined, n (%)   | 1.0               |           | 1139 (34.0) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | None              |           | 2207 (66.0) |\n+-----------------------+-------------------+-----------+-------------+\n| Chemotherapy, n (%)   | 0                 |           | 1923 (57.5) |\n+-----------------------+-------------------+-----------+-------------+\n|                       | 1                 |           | 1423 (42.5) |\n+-----------------------+-------------------+-----------+-------------+\n\nOutlier Report:\n  - No Outliers found in Sex\n  - Outliers found in T Stage: ['nan: 12 out of 3346', 'T2b: 5 out of 3346', 'T2a: 4 out of 3346', 'TX: 4 out of 3346', 'T3 (2): 3 out of 3346', 'T2 (2): 1 out of 3346', 'T1 (2): 1 out of 3346', 'rT0: 1 out of 3346']\n  - Outliers found in N Stage: ['N3b: 28 out of 3346', 'N3a: 13 out of 3346', 'nan: 13 out of 3346', 'NX: 1 out of 3346']\n  - Outliers found in Stage: ['nan: 27 out of 3346', 'IV: 12 out of 3346', 'X: 6 out of 3346', 'IIA: 2 out of 3346', 'IIIA: 2 out of 3346', 'IIIC: 2 out of 3346', 'IVC: 2 out of 3346', 'IB: 1 out of 3346', 'IIB: 1 out of 3346']\n  - No Outliers found in Smoking Status\n  - Outliers found in Disease Site: ['paranasal sinus: 28 out of 3346', 'skin: 24 out of 3346', 'sarcoma: 20 out of 3346', 'paraganglioma: 7 out of 3346', 'salivary glands: 4 out of 3346', 'other: 2 out of 3346', 'benign tumor: 1 out of 3346', 'lacrimal gland: 1 out of 3346', 'orbit: 1 out of 3346']\n  - No Outliers found in HPV Combined\n  - No Outliers found in Chemotherapy\n\n</pre> <pre>         [info     ] Plotting Pairplot...           [jarvais] call=visualization.__call__:118\n15:02:50 [info     ] Plotting UMAP...               [jarvais] call=visualization.__call__:124\n15:03:00 [info     ] Plotting Frequency Table...    [jarvais] call=visualization.__call__:121\n15:03:07 [info     ] Plotting Multiplot...          [jarvais] call=visualization.__call__:136\n15:03:11 [warning  ] One-hot encoding is disabled.  [jarvais] call=encoding.__call__:40\nFont MPDFAA+Inter28ptBold is missing the following glyphs: '\n' (\\n)\n</pre> In\u00a0[3]: Copied! <pre>from jarvais.trainer import TrainerSupervised\n\ndf = pd.read_csv('./outputs/analyzer/updated_data.csv')\n\ntrainer = TrainerSupervised(task='regression', output_dir='./outputs/trainer')\ntrainer.run(df, 'Dose')\n</pre> from jarvais.trainer import TrainerSupervised  df = pd.read_csv('./outputs/analyzer/updated_data.csv')  trainer = TrainerSupervised(task='regression', output_dir='./outputs/trainer') trainer.run(df, 'Dose') <pre>Training fold 1/5...\nFold 1 score: 0.6668817529452749\nTraining fold 2/5...\nFold 2 score: 0.5944331638262828\nTraining fold 3/5...\nFold 3 score: 0.6045543370974688\nTraining fold 4/5...\nFold 4 score: 0.6360018803349704\nTraining fold 5/5...\nFold 5 score: 0.590947874967624\n\nModel Leaderboard (Displays values in \"mean [min, max]\" format across training folds)\n------------------------------------------------------------------------------------\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| model                 | score_test                 | score_val                  | score_train                |\n+=======================+============================+============================+============================+\n| WeightedEnsemble_L2   | R2 0.62 [0.61, 0.63]       | R2 0.62 [0.59, 0.67]       | R2 0.71 [0.69, 0.76]       |\n|                       | RMSE: -3.55 [-3.6, -3.49]  | RMSE: -3.54 [-3.87, -3.37] | RMSE: -3.09 [-3.21, -2.79] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| LightGBMXT            | R2 0.61 [0.6, 0.62]        | R2 0.6 [0.58, 0.64]        | R2 0.68 [0.65, 0.71]       |\n|                       | RMSE: -3.61 [-3.64, -3.58] | RMSE: -3.61 [-3.88, -3.47] | RMSE: -3.27 [-3.39, -3.04] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| LightGBM              | R2 0.61 [0.6, 0.61]        | R2 0.58 [0.54, 0.64]       | R2 0.7 [0.68, 0.71]        |\n|                       | RMSE: -3.63 [-3.67, -3.59] | RMSE: -3.7 [-4.09, -3.52]  | RMSE: -3.14 [-3.2, -3.1]   |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| CatBoost              | R2 0.61 [0.59, 0.63]       | R2 0.6 [0.57, 0.65]        | R2 0.67 [0.65, 0.7]        |\n|                       | RMSE: -3.62 [-3.69, -3.5]  | RMSE: -3.63 [-3.97, -3.47] | RMSE: -3.29 [-3.43, -3.12] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| NeuralNetFastAI       | R2 0.61 [0.59, 0.62]       | R2 0.6 [0.56, 0.66]        | R2 0.65 [0.61, 0.68]       |\n|                       | RMSE: -3.6 [-3.7, -3.54]   | RMSE: -3.63 [-3.94, -3.42] | RMSE: -3.39 [-3.56, -3.19] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| XGBoost               | R2 0.6 [0.58, 0.62]        | R2 0.58 [0.54, 0.62]       | R2 0.73 [0.69, 0.82]       |\n|                       | RMSE: -3.63 [-3.73, -3.56] | RMSE: -3.74 [-4.06, -3.54] | RMSE: -2.95 [-3.15, -2.43] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| RandomForestMSE       | R2 0.58 [0.57, 0.59]       | R2 0.55 [0.5, 0.59]        | R2 0.93 [0.93, 0.94]       |\n|                       | RMSE: -3.74 [-3.78, -3.7]  | RMSE: -3.86 [-4.19, -3.72] | RMSE: -1.5 [-1.54, -1.42]  |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| ExtraTreesMSE         | R2 0.57 [0.56, 0.58]       | R2 0.54 [0.5, 0.6]         | R2 0.93 [0.93, 0.94]       |\n|                       | RMSE: -3.79 [-3.83, -3.74] | RMSE: -3.89 [-4.21, -3.7]  | RMSE: -1.51 [-1.53, -1.43] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| LightGBMLarge         | R2 0.57 [0.56, 0.58]       | R2 0.56 [0.52, 0.59]       | R2 0.83 [0.79, 0.86]       |\n|                       | RMSE: -3.78 [-3.83, -3.73] | RMSE: -3.83 [-4.13, -3.72] | RMSE: -2.37 [-2.69, -2.16] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| NeuralNetTorch        | R2 0.54 [0.52, 0.55]       | R2 0.51 [0.44, 0.6]        | R2 0.57 [0.51, 0.6]        |\n|                       | RMSE: -3.93 [-3.99, -3.86] | RMSE: -4.01 [-4.52, -3.67] | RMSE: -3.78 [-3.97, -3.65] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| SimpleRegressionModel | R2 0.41 [0.4, 0.41]        | R2 0.4 [0.38, 0.43]        | R2 0.4 [0.39, 0.41]        |\n|                       | RMSE: -4.44 [-4.45, -4.44] | RMSE: -4.46 [-4.77, -4.22] | RMSE: -4.44 [-4.5, -4.37]  |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| KNeighborsDist        | R2 -0.13 [-0.17, -0.1]     | R2 -0.19 [-0.3, -0.09]     | R2 0.27 [0.26, 0.27]       |\n|                       | RMSE: -6.14 [-6.23, -6.05] | RMSE: -6.27 [-6.54, -6.06] | RMSE: -4.93 [-5.01, -4.83] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n| KNeighborsUnif        | R2 -0.08 [-0.1, -0.05]     | R2 -0.11 [-0.14, -0.05]    | R2 0.19 [0.19, 0.21]       |\n|                       | RMSE: -6.01 [-6.05, -5.93] | RMSE: -6.04 [-6.43, -5.68] | RMSE: -5.16 [-5.25, -5.05] |\n+-----------------------+----------------------------+----------------------------+----------------------------+\n</pre> In\u00a0[4]: Copied! <pre>from jarvais.explainer import Explainer\n\nsensitive_features = {k: trainer.X_test[k] for k in ['N Stage', 'Disease Site', 'Sex']}\n\nexp = Explainer.from_trainer(trainer, sensitive_features=sensitive_features)\nexp.run()\n</pre> from jarvais.explainer import Explainer  sensitive_features = {k: trainer.X_test[k] for k in ['N Stage', 'Disease Site', 'Sex']}  exp = Explainer.from_trainer(trainer, sensitive_features=sensitive_features) exp.run() <pre>\u26a0\ufe0f  **Possible Bias Detected in N Stage** \u26a0\ufe0f\n\n=== Subgroup Analysis for 'N Stage' Using OLS Regression ===\n\nModel Statistics:\n    R-squared:                  0.137\n    F-statistic:                15.034\n    F-statistic p-value:        0.0000\n    AIC:                        3174.85\n    Log-Likelihood:             -1579.42\nModel Coefficients:\n    +---------------+---------------+------------------+\n    | Feature       |   Coefficient |   Standard Error |\n    +===============+===============+==================+\n    | const         |         1.937 |            0.139 |\n    +---------------+---------------+------------------+\n    | N Stage_N0    |         1.571 |            0.206 |\n    +---------------+---------------+------------------+\n    | N Stage_N1    |         0.829 |            0.329 |\n    +---------------+---------------+------------------+\n    | N Stage_N2    |        -1.328 |            0.380 |\n    +---------------+---------------+------------------+\n    | N Stage_N2a   |         0.692 |            0.450 |\n    +---------------+---------------+------------------+\n    | N Stage_N2b   |        -0.428 |            0.221 |\n    +---------------+---------------+------------------+\n    | N Stage_N2c   |        -0.065 |            0.264 |\n    +---------------+---------------+------------------+\n    | N Stage_N3    |        -1.225 |            0.403 |\n    +---------------+---------------+------------------+\n    | N Stage_Other |         1.891 |            0.769 |\n    +---------------+---------------+------------------+\n</pre> <pre>\n=== Subgroup Analysis for 'N Stage' using FairLearn ===\n\n    +--------------------------+-------------------+-------------------+-------------------+------------------+\n    |                          | N0                | N1                | N2                | N2a              |\n    +==========================+===================+===================+===================+==================+\n    | mean_prediction          | 63.11251277751751 | 67.65180969238281 | 70.02266953631145 | 67.6926611491612 |\n    +--------------------------+-------------------+-------------------+-------------------+------------------+\n    | Relative mean_prediction | 1.000 \u2705          | 1.072 \u2705          | 1.109 \u2705          | 1.073 \u2705         |\n    +--------------------------+-------------------+-------------------+-------------------+------------------+ \n\n\u26a0\ufe0f  **Possible Bias Detected in Disease Site** \u26a0\ufe0f\n\n=== Subgroup Analysis for 'Disease Site' Using OLS Regression ===\n\nModel Statistics:\n    R-squared:                  0.156\n    F-statistic:                15.229\n    F-statistic p-value:        0.0000\n    AIC:                        3162.36\n    Log-Likelihood:             -1572.18\nModel Coefficients:\n    +--------------------------------+---------------+------------------+\n    | Feature                        |   Coefficient |   Standard Error |\n    +================================+===============+==================+\n    | const                          |         2.913 |            0.158 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_Other             |         2.073 |            0.609 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_esophagus         |         4.174 |            0.820 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_hypopharynx       |        -0.884 |            0.411 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_larynx            |         0.181 |            0.237 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_lip &amp; oral cavity |         1.705 |            0.521 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_nasal cavity      |        -0.596 |            0.629 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_nasopharynx       |        -2.388 |            0.315 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_oropharynx        |        -1.038 |            0.206 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_unknown           |        -0.314 |            0.389 |\n    +--------------------------------+---------------+------------------+\n</pre> <pre>\n=== Subgroup Analysis for 'Disease Site' using FairLearn ===\n\n    +--------------------------+-------------------+-------------------+-------------------+--------------------+\n    |                          | Other             | esophagus         | hypopharynx       | larynx             |\n    +==========================+===================+===================+===================+====================+\n    | mean_prediction          | 65.05188573201498 | 66.81039047241211 | 67.64559576246474 | 62.185764588505386 |\n    +--------------------------+-------------------+-------------------+-------------------+--------------------+\n    | Relative mean_prediction | 0.947 \u2705          | 0.972 \u2705          | 0.984 \u2705          | 0.905 \u2705           |\n    +--------------------------+-------------------+-------------------+-------------------+--------------------+ \n\n</pre>"},{"location":"tutorials/survival/","title":"Survival","text":"In\u00a0[1]: Copied! <pre>import sys\nimport pandas as pd\nimport os\n\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n\nsys.path.append(project_root)\ndata_dir = os.path.join(project_root, 'data')\n\ndata_file_path = os.path.join(data_dir, 'RADCURE_processed_clinical.csv')\ndf = pd.read_csv(data_file_path, index_col=0)\n\ndf.drop(columns=[\"Study ID\"], inplace=True)\n</pre> import sys import pandas as pd import os  project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))  sys.path.append(project_root) data_dir = os.path.join(project_root, 'data')  data_file_path = os.path.join(data_dir, 'RADCURE_processed_clinical.csv') df = pd.read_csv(data_file_path, index_col=0)  df.drop(columns=[\"Study ID\"], inplace=True) In\u00a0[2]: Copied! <pre>from jarvais.analyzer import Analyzer\nfrom rich import print\n\nanalyzer = Analyzer(\n    data=df, \n    output_dir='./survival_outputs/analyzer',\n    categorical_columns= [\n      \"Sex\",\n      \"T Stage\",\n      \"N Stage\",\n      \"Stage\",\n      \"Smoking Status\",\n      \"Disease Site\",\n      \"death\",\n      \"HPV Combined\",\n      \"Chemotherapy\"\n    ],\n    continuous_columns = [\n      \"survival_time\",\n      \"age at dx\",\n      \"Dose\"\n    ],\n    target_variable='death', \n    task='classification'\n)\n\nprint(analyzer)\n\nanalyzer.run()\n</pre> from jarvais.analyzer import Analyzer from rich import print  analyzer = Analyzer(     data=df,      output_dir='./survival_outputs/analyzer',     categorical_columns= [       \"Sex\",       \"T Stage\",       \"N Stage\",       \"Stage\",       \"Smoking Status\",       \"Disease Site\",       \"death\",       \"HPV Combined\",       \"Chemotherapy\"     ],     continuous_columns = [       \"survival_time\",       \"age at dx\",       \"Dose\"     ],     target_variable='death',      task='classification' )  print(analyzer)  analyzer.run() <pre>/home/joshua-siraj/Documents/CDI/jarvais/.pixi/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n14:41:01 [warning  ] Date columns not specified. Inferring from remaining columns. [jarvais] call=analyzer.__init__:76\n</pre> <pre>Analyzer(\n    AnalyzerSettings(\n        output_dir=PosixPath('survival_outputs/analyzer'),\n        categorical_columns=[\n            'Sex',\n            'T Stage',\n            'N Stage',\n            'Stage',\n            'Smoking Status',\n            'Disease Site',\n            'death',\n            'HPV Combined',\n            'Chemotherapy'\n        ],\n        continuous_columns=['survival_time', 'age at dx', 'Dose'],\n        date_columns=[],\n        task='classification',\n        target_variable='death',\n        generate_report=True,\n        settings_path=None,\n        settings_schema_path=None,\n        missingness=MissingnessModule(\n            categorical_strategy={\n                'Sex': 'unknown',\n                'T Stage': 'unknown',\n                'N Stage': 'unknown',\n                'Stage': 'unknown',\n                'Smoking Status': 'unknown',\n                'Disease Site': 'unknown',\n                'death': 'unknown',\n                'HPV Combined': 'unknown',\n                'Chemotherapy': 'unknown'\n            },\n            continuous_strategy={'survival_time': 'median', 'age at dx': 'median', 'Dose': 'median'},\n            enabled=True\n        ),\n        outlier=OutlierModule(\n            categorical_strategy={\n                'Sex': 'frequency',\n                'T Stage': 'frequency',\n                'N Stage': 'frequency',\n                'Stage': 'frequency',\n                'Smoking Status': 'frequency',\n                'Disease Site': 'frequency',\n                'death': 'frequency',\n                'HPV Combined': 'frequency',\n                'Chemotherapy': 'frequency'\n            },\n            continuous_strategy={'survival_time': 'none', 'age at dx': 'none', 'Dose': 'none'},\n            threshold=0.01,\n            enabled=True\n        ),\n        encoding=OneHotEncodingModule(\n            columns=[\n                'Sex',\n                'T Stage',\n                'N Stage',\n                'Stage',\n                'Smoking Status',\n                'Disease Site',\n                'HPV Combined',\n                'Chemotherapy'\n            ],\n            target_variable='death',\n            prefix_sep='|',\n            enabled=True\n        ),\n        visualization=VisualizationModule(\n            plots=['corr', 'pairplot', 'umap', 'frequency_table', 'multiplot'],\n            enabled=True\n        )\n    )\n)\n</pre> <pre>         [info     ] Performing missingness analysis... [jarvais] call=missingness.__call__:43\n         [info     ] Performing outlier analysis... [jarvais] call=outlier.__call__:53\n         [info     ] Plotting Correlation Matrix... [jarvais] call=visualization.__call__:115\n</pre> <pre>+--------------------------+-------------------+-----------+-------------+\n|                          |                   | Missing   | Overall     |\n+==========================+===================+===========+=============+\n| n                        |                   |           | 3346        |\n+--------------------------+-------------------+-----------+-------------+\n| survival_time, mean (SD) |                   | 0         | 4.1 (2.7)   |\n+--------------------------+-------------------+-----------+-------------+\n| age at dx, mean (SD)     |                   | 0         | 62.3 (11.6) |\n+--------------------------+-------------------+-----------+-------------+\n| Dose, mean (SD)          |                   | 0         | 66.7 (5.8)  |\n+--------------------------+-------------------+-----------+-------------+\n| Sex, n (%)               | Female            |           | 686 (20.5)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | Male              |           | 2660 (79.5) |\n+--------------------------+-------------------+-----------+-------------+\n| T Stage, n (%)           | None              |           | 12 (0.4)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T0                |           | 167 (5.0)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T1                |           | 454 (13.6)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T1 (2)            |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T1a               |           | 179 (5.3)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T1b               |           | 88 (2.6)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T2                |           | 927 (27.7)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T2 (2)            |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T2a               |           | 4 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T2b               |           | 5 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T3                |           | 861 (25.7)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T3 (2)            |           | 3 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T4                |           | 116 (3.5)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T4a               |           | 358 (10.7)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | T4b               |           | 121 (3.6)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | TX                |           | 4 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | Tis               |           | 44 (1.3)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | rT0               |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n| N Stage, n (%)           | N0                |           | 1147 (34.3) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N1                |           | 344 (10.3)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N2                |           | 182 (5.4)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N2a               |           | 125 (3.7)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N2b               |           | 791 (23.6)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N2c               |           | 532 (15.9)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N3                |           | 170 (5.1)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N3a               |           | 13 (0.4)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | N3b               |           | 28 (0.8)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | NX                |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | None              |           | 13 (0.4)    |\n+--------------------------+-------------------+-----------+-------------+\n| Stage, n (%)             | 0                 |           | 44 (1.3)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | I                 |           | 352 (10.5)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IB                |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | II                |           | 400 (12.0)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IIA               |           | 2 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IIB               |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | III               |           | 605 (18.1)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IIIA              |           | 2 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IIIC              |           | 2 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IV                |           | 12 (0.4)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IVA               |           | 1581 (47.3) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IVB               |           | 309 (9.2)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | IVC               |           | 2 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | None              |           | 27 (0.8)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | X                 |           | 6 (0.2)     |\n+--------------------------+-------------------+-----------+-------------+\n| Smoking Status, n (%)    | Current           |           | 1139 (34.0) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | Ex-smoker         |           | 1290 (38.6) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | Non-smoker        |           | 872 (26.1)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | unknown           |           | 45 (1.3)    |\n+--------------------------+-------------------+-----------+-------------+\n| Disease Site, n (%)      | benign tumor      |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | esophagus         |           | 33 (1.0)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | hypopharynx       |           | 162 (4.8)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | lacrimal gland    |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | larynx            |           | 877 (26.2)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | lip &amp; oral cavity |           | 100 (3.0)   |\n+--------------------------+-------------------+-----------+-------------+\n|                          | nasal cavity      |           | 62 (1.9)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | nasopharynx       |           | 355 (10.6)  |\n+--------------------------+-------------------+-----------+-------------+\n|                          | orbit             |           | 1 (0.0)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | oropharynx        |           | 1501 (44.9) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | other             |           | 2 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | paraganglioma     |           | 7 (0.2)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | paranasal sinus   |           | 28 (0.8)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | salivary glands   |           | 4 (0.1)     |\n+--------------------------+-------------------+-----------+-------------+\n|                          | sarcoma           |           | 20 (0.6)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | skin              |           | 24 (0.7)    |\n+--------------------------+-------------------+-----------+-------------+\n|                          | unknown           |           | 168 (5.0)   |\n+--------------------------+-------------------+-----------+-------------+\n| death, n (%)             | 0                 |           | 2288 (68.4) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | 1                 |           | 1058 (31.6) |\n+--------------------------+-------------------+-----------+-------------+\n| HPV Combined, n (%)      | 1.0               |           | 1139 (34.0) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | None              |           | 2207 (66.0) |\n+--------------------------+-------------------+-----------+-------------+\n| Chemotherapy, n (%)      | 0                 |           | 1923 (57.5) |\n+--------------------------+-------------------+-----------+-------------+\n|                          | 1                 |           | 1423 (42.5) |\n+--------------------------+-------------------+-----------+-------------+\n\nOutlier Report:\n  - No Outliers found in Sex\n  - Outliers found in T Stage: ['nan: 12 out of 3346', 'T2b: 5 out of 3346', 'T2a: 4 out of 3346', 'TX: 4 out of 3346', 'T3 (2): 3 out of 3346', 'T2 (2): 1 out of 3346', 'T1 (2): 1 out of 3346', 'rT0: 1 out of 3346']\n  - Outliers found in N Stage: ['N3b: 28 out of 3346', 'N3a: 13 out of 3346', 'nan: 13 out of 3346', 'NX: 1 out of 3346']\n  - Outliers found in Stage: ['nan: 27 out of 3346', 'IV: 12 out of 3346', 'X: 6 out of 3346', 'IIA: 2 out of 3346', 'IIIA: 2 out of 3346', 'IIIC: 2 out of 3346', 'IVC: 2 out of 3346', 'IB: 1 out of 3346', 'IIB: 1 out of 3346']\n  - No Outliers found in Smoking Status\n  - Outliers found in Disease Site: ['paranasal sinus: 28 out of 3346', 'skin: 24 out of 3346', 'sarcoma: 20 out of 3346', 'paraganglioma: 7 out of 3346', 'salivary glands: 4 out of 3346', 'other: 2 out of 3346', 'benign tumor: 1 out of 3346', 'lacrimal gland: 1 out of 3346', 'orbit: 1 out of 3346']\n  - No Outliers found in death\n  - No Outliers found in HPV Combined\n  - No Outliers found in Chemotherapy\n\n</pre> <pre>         [info     ] Plotting Pairplot...           [jarvais] call=visualization.__call__:118\n14:41:04 [info     ] Plotting UMAP...               [jarvais] call=visualization.__call__:124\n14:41:13 [info     ] Plotting Frequency Table...    [jarvais] call=visualization.__call__:121\n14:41:22 [info     ] Plotting Multiplot...          [jarvais] call=visualization.__call__:136\nFont MPDFAA+Inter28ptBold is missing the following glyphs: '\n' (\\n)\n</pre> In\u00a0[3]: Copied! <pre>from jarvais.trainer import TrainerSupervised\n\ndf = pd.read_csv('./survival_outputs/analyzer/updated_data.csv')\ndf.rename(columns={'survival_time': 'time', 'death':'event'}, inplace=True)\n\ntrainer = TrainerSupervised(task='survival', output_dir='./radcure_outputs/ED_trainer_explainer',)\ntrainer.run(df, ['event','time'])\n</pre> from jarvais.trainer import TrainerSupervised  df = pd.read_csv('./survival_outputs/analyzer/updated_data.csv') df.rename(columns={'survival_time': 'time', 'death':'event'}, inplace=True)  trainer = TrainerSupervised(task='survival', output_dir='./radcure_outputs/ED_trainer_explainer',) trainer.run(df, ['event','time']) <pre>Training MTLR...\n  Best trial:\n    Params: \n      C1: 0.01\n      dropout: 0.3292835147016483\n      dims: [64, 64]\nTraining DeepSurv...\n  Best trial:\n    Params: \n      l2_reg: 0.006451742136969566\n      dropout: 0.360636124048913\n      dims: [256, 256, 256]\nTraining CoxPH...\nTraining GradientBoosting...\nTraining RandomForest...\nTraining SVM...\n\nConsolidated C-index Scores:\nMTLR: 0.6482\nDeepSurv: 0.6863\nCoxPH: 0.7212\nGradientBoosting: 0.7152\nRandomForest: 0.7156\nSVM: 0.7156\n</pre> In\u00a0[4]: Copied! <pre>from jarvais.explainer import Explainer\n\nexp = Explainer.from_trainer(trainer)\nexp.run()\n</pre> from jarvais.explainer import Explainer  exp = Explainer.from_trainer(trainer) exp.run() <pre>\u26a0\ufe0f  **Possible Bias Detected in Disease Site** \u26a0\ufe0f\n=== Subgroup Analysis for 'Disease Site' Using Cox Proportional Hazards Model ===\n\nModel Statistics:\n    AIC (Partial):               2393.09\n    Log-Likelihood:              -1187.55\n    Log-Likelihood Ratio p-value: 0.0003\n    Concordance Index (C-index):   0.59\nModel Coefficients:\n    +--------------------------------+---------------+------------------+\n    | Feature                        |   Coefficient |   Standard Error |\n    +================================+===============+==================+\n    | Disease Site_Other             |         0.424 |            4.571 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_esophagus         |         0.795 |            4.577 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_hypopharynx       |         0.746 |            4.556 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_larynx            |         0.202 |            4.552 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_lip &amp; oral cavity |         0.666 |            4.561 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_nasal cavity      |        -1.205 |            4.653 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_nasopharynx       |        -0.925 |            4.558 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_oropharynx        |        -0.046 |            4.551 |\n    +--------------------------------+---------------+------------------+\n    | Disease Site_unknown           |         0.142 |            4.557 |\n    +--------------------------------+---------------+------------------+\n\u26a0\ufe0f  **Possible Bias Detected in Smoking Status** \u26a0\ufe0f\n=== Subgroup Analysis for 'Smoking Status' Using Cox Proportional Hazards Model ===\n\nModel Statistics:\n    AIC (Partial):               2375.84\n    Log-Likelihood:              -1183.92\n    Log-Likelihood Ratio p-value: 0.0000\n    Concordance Index (C-index):   0.61\nModel Coefficients:\n    +---------------------------+---------------+------------------+\n    | Feature                   |   Coefficient |   Standard Error |\n    +===========================+===============+==================+\n    | Smoking Status_Current    |         0.508 |            4.720 |\n    +---------------------------+---------------+------------------+\n    | Smoking Status_Ex-smoker  |         0.070 |            4.720 |\n    +---------------------------+---------------+------------------+\n    | Smoking Status_Non-smoker |        -0.710 |            4.721 |\n    +---------------------------+---------------+------------------+\n    | Smoking Status_unknown    |         0.417 |            4.753 |\n    +---------------------------+---------------+------------------+\n</pre>"}]}